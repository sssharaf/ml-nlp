{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "start-servers-play.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sssharaf/ml-nlp/blob/master/start_servers_play.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5lt8ZaM-nCy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "2b0e3393-c082-4735-9d0a-6a01a907fdd3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls -ltr /gdrive/'My Drive'/ML/data/start-servers-play\n",
        "!pip install pytorch_transformers\n",
        "!ln -s  /gdrive/'My Drive'/ML/data/start-servers-play data\n",
        "!ls -ltr data/*"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "total 2\n",
            "-rw------- 1 root root 1339 Aug 10 17:21 train.csv\n",
            "-rw------- 1 root root  437 Aug 10 17:24 val.csv\n",
            "Requirement already satisfied: pytorch_transformers in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (0.1.83)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (0.0.35)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.17.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.3.0+cu100)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.10.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2019.11.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2.8)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.4 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (1.13.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.4->boto3->pytorch_transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.4->boto3->pytorch_transformers) (0.15.2)\n",
            "ln: failed to create symbolic link 'data/start-servers-play': Function not implemented\n",
            "-rw------- 1 root root 1339 Aug 10 17:21 data/train.csv\n",
            "-rw------- 1 root root  437 Aug 10 17:24 data/val.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjxaBXQ6_z49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import pytorch_transformers as pt\n",
        "from pytorch_transformers import BertTokenizer, BertConfig,BertForMaskedLM,BertModel,DistilBertTokenizer, DistilBertModel,DistilBertForSequenceClassification \n",
        "import os\n",
        "import typing\n",
        "from typing import Dict,List,Sequence,Set\n",
        "from types import SimpleNamespace as SN\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight,compute_sample_weight\n",
        "T_BertTokenizer = typing.NewType(\"BertTokenizer\",BertTokenizer)\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziNHWdP8_8UQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab28582c-5368-4a47-c2c7-e6c4d84a7613"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 916029.35B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AExKHXwAJlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_df = pd.read_csv('data/train.csv',dtype={'action':'category','component':'category'})\n",
        "val_df = pd.read_csv('data/val.csv',dtype={'action':'category','component':'category'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-PHDUlxAkSd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "13d68b9b-bfa8-4aa6-bab3-177be7f828c6"
      },
      "source": [
        "a_ohe = OneHotEncoder(sparse=False)\n",
        "a_ohe.fit(trn_df.loc[:,['action']])\n",
        "print(a_ohe.transform([['start']]))\n",
        "print(a_ohe.categories_[0])\n",
        "c_ohe = OneHotEncoder(sparse=False)\n",
        "c_ohe.fit(trn_df.loc[:,['component']])\n",
        "\n",
        "action_le = LabelEncoder()\n",
        "action_le.fit(trn_df.action)\n",
        "component_le = LabelEncoder()\n",
        "component_le.fit(trn_df.component)"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-197-16e00de4e36c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma_ohe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma_ohe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_ohe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_ohe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mc_ohe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    730\u001b[0m                                        copy=True)\n\u001b[1;32m    731\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform_new\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;34m\"\"\"New implementation assuming categorical input\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;31m# validation of X happens in _check_X called by _transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0mX_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mX_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mX_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iloc'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m# if not a dataframe, do normal check_array validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mX_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             if (not hasattr(X, 'dtype')\n\u001b[1;32m     51\u001b[0m                     and np.issubdtype(X_temp.dtype, np.str_)):\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=['start'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPyjvoDcAa6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self,df:DataFrame,max_len = 16):\n",
        "        self.df = df\n",
        "        self.max_len=max_len\n",
        "        self.action = self.df.action.cat.codes\n",
        "        self.component = self.df.component.cat.codes\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        X = self.df.iloc[index]['comment_text']\n",
        "        X = f\"[CLS] {X}[SEP]\"\n",
        "        encoded = torch.tensor(tokenizer.encode(X),dtype=torch.long)\n",
        "        X = torch.zeros(self.max_len,dtype=torch.long)\n",
        "        X[:len(encoded)] = encoded\n",
        "        X[len(encoded)+1:] = torch.tensor(tokenizer.pad_token_id,dtype=torch.long)\n",
        "        X_attn_mask = X!=tokenizer.pad_token_id\n",
        "        Y1 = self.df.iloc[index]['action']\n",
        "        Y1 = action_le.transform([Y1])\n",
        "        #Y1 = a_ohe.transform([[Y1]])\n",
        "        Y1 = torch.tensor(Y1,dtype=torch.long)\n",
        "        Y2 = self.df.iloc[index]['component']\n",
        "        Y2 = component_le.transform([Y2])\n",
        "        #Y2 = c_ohe.transform([[Y2]])\n",
        "        Y2 = torch.tensor(Y2, dtype=torch.long)\n",
        "        return (X,X_attn_mask),(Y1.squeeze(),Y2.squeeze())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def components(self):\n",
        "        return self.component"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPDP-89dCVOQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "724340a3-56e8-40a2-c836-b28c6e018f14"
      },
      "source": [
        "trn_ds = MyDataset(trn_df)\n",
        "val_ds = MyDataset(val_df)\n",
        "trn_ds[2]"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((tensor([  101,  2644,  1996,  3119, 13151,   102,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0]),\n",
              "  tensor([ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
              "          False, False, False, False, False, False])),\n",
              " (tensor(1), tensor(1)))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXTXXGTiCzmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_dl = DataLoader(dataset=trn_ds,batch_size=4,pin_memory=True)\n",
        "val_dl = DataLoader(dataset=val_ds,batch_size=4,pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQUqP1PEGuFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model\n",
        "class MyModel(nn.Module):\n",
        "  def __init__(self, freeze_bert = True):\n",
        "    super().__init__()\n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased')\n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(768,len(a_ohe.categories_[0]),bias=False)\n",
        "    )\n",
        "    self.component_cls_lyr = nn.Sequential(\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(768,len(c_ohe.categories_[0]),bias=False)\n",
        "    )\n",
        "    \n",
        "\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        for p in self.bert_lyr.parameters():\n",
        "            p.requires_grad = False\n",
        "    #nn.init.xavier_uniform_(self.action_cls_lyr.weight)\n",
        "    #nn.init.xavier_uniform_(self.component_cls_lyr.weight)\n",
        "\n",
        "  def forward(self, seq, attn_masks):\n",
        "    seq_emb,ctx = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    return self.action_cls_lyr(ctx),self.component_cls_lyr(ctx)\n",
        "\n",
        "model = MyModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s11DA4cwKnDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_XdOgy_DHCU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f0d6e559-967c-4cfb-f14c-f9ad00fd5153"
      },
      "source": [
        "print(f'Device type is {DEVICE.type}')\n",
        "model.to(DEVICE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=1e-3, )\n",
        "action_criterion = nn.modules.loss.CrossEntropyLoss(reduction='mean')\n",
        "component_criterion = nn.modules.loss.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "n_epochs = 1000\n",
        "\n",
        "def evaluate_model(model:MyModel,dl:DataLoader,optimizer):\n",
        "  t_loss=0\n",
        "  for (X, attn_mask),(Y1,Y2) in dl:\n",
        "    X , attn_mask,Y1,Y2 = X.to(DEVICE),attn_mask.to(DEVICE),Y1.to(DEVICE),Y2.to(DEVICE)\n",
        "    p_a,p_c = model(X,attn_mask)\n",
        "    loss = action_criterion(p_a,Y1) + component_criterion(p_c,Y2)\n",
        "    if optimizer is not None:\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    t_loss += loss.item()\n",
        "  return t_loss\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  t_loss=0\n",
        "  t_loss = evaluate_model(model,trn_dl,optimizer)\n",
        "  v_loss = 0\n",
        "  with torch.no_grad():\n",
        "    v_loss = evaluate_model(model,val_dl,None)\n",
        "  # for (X, attn_mask),(Y1,Y2) in trn_dl:\n",
        "  #   X , attn_mask,Y1,Y2 = X.to(DEVICE),attn_mask.to(DEVICE),Y1.to(DEVICE),Y2.to(DEVICE)\n",
        "  #   p_a,p_c = model(X,attn_mask)\n",
        "  #   loss = action_criterion(p_a,Y1) + component_criterion(p_c,Y2)\n",
        "  #   optimizer.zero_grad()\n",
        "  #   loss.backward()\n",
        "  #   optimizer.step()\n",
        "  #   t_loss += loss.item()\n",
        "  print(f'Epoch:{epoch} Training loss={t_loss} , Validation loss:{v_loss}')\n"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device type is cuda\n",
            "Epoch:0 Training loss=33.82660889625549 , Validation loss:6.631506323814392\n",
            "Epoch:1 Training loss=19.851772665977478 , Validation loss:6.9083579778671265\n",
            "Epoch:2 Training loss=23.31220269203186 , Validation loss:6.341805577278137\n",
            "Epoch:3 Training loss=23.073599219322205 , Validation loss:6.081575989723206\n",
            "Epoch:4 Training loss=21.743334650993347 , Validation loss:6.24396014213562\n",
            "Epoch:5 Training loss=21.57986319065094 , Validation loss:6.531764507293701\n",
            "Epoch:6 Training loss=20.794257164001465 , Validation loss:6.160957455635071\n",
            "Epoch:7 Training loss=20.943405270576477 , Validation loss:6.282789826393127\n",
            "Epoch:8 Training loss=20.567913055419922 , Validation loss:6.126418590545654\n",
            "Epoch:9 Training loss=20.280345916748047 , Validation loss:5.952383399009705\n",
            "Epoch:10 Training loss=19.91410493850708 , Validation loss:6.207619547843933\n",
            "Epoch:11 Training loss=19.423296689987183 , Validation loss:5.8740071058273315\n",
            "Epoch:12 Training loss=19.606853485107422 , Validation loss:6.011393070220947\n",
            "Epoch:13 Training loss=19.08965766429901 , Validation loss:5.820986747741699\n",
            "Epoch:14 Training loss=19.18287420272827 , Validation loss:6.075615048408508\n",
            "Epoch:15 Training loss=19.014647126197815 , Validation loss:6.126940965652466\n",
            "Epoch:16 Training loss=18.271671772003174 , Validation loss:5.973446846008301\n",
            "Epoch:17 Training loss=18.613534092903137 , Validation loss:5.773473501205444\n",
            "Epoch:18 Training loss=18.150917291641235 , Validation loss:6.020817160606384\n",
            "Epoch:19 Training loss=18.152551412582397 , Validation loss:5.755445241928101\n",
            "Epoch:20 Training loss=17.737558245658875 , Validation loss:5.881271719932556\n",
            "Epoch:21 Training loss=18.2706561088562 , Validation loss:5.576648712158203\n",
            "Epoch:22 Training loss=17.15356194972992 , Validation loss:5.8626625537872314\n",
            "Epoch:23 Training loss=17.543651282787323 , Validation loss:5.813368916511536\n",
            "Epoch:24 Training loss=17.526034235954285 , Validation loss:5.826799273490906\n",
            "Epoch:25 Training loss=17.027886927127838 , Validation loss:5.9848480224609375\n",
            "Epoch:26 Training loss=17.040781021118164 , Validation loss:6.024535536766052\n",
            "Epoch:27 Training loss=16.706146121025085 , Validation loss:5.909820079803467\n",
            "Epoch:28 Training loss=16.41595095396042 , Validation loss:5.976503849029541\n",
            "Epoch:29 Training loss=15.980805695056915 , Validation loss:5.6557005643844604\n",
            "Epoch:30 Training loss=16.114898443222046 , Validation loss:5.967503309249878\n",
            "Epoch:31 Training loss=15.925290942192078 , Validation loss:5.750086784362793\n",
            "Epoch:32 Training loss=15.859499275684357 , Validation loss:6.024373292922974\n",
            "Epoch:33 Training loss=15.773075878620148 , Validation loss:5.489633321762085\n",
            "Epoch:34 Training loss=15.717240989208221 , Validation loss:5.689035177230835\n",
            "Epoch:35 Training loss=15.867351233959198 , Validation loss:5.435329914093018\n",
            "Epoch:36 Training loss=15.227394819259644 , Validation loss:5.751365900039673\n",
            "Epoch:37 Training loss=15.588347792625427 , Validation loss:5.643871545791626\n",
            "Epoch:38 Training loss=15.473919987678528 , Validation loss:5.505310893058777\n",
            "Epoch:39 Training loss=15.373068988323212 , Validation loss:5.646354079246521\n",
            "Epoch:40 Training loss=14.773536384105682 , Validation loss:5.6850913763046265\n",
            "Epoch:41 Training loss=14.907483756542206 , Validation loss:5.460119605064392\n",
            "Epoch:42 Training loss=14.824554741382599 , Validation loss:5.875408887863159\n",
            "Epoch:43 Training loss=15.237597525119781 , Validation loss:5.483119010925293\n",
            "Epoch:44 Training loss=14.37685078382492 , Validation loss:5.71956729888916\n",
            "Epoch:45 Training loss=14.108020007610321 , Validation loss:5.425473093986511\n",
            "Epoch:46 Training loss=14.345892548561096 , Validation loss:5.719894170761108\n",
            "Epoch:47 Training loss=14.139522194862366 , Validation loss:5.495068311691284\n",
            "Epoch:48 Training loss=14.16891074180603 , Validation loss:5.711908578872681\n",
            "Epoch:49 Training loss=13.947231113910675 , Validation loss:5.506489396095276\n",
            "Epoch:50 Training loss=13.470155477523804 , Validation loss:5.23141872882843\n",
            "Epoch:51 Training loss=13.541672825813293 , Validation loss:5.433022737503052\n",
            "Epoch:52 Training loss=14.215576887130737 , Validation loss:5.530933499336243\n",
            "Epoch:53 Training loss=13.503739833831787 , Validation loss:5.633678555488586\n",
            "Epoch:54 Training loss=13.808202743530273 , Validation loss:5.516334533691406\n",
            "Epoch:55 Training loss=13.333504557609558 , Validation loss:5.696650385856628\n",
            "Epoch:56 Training loss=13.616851150989532 , Validation loss:5.660534858703613\n",
            "Epoch:57 Training loss=13.128589034080505 , Validation loss:5.460803389549255\n",
            "Epoch:58 Training loss=13.138199090957642 , Validation loss:5.2325170040130615\n",
            "Epoch:59 Training loss=13.323105454444885 , Validation loss:5.115691423416138\n",
            "Epoch:60 Training loss=12.2160142660141 , Validation loss:5.442845463752747\n",
            "Epoch:61 Training loss=12.916977047920227 , Validation loss:5.4965163469314575\n",
            "Epoch:62 Training loss=12.596318781375885 , Validation loss:5.543148875236511\n",
            "Epoch:63 Training loss=12.330849468708038 , Validation loss:5.6587101221084595\n",
            "Epoch:64 Training loss=12.526924192905426 , Validation loss:5.273459315299988\n",
            "Epoch:65 Training loss=12.656865894794464 , Validation loss:5.394136190414429\n",
            "Epoch:66 Training loss=12.388708889484406 , Validation loss:5.414039254188538\n",
            "Epoch:67 Training loss=12.26931893825531 , Validation loss:5.1637773513793945\n",
            "Epoch:68 Training loss=12.493742287158966 , Validation loss:5.598768949508667\n",
            "Epoch:69 Training loss=11.929507553577423 , Validation loss:5.354722023010254\n",
            "Epoch:70 Training loss=11.982000172138214 , Validation loss:5.301806569099426\n",
            "Epoch:71 Training loss=11.913959920406342 , Validation loss:5.337618827819824\n",
            "Epoch:72 Training loss=11.563188016414642 , Validation loss:5.485276699066162\n",
            "Epoch:73 Training loss=12.03459495306015 , Validation loss:5.1992411613464355\n",
            "Epoch:74 Training loss=11.587101936340332 , Validation loss:4.975917339324951\n",
            "Epoch:75 Training loss=11.668227016925812 , Validation loss:5.411170959472656\n",
            "Epoch:76 Training loss=11.738260507583618 , Validation loss:5.157137155532837\n",
            "Epoch:77 Training loss=11.257744073867798 , Validation loss:5.3093647956848145\n",
            "Epoch:78 Training loss=10.97452574968338 , Validation loss:5.171521067619324\n",
            "Epoch:79 Training loss=11.450983166694641 , Validation loss:5.211471319198608\n",
            "Epoch:80 Training loss=11.352068930864334 , Validation loss:5.233637094497681\n",
            "Epoch:81 Training loss=11.342729985713959 , Validation loss:4.786915063858032\n",
            "Epoch:82 Training loss=11.365446090698242 , Validation loss:5.572119951248169\n",
            "Epoch:83 Training loss=11.332550704479218 , Validation loss:4.8420515060424805\n",
            "Epoch:84 Training loss=10.699711561203003 , Validation loss:5.304768681526184\n",
            "Epoch:85 Training loss=11.155245125293732 , Validation loss:5.217518448829651\n",
            "Epoch:86 Training loss=10.799519181251526 , Validation loss:4.8485729694366455\n",
            "Epoch:87 Training loss=10.771791934967041 , Validation loss:5.074499726295471\n",
            "Epoch:88 Training loss=10.787118196487427 , Validation loss:4.900100827217102\n",
            "Epoch:89 Training loss=10.401808261871338 , Validation loss:4.938384652137756\n",
            "Epoch:90 Training loss=10.440228641033173 , Validation loss:5.10719633102417\n",
            "Epoch:91 Training loss=10.546866655349731 , Validation loss:5.110257387161255\n",
            "Epoch:92 Training loss=10.502430379390717 , Validation loss:5.365782737731934\n",
            "Epoch:93 Training loss=10.087005615234375 , Validation loss:4.768915176391602\n",
            "Epoch:94 Training loss=10.350135743618011 , Validation loss:4.996520042419434\n",
            "Epoch:95 Training loss=10.167739033699036 , Validation loss:5.020250916481018\n",
            "Epoch:96 Training loss=10.597752571105957 , Validation loss:5.022226810455322\n",
            "Epoch:97 Training loss=10.221045017242432 , Validation loss:4.836966037750244\n",
            "Epoch:98 Training loss=10.402569711208344 , Validation loss:5.064326286315918\n",
            "Epoch:99 Training loss=10.460549712181091 , Validation loss:4.629514932632446\n",
            "Epoch:100 Training loss=9.836340188980103 , Validation loss:5.168159484863281\n",
            "Epoch:101 Training loss=10.239973247051239 , Validation loss:4.847500205039978\n",
            "Epoch:102 Training loss=10.018437445163727 , Validation loss:5.11909806728363\n",
            "Epoch:103 Training loss=10.35021960735321 , Validation loss:4.975878596305847\n",
            "Epoch:104 Training loss=9.90511041879654 , Validation loss:4.85560417175293\n",
            "Epoch:105 Training loss=10.041490346193314 , Validation loss:4.882969617843628\n",
            "Epoch:106 Training loss=10.351168304681778 , Validation loss:5.170255184173584\n",
            "Epoch:107 Training loss=10.188296258449554 , Validation loss:4.897117853164673\n",
            "Epoch:108 Training loss=9.533791899681091 , Validation loss:4.9065306186676025\n",
            "Epoch:109 Training loss=9.387434720993042 , Validation loss:4.799017310142517\n",
            "Epoch:110 Training loss=9.825041711330414 , Validation loss:4.746029496192932\n",
            "Epoch:111 Training loss=10.029796302318573 , Validation loss:4.7664806842803955\n",
            "Epoch:112 Training loss=9.130603909492493 , Validation loss:5.160332441329956\n",
            "Epoch:113 Training loss=9.516424834728241 , Validation loss:4.753365874290466\n",
            "Epoch:114 Training loss=9.833981603384018 , Validation loss:5.106477856636047\n",
            "Epoch:115 Training loss=9.387058079242706 , Validation loss:4.569073796272278\n",
            "Epoch:116 Training loss=8.997140169143677 , Validation loss:4.76347029209137\n",
            "Epoch:117 Training loss=9.182046204805374 , Validation loss:4.9296663999557495\n",
            "Epoch:118 Training loss=9.149752110242844 , Validation loss:4.691371202468872\n",
            "Epoch:119 Training loss=9.519547879695892 , Validation loss:5.083609223365784\n",
            "Epoch:120 Training loss=9.426263988018036 , Validation loss:4.805097818374634\n",
            "Epoch:121 Training loss=9.037718415260315 , Validation loss:4.86150848865509\n",
            "Epoch:122 Training loss=8.632308214902878 , Validation loss:4.517041563987732\n",
            "Epoch:123 Training loss=9.594335079193115 , Validation loss:4.544317603111267\n",
            "Epoch:124 Training loss=9.102021992206573 , Validation loss:4.785602688789368\n",
            "Epoch:125 Training loss=8.69016844034195 , Validation loss:4.5144665241241455\n",
            "Epoch:126 Training loss=9.157862722873688 , Validation loss:4.919942021369934\n",
            "Epoch:127 Training loss=9.055228292942047 , Validation loss:4.829950332641602\n",
            "Epoch:128 Training loss=8.569002091884613 , Validation loss:4.778209745883942\n",
            "Epoch:129 Training loss=9.095519840717316 , Validation loss:4.755064964294434\n",
            "Epoch:130 Training loss=8.911351084709167 , Validation loss:4.877438426017761\n",
            "Epoch:131 Training loss=8.84938097000122 , Validation loss:4.602695107460022\n",
            "Epoch:132 Training loss=8.623004853725433 , Validation loss:4.285814523696899\n",
            "Epoch:133 Training loss=8.457715004682541 , Validation loss:4.3385385274887085\n",
            "Epoch:134 Training loss=7.840334057807922 , Validation loss:4.5170347690582275\n",
            "Epoch:135 Training loss=8.846544206142426 , Validation loss:4.351086378097534\n",
            "Epoch:136 Training loss=8.496946394443512 , Validation loss:4.406291961669922\n",
            "Epoch:137 Training loss=8.392043054103851 , Validation loss:4.70244836807251\n",
            "Epoch:138 Training loss=8.272655248641968 , Validation loss:4.598146200180054\n",
            "Epoch:139 Training loss=7.931237876415253 , Validation loss:4.550828814506531\n",
            "Epoch:140 Training loss=8.542758524417877 , Validation loss:4.8580543994903564\n",
            "Epoch:141 Training loss=8.677129447460175 , Validation loss:5.297595262527466\n",
            "Epoch:142 Training loss=8.382444947957993 , Validation loss:4.43474817276001\n",
            "Epoch:143 Training loss=7.9542602598667145 , Validation loss:4.6626927852630615\n",
            "Epoch:144 Training loss=7.947638750076294 , Validation loss:4.522036790847778\n",
            "Epoch:145 Training loss=8.205642461776733 , Validation loss:4.809330105781555\n",
            "Epoch:146 Training loss=8.465909242630005 , Validation loss:4.301653981208801\n",
            "Epoch:147 Training loss=8.235952734947205 , Validation loss:4.713990211486816\n",
            "Epoch:148 Training loss=8.285188794136047 , Validation loss:4.940771102905273\n",
            "Epoch:149 Training loss=7.741778910160065 , Validation loss:4.2949652671813965\n",
            "Epoch:150 Training loss=8.001177251338959 , Validation loss:4.488270401954651\n",
            "Epoch:151 Training loss=8.480223208665848 , Validation loss:4.712831139564514\n",
            "Epoch:152 Training loss=8.56486627459526 , Validation loss:4.563542485237122\n",
            "Epoch:153 Training loss=7.7370316088199615 , Validation loss:4.931070685386658\n",
            "Epoch:154 Training loss=7.887118995189667 , Validation loss:4.52409154176712\n",
            "Epoch:155 Training loss=7.507009446620941 , Validation loss:4.988147974014282\n",
            "Epoch:156 Training loss=8.444013118743896 , Validation loss:4.488727569580078\n",
            "Epoch:157 Training loss=7.082821577787399 , Validation loss:4.807024836540222\n",
            "Epoch:158 Training loss=7.468192309141159 , Validation loss:3.9994872212409973\n",
            "Epoch:159 Training loss=7.4607973992824554 , Validation loss:4.745601415634155\n",
            "Epoch:160 Training loss=7.867142051458359 , Validation loss:4.4501011967659\n",
            "Epoch:161 Training loss=7.490549743175507 , Validation loss:4.219916939735413\n",
            "Epoch:162 Training loss=7.428900957107544 , Validation loss:4.0218393206596375\n",
            "Epoch:163 Training loss=8.005586087703705 , Validation loss:4.444594979286194\n",
            "Epoch:164 Training loss=7.17324286699295 , Validation loss:5.1711952686309814\n",
            "Epoch:165 Training loss=7.4954672157764435 , Validation loss:5.0255467891693115\n",
            "Epoch:166 Training loss=7.503338277339935 , Validation loss:4.282817721366882\n",
            "Epoch:167 Training loss=7.366518557071686 , Validation loss:4.356132984161377\n",
            "Epoch:168 Training loss=7.565447002649307 , Validation loss:4.4768900871276855\n",
            "Epoch:169 Training loss=7.264153480529785 , Validation loss:4.756445646286011\n",
            "Epoch:170 Training loss=7.368476092815399 , Validation loss:4.590288400650024\n",
            "Epoch:171 Training loss=7.268005013465881 , Validation loss:4.060955047607422\n",
            "Epoch:172 Training loss=6.720148980617523 , Validation loss:4.1493529081344604\n",
            "Epoch:173 Training loss=8.01966854929924 , Validation loss:4.3833465576171875\n",
            "Epoch:174 Training loss=7.11813098192215 , Validation loss:4.582253932952881\n",
            "Epoch:175 Training loss=7.352799445390701 , Validation loss:4.802446365356445\n",
            "Epoch:176 Training loss=7.1233276426792145 , Validation loss:4.5953370332717896\n",
            "Epoch:177 Training loss=7.1456722021102905 , Validation loss:4.411515474319458\n",
            "Epoch:178 Training loss=6.76969251036644 , Validation loss:4.448752284049988\n",
            "Epoch:179 Training loss=7.013319939374924 , Validation loss:4.599422812461853\n",
            "Epoch:180 Training loss=7.390090584754944 , Validation loss:4.968340635299683\n",
            "Epoch:181 Training loss=6.803005218505859 , Validation loss:4.53307843208313\n",
            "Epoch:182 Training loss=6.682881265878677 , Validation loss:4.695458292961121\n",
            "Epoch:183 Training loss=6.946528047323227 , Validation loss:4.086592197418213\n",
            "Epoch:184 Training loss=6.961088180541992 , Validation loss:4.5191885232925415\n",
            "Epoch:185 Training loss=6.86029252409935 , Validation loss:4.181648313999176\n",
            "Epoch:186 Training loss=6.798216789960861 , Validation loss:4.2962223291397095\n",
            "Epoch:187 Training loss=7.177333235740662 , Validation loss:4.375582158565521\n",
            "Epoch:188 Training loss=6.580376923084259 , Validation loss:4.086384892463684\n",
            "Epoch:189 Training loss=7.057056814432144 , Validation loss:4.285839915275574\n",
            "Epoch:190 Training loss=7.3689528703689575 , Validation loss:4.003926992416382\n",
            "Epoch:191 Training loss=6.9646183252334595 , Validation loss:4.06712144613266\n",
            "Epoch:192 Training loss=6.804476022720337 , Validation loss:4.64873206615448\n",
            "Epoch:193 Training loss=6.466638475656509 , Validation loss:4.385406494140625\n",
            "Epoch:194 Training loss=7.442938953638077 , Validation loss:4.482987582683563\n",
            "Epoch:195 Training loss=6.329486459493637 , Validation loss:4.571826219558716\n",
            "Epoch:196 Training loss=7.075008928775787 , Validation loss:4.254973530769348\n",
            "Epoch:197 Training loss=6.819200575351715 , Validation loss:4.253930926322937\n",
            "Epoch:198 Training loss=6.823667734861374 , Validation loss:4.385195851325989\n",
            "Epoch:199 Training loss=6.80726683139801 , Validation loss:4.6606109738349915\n",
            "Epoch:200 Training loss=6.644542813301086 , Validation loss:4.0834460854530334\n",
            "Epoch:201 Training loss=6.262578040361404 , Validation loss:4.015786647796631\n",
            "Epoch:202 Training loss=6.112852871417999 , Validation loss:4.592909157276154\n",
            "Epoch:203 Training loss=6.29500287771225 , Validation loss:4.191940009593964\n",
            "Epoch:204 Training loss=6.753435730934143 , Validation loss:4.921820878982544\n",
            "Epoch:205 Training loss=6.855673789978027 , Validation loss:4.18731415271759\n",
            "Epoch:206 Training loss=6.761327147483826 , Validation loss:4.325161337852478\n",
            "Epoch:207 Training loss=7.034368515014648 , Validation loss:4.240983605384827\n",
            "Epoch:208 Training loss=5.966854274272919 , Validation loss:4.433992147445679\n",
            "Epoch:209 Training loss=6.841753453016281 , Validation loss:4.111520171165466\n",
            "Epoch:210 Training loss=6.665711015462875 , Validation loss:4.015596628189087\n",
            "Epoch:211 Training loss=6.598158746957779 , Validation loss:4.052743971347809\n",
            "Epoch:212 Training loss=6.806934326887131 , Validation loss:4.393367111682892\n",
            "Epoch:213 Training loss=6.5058799386024475 , Validation loss:4.183035314083099\n",
            "Epoch:214 Training loss=6.690321117639542 , Validation loss:4.017291784286499\n",
            "Epoch:215 Training loss=6.380138695240021 , Validation loss:4.064406514167786\n",
            "Epoch:216 Training loss=6.046531945466995 , Validation loss:4.523039758205414\n",
            "Epoch:217 Training loss=6.232105493545532 , Validation loss:4.445579171180725\n",
            "Epoch:218 Training loss=6.159555733203888 , Validation loss:4.1332210302352905\n",
            "Epoch:219 Training loss=5.729493349790573 , Validation loss:4.31349503993988\n",
            "Epoch:220 Training loss=5.876923322677612 , Validation loss:3.9348273873329163\n",
            "Epoch:221 Training loss=6.178512990474701 , Validation loss:3.8203224539756775\n",
            "Epoch:222 Training loss=6.435194402933121 , Validation loss:4.360433459281921\n",
            "Epoch:223 Training loss=6.059803307056427 , Validation loss:4.530371308326721\n",
            "Epoch:224 Training loss=6.138404726982117 , Validation loss:4.276392698287964\n",
            "Epoch:225 Training loss=5.6675489246845245 , Validation loss:4.038448095321655\n",
            "Epoch:226 Training loss=5.9897946417331696 , Validation loss:4.1590646505355835\n",
            "Epoch:227 Training loss=5.926353991031647 , Validation loss:4.295713782310486\n",
            "Epoch:228 Training loss=5.325544148683548 , Validation loss:4.477083921432495\n",
            "Epoch:229 Training loss=6.266572505235672 , Validation loss:3.8000544905662537\n",
            "Epoch:230 Training loss=5.4152171313762665 , Validation loss:4.1628777384758\n",
            "Epoch:231 Training loss=5.910301566123962 , Validation loss:4.28599488735199\n",
            "Epoch:232 Training loss=6.387713521718979 , Validation loss:4.298026442527771\n",
            "Epoch:233 Training loss=6.288602411746979 , Validation loss:4.357508659362793\n",
            "Epoch:234 Training loss=5.826792240142822 , Validation loss:4.625645637512207\n",
            "Epoch:235 Training loss=6.4011193215847015 , Validation loss:4.652245879173279\n",
            "Epoch:236 Training loss=5.764131963253021 , Validation loss:4.435270547866821\n",
            "Epoch:237 Training loss=5.82223317027092 , Validation loss:3.8449138402938843\n",
            "Epoch:238 Training loss=5.620683789253235 , Validation loss:4.2241421937942505\n",
            "Epoch:239 Training loss=6.411458313465118 , Validation loss:4.082053482532501\n",
            "Epoch:240 Training loss=5.94726487994194 , Validation loss:4.323097586631775\n",
            "Epoch:241 Training loss=5.234738290309906 , Validation loss:4.450231313705444\n",
            "Epoch:242 Training loss=6.070673078298569 , Validation loss:4.024228394031525\n",
            "Epoch:243 Training loss=6.100634306669235 , Validation loss:4.657523155212402\n",
            "Epoch:244 Training loss=4.9689894914627075 , Validation loss:3.8992987871170044\n",
            "Epoch:245 Training loss=5.21314150094986 , Validation loss:4.406287133693695\n",
            "Epoch:246 Training loss=5.738317966461182 , Validation loss:4.062990963459015\n",
            "Epoch:247 Training loss=5.707926765084267 , Validation loss:4.294213354587555\n",
            "Epoch:248 Training loss=5.534803956747055 , Validation loss:4.624620676040649\n",
            "Epoch:249 Training loss=5.613659679889679 , Validation loss:4.367322683334351\n",
            "Epoch:250 Training loss=5.722794324159622 , Validation loss:3.9826327562332153\n",
            "Epoch:251 Training loss=5.061255246400833 , Validation loss:4.462890028953552\n",
            "Epoch:252 Training loss=5.182875066995621 , Validation loss:4.789979875087738\n",
            "Epoch:253 Training loss=5.386763721704483 , Validation loss:4.580007910728455\n",
            "Epoch:254 Training loss=5.533085912466049 , Validation loss:4.219341993331909\n",
            "Epoch:255 Training loss=5.372372284531593 , Validation loss:4.167222023010254\n",
            "Epoch:256 Training loss=6.061414480209351 , Validation loss:4.298775136470795\n",
            "Epoch:257 Training loss=5.879225879907608 , Validation loss:4.353525996208191\n",
            "Epoch:258 Training loss=5.773552909493446 , Validation loss:3.633226752281189\n",
            "Epoch:259 Training loss=5.466457009315491 , Validation loss:4.11583286523819\n",
            "Epoch:260 Training loss=5.9081025421619415 , Validation loss:4.29500687122345\n",
            "Epoch:261 Training loss=5.400168791413307 , Validation loss:3.8526570796966553\n",
            "Epoch:262 Training loss=5.772337168455124 , Validation loss:4.266281545162201\n",
            "Epoch:263 Training loss=5.565730184316635 , Validation loss:3.8567757606506348\n",
            "Epoch:264 Training loss=5.096363842487335 , Validation loss:3.891652524471283\n",
            "Epoch:265 Training loss=5.052085727453232 , Validation loss:4.028796434402466\n",
            "Epoch:266 Training loss=5.776901870965958 , Validation loss:4.1076964139938354\n",
            "Epoch:267 Training loss=4.687576144933701 , Validation loss:4.070895254611969\n",
            "Epoch:268 Training loss=5.421132460236549 , Validation loss:3.971997320652008\n",
            "Epoch:269 Training loss=6.076366141438484 , Validation loss:3.9515973329544067\n",
            "Epoch:270 Training loss=5.136575311422348 , Validation loss:4.081685125827789\n",
            "Epoch:271 Training loss=5.750287055969238 , Validation loss:3.889451503753662\n",
            "Epoch:272 Training loss=5.36298942565918 , Validation loss:3.8565458059310913\n",
            "Epoch:273 Training loss=4.516853243112564 , Validation loss:4.00153785943985\n",
            "Epoch:274 Training loss=4.966254040598869 , Validation loss:3.9774853587150574\n",
            "Epoch:275 Training loss=4.973839417099953 , Validation loss:3.7717143297195435\n",
            "Epoch:276 Training loss=5.374468088150024 , Validation loss:3.9833677411079407\n",
            "Epoch:277 Training loss=4.562416270375252 , Validation loss:3.7369852662086487\n",
            "Epoch:278 Training loss=4.737780123949051 , Validation loss:4.120695114135742\n",
            "Epoch:279 Training loss=4.885886043310165 , Validation loss:4.033771634101868\n",
            "Epoch:280 Training loss=5.217111676931381 , Validation loss:3.627181649208069\n",
            "Epoch:281 Training loss=5.2441079914569855 , Validation loss:3.4074313044548035\n",
            "Epoch:282 Training loss=5.391953319311142 , Validation loss:3.735071361064911\n",
            "Epoch:283 Training loss=4.83606630563736 , Validation loss:3.7698304057121277\n",
            "Epoch:284 Training loss=4.687160715460777 , Validation loss:4.106074392795563\n",
            "Epoch:285 Training loss=5.815249860286713 , Validation loss:4.0760563015937805\n",
            "Epoch:286 Training loss=4.669980138540268 , Validation loss:4.664771735668182\n",
            "Epoch:287 Training loss=4.814278185367584 , Validation loss:3.7217425107955933\n",
            "Epoch:288 Training loss=4.693681433796883 , Validation loss:3.6757982969284058\n",
            "Epoch:289 Training loss=5.390599012374878 , Validation loss:3.6930482983589172\n",
            "Epoch:290 Training loss=5.18092043697834 , Validation loss:4.234694957733154\n",
            "Epoch:291 Training loss=4.477668583393097 , Validation loss:3.862108588218689\n",
            "Epoch:292 Training loss=4.619151562452316 , Validation loss:4.868879675865173\n",
            "Epoch:293 Training loss=5.023878753185272 , Validation loss:3.782289445400238\n",
            "Epoch:294 Training loss=4.434823736548424 , Validation loss:4.195054411888123\n",
            "Epoch:295 Training loss=4.774404615163803 , Validation loss:3.929194211959839\n",
            "Epoch:296 Training loss=4.7235876470804214 , Validation loss:4.325697302818298\n",
            "Epoch:297 Training loss=4.821444571018219 , Validation loss:4.080602169036865\n",
            "Epoch:298 Training loss=4.3409415781497955 , Validation loss:4.607058584690094\n",
            "Epoch:299 Training loss=5.442910254001617 , Validation loss:3.8021146059036255\n",
            "Epoch:300 Training loss=4.914421781897545 , Validation loss:3.6320789456367493\n",
            "Epoch:301 Training loss=4.655220448970795 , Validation loss:4.034992158412933\n",
            "Epoch:302 Training loss=4.384225577116013 , Validation loss:4.275793194770813\n",
            "Epoch:303 Training loss=4.745980486273766 , Validation loss:3.930042803287506\n",
            "Epoch:304 Training loss=4.564396753907204 , Validation loss:3.6794063448905945\n",
            "Epoch:305 Training loss=4.376253515481949 , Validation loss:4.009438693523407\n",
            "Epoch:306 Training loss=4.706347361207008 , Validation loss:4.287266552448273\n",
            "Epoch:307 Training loss=4.814410418272018 , Validation loss:3.8149697184562683\n",
            "Epoch:308 Training loss=4.405520260334015 , Validation loss:3.7789978981018066\n",
            "Epoch:309 Training loss=4.512532964348793 , Validation loss:3.7837219834327698\n",
            "Epoch:310 Training loss=4.590304493904114 , Validation loss:3.4557541608810425\n",
            "Epoch:311 Training loss=4.186976984143257 , Validation loss:4.25290060043335\n",
            "Epoch:312 Training loss=4.8054623901844025 , Validation loss:4.05903172492981\n",
            "Epoch:313 Training loss=4.1590239852666855 , Validation loss:4.619750261306763\n",
            "Epoch:314 Training loss=4.325570672750473 , Validation loss:4.672811448574066\n",
            "Epoch:315 Training loss=4.738205403089523 , Validation loss:3.7296151518821716\n",
            "Epoch:316 Training loss=3.9936861097812653 , Validation loss:4.0441702008247375\n",
            "Epoch:317 Training loss=5.196282982826233 , Validation loss:4.19255268573761\n",
            "Epoch:318 Training loss=4.185486450791359 , Validation loss:3.613878846168518\n",
            "Epoch:319 Training loss=3.9364854246377945 , Validation loss:4.887473940849304\n",
            "Epoch:320 Training loss=5.203624069690704 , Validation loss:3.9865721464157104\n",
            "Epoch:321 Training loss=3.902976006269455 , Validation loss:3.798541784286499\n",
            "Epoch:322 Training loss=3.930612191557884 , Validation loss:3.8294320106506348\n",
            "Epoch:323 Training loss=4.417948842048645 , Validation loss:3.5590808391571045\n",
            "Epoch:324 Training loss=4.578524738550186 , Validation loss:3.880145788192749\n",
            "Epoch:325 Training loss=4.305091664195061 , Validation loss:4.52591347694397\n",
            "Epoch:326 Training loss=4.558648377656937 , Validation loss:3.4208303093910217\n",
            "Epoch:327 Training loss=4.285162299871445 , Validation loss:3.8399999141693115\n",
            "Epoch:328 Training loss=4.7103897631168365 , Validation loss:3.6339223980903625\n",
            "Epoch:329 Training loss=4.700892001390457 , Validation loss:4.175491631031036\n",
            "Epoch:330 Training loss=4.331610217690468 , Validation loss:4.28709751367569\n",
            "Epoch:331 Training loss=4.838168531656265 , Validation loss:3.4026764035224915\n",
            "Epoch:332 Training loss=4.6858334839344025 , Validation loss:3.695288836956024\n",
            "Epoch:333 Training loss=3.931685358285904 , Validation loss:4.374701976776123\n",
            "Epoch:334 Training loss=4.868046998977661 , Validation loss:3.356062948703766\n",
            "Epoch:335 Training loss=4.863925814628601 , Validation loss:4.249484539031982\n",
            "Epoch:336 Training loss=3.7934681177139282 , Validation loss:4.713144481182098\n",
            "Epoch:337 Training loss=4.270916283130646 , Validation loss:4.30222749710083\n",
            "Epoch:338 Training loss=4.901196837425232 , Validation loss:4.757729649543762\n",
            "Epoch:339 Training loss=4.125707983970642 , Validation loss:3.6547062397003174\n",
            "Epoch:340 Training loss=4.045758426189423 , Validation loss:3.734587788581848\n",
            "Epoch:341 Training loss=4.629811331629753 , Validation loss:4.431468844413757\n",
            "Epoch:342 Training loss=4.464747130870819 , Validation loss:3.849162220954895\n",
            "Epoch:343 Training loss=4.109065189957619 , Validation loss:3.906136691570282\n",
            "Epoch:344 Training loss=3.8491155207157135 , Validation loss:3.781307637691498\n",
            "Epoch:345 Training loss=4.086243391036987 , Validation loss:3.7800681591033936\n",
            "Epoch:346 Training loss=4.625675305724144 , Validation loss:4.10289192199707\n",
            "Epoch:347 Training loss=4.574141412973404 , Validation loss:4.556609570980072\n",
            "Epoch:348 Training loss=4.095478609204292 , Validation loss:4.12311726808548\n",
            "Epoch:349 Training loss=3.8732051849365234 , Validation loss:3.807294487953186\n",
            "Epoch:350 Training loss=3.5289154052734375 , Validation loss:3.59404718875885\n",
            "Epoch:351 Training loss=4.277377098798752 , Validation loss:4.340052127838135\n",
            "Epoch:352 Training loss=4.357835173606873 , Validation loss:3.9625524282455444\n",
            "Epoch:353 Training loss=4.266543209552765 , Validation loss:4.175843119621277\n",
            "Epoch:354 Training loss=4.449687883257866 , Validation loss:3.744345188140869\n",
            "Epoch:355 Training loss=4.187269777059555 , Validation loss:3.759537696838379\n",
            "Epoch:356 Training loss=4.791642874479294 , Validation loss:3.5387083292007446\n",
            "Epoch:357 Training loss=3.5772702544927597 , Validation loss:3.243457615375519\n",
            "Epoch:358 Training loss=4.193167611956596 , Validation loss:3.520258665084839\n",
            "Epoch:359 Training loss=4.348183676600456 , Validation loss:3.9815430343151093\n",
            "Epoch:360 Training loss=4.406179130077362 , Validation loss:3.6193604469299316\n",
            "Epoch:361 Training loss=4.408096104860306 , Validation loss:4.199358701705933\n",
            "Epoch:362 Training loss=3.929740011692047 , Validation loss:3.436915636062622\n",
            "Epoch:363 Training loss=4.180905073881149 , Validation loss:4.075562417507172\n",
            "Epoch:364 Training loss=3.6517120748758316 , Validation loss:4.023840606212616\n",
            "Epoch:365 Training loss=3.4510879069566727 , Validation loss:4.149259567260742\n",
            "Epoch:366 Training loss=4.607828974723816 , Validation loss:3.3556759357452393\n",
            "Epoch:367 Training loss=3.871978461742401 , Validation loss:3.4614944458007812\n",
            "Epoch:368 Training loss=4.473904266953468 , Validation loss:4.158202290534973\n",
            "Epoch:369 Training loss=3.78097403049469 , Validation loss:4.137077510356903\n",
            "Epoch:370 Training loss=3.443851202726364 , Validation loss:4.504684329032898\n",
            "Epoch:371 Training loss=4.750808045268059 , Validation loss:3.969617545604706\n",
            "Epoch:372 Training loss=3.797152817249298 , Validation loss:3.197486937046051\n",
            "Epoch:373 Training loss=3.424630507826805 , Validation loss:3.619351029396057\n",
            "Epoch:374 Training loss=3.945237785577774 , Validation loss:3.7515361309051514\n",
            "Epoch:375 Training loss=4.30582857131958 , Validation loss:4.087119221687317\n",
            "Epoch:376 Training loss=3.7360575944185257 , Validation loss:4.2132391929626465\n",
            "Epoch:377 Training loss=4.167376011610031 , Validation loss:4.091408908367157\n",
            "Epoch:378 Training loss=3.530922904610634 , Validation loss:3.3044305443763733\n",
            "Epoch:379 Training loss=3.865150138735771 , Validation loss:3.6217305660247803\n",
            "Epoch:380 Training loss=3.962131679058075 , Validation loss:3.4455878138542175\n",
            "Epoch:381 Training loss=4.216133743524551 , Validation loss:3.563521921634674\n",
            "Epoch:382 Training loss=4.093608491122723 , Validation loss:3.818133443593979\n",
            "Epoch:383 Training loss=3.4920364767313004 , Validation loss:3.5572978258132935\n",
            "Epoch:384 Training loss=3.1406477987766266 , Validation loss:3.409191608428955\n",
            "Epoch:385 Training loss=3.7536258846521378 , Validation loss:4.467928051948547\n",
            "Epoch:386 Training loss=4.358168080449104 , Validation loss:3.9053565859794617\n",
            "Epoch:387 Training loss=3.4297328293323517 , Validation loss:3.5980553030967712\n",
            "Epoch:388 Training loss=3.554027795791626 , Validation loss:3.8712114691734314\n",
            "Epoch:389 Training loss=3.707507625222206 , Validation loss:4.0567174553871155\n",
            "Epoch:390 Training loss=3.907035142183304 , Validation loss:3.7064698338508606\n",
            "Epoch:391 Training loss=4.451886683702469 , Validation loss:3.913136601448059\n",
            "Epoch:392 Training loss=3.7533053159713745 , Validation loss:3.686985671520233\n",
            "Epoch:393 Training loss=3.252056747674942 , Validation loss:4.411813199520111\n",
            "Epoch:394 Training loss=3.4776561558246613 , Validation loss:4.071028709411621\n",
            "Epoch:395 Training loss=3.9640005230903625 , Validation loss:4.5096617341041565\n",
            "Epoch:396 Training loss=3.2570864111185074 , Validation loss:3.913360059261322\n",
            "Epoch:397 Training loss=3.84625406563282 , Validation loss:3.220830798149109\n",
            "Epoch:398 Training loss=3.759551927447319 , Validation loss:3.1579620242118835\n",
            "Epoch:399 Training loss=3.5534286350011826 , Validation loss:3.7427121996879578\n",
            "Epoch:400 Training loss=4.112910479307175 , Validation loss:3.8016350269317627\n",
            "Epoch:401 Training loss=3.7490299344062805 , Validation loss:3.5954805612564087\n",
            "Epoch:402 Training loss=3.7488974034786224 , Validation loss:3.885746955871582\n",
            "Epoch:403 Training loss=3.7813178449869156 , Validation loss:3.3975207209587097\n",
            "Epoch:404 Training loss=3.4777355939149857 , Validation loss:2.9552719593048096\n",
            "Epoch:405 Training loss=3.4112500995397568 , Validation loss:3.7863014936447144\n",
            "Epoch:406 Training loss=3.905618816614151 , Validation loss:3.5282121300697327\n",
            "Epoch:407 Training loss=3.9958434104919434 , Validation loss:4.184313952922821\n",
            "Epoch:408 Training loss=3.5420678853988647 , Validation loss:4.3094611167907715\n",
            "Epoch:409 Training loss=3.170222893357277 , Validation loss:4.325258791446686\n",
            "Epoch:410 Training loss=2.985611155629158 , Validation loss:3.76902574300766\n",
            "Epoch:411 Training loss=3.7854768335819244 , Validation loss:3.5184680819511414\n",
            "Epoch:412 Training loss=3.1371355652809143 , Validation loss:3.870665729045868\n",
            "Epoch:413 Training loss=3.1978118270635605 , Validation loss:4.233867168426514\n",
            "Epoch:414 Training loss=3.411714658141136 , Validation loss:3.5731242895126343\n",
            "Epoch:415 Training loss=3.436233952641487 , Validation loss:3.8446863293647766\n",
            "Epoch:416 Training loss=3.190272271633148 , Validation loss:3.7051920294761658\n",
            "Epoch:417 Training loss=3.3914353996515274 , Validation loss:3.573400318622589\n",
            "Epoch:418 Training loss=3.282675862312317 , Validation loss:3.5416956543922424\n",
            "Epoch:419 Training loss=3.4524555653333664 , Validation loss:3.675631731748581\n",
            "Epoch:420 Training loss=3.3992307633161545 , Validation loss:4.114111423492432\n",
            "Epoch:421 Training loss=3.372790038585663 , Validation loss:3.290146827697754\n",
            "Epoch:422 Training loss=3.1825686246156693 , Validation loss:4.188931941986084\n",
            "Epoch:423 Training loss=3.4394119530916214 , Validation loss:4.052710175514221\n",
            "Epoch:424 Training loss=3.46014067530632 , Validation loss:3.113533318042755\n",
            "Epoch:425 Training loss=3.3576941937208176 , Validation loss:3.371486485004425\n",
            "Epoch:426 Training loss=3.7785192728042603 , Validation loss:4.558013081550598\n",
            "Epoch:427 Training loss=3.278273493051529 , Validation loss:3.8531312942504883\n",
            "Epoch:428 Training loss=3.400346577167511 , Validation loss:3.71280437707901\n",
            "Epoch:429 Training loss=3.570757895708084 , Validation loss:3.9813133478164673\n",
            "Epoch:430 Training loss=3.295078456401825 , Validation loss:4.0683305859565735\n",
            "Epoch:431 Training loss=3.5632072538137436 , Validation loss:3.719953238964081\n",
            "Epoch:432 Training loss=3.3528246134519577 , Validation loss:3.3241411447525024\n",
            "Epoch:433 Training loss=3.124401643872261 , Validation loss:4.248990893363953\n",
            "Epoch:434 Training loss=3.546755611896515 , Validation loss:3.515650153160095\n",
            "Epoch:435 Training loss=3.501897946000099 , Validation loss:3.306735575199127\n",
            "Epoch:436 Training loss=3.722391277551651 , Validation loss:4.171328186988831\n",
            "Epoch:437 Training loss=3.3806288689374924 , Validation loss:4.118290543556213\n",
            "Epoch:438 Training loss=3.2885048389434814 , Validation loss:4.26126754283905\n",
            "Epoch:439 Training loss=3.8638960123062134 , Validation loss:3.644615411758423\n",
            "Epoch:440 Training loss=3.2831488102674484 , Validation loss:3.9717695116996765\n",
            "Epoch:441 Training loss=3.261276587843895 , Validation loss:3.477082669734955\n",
            "Epoch:442 Training loss=3.3156224489212036 , Validation loss:3.593757212162018\n",
            "Epoch:443 Training loss=3.166637748479843 , Validation loss:3.251317620277405\n",
            "Epoch:444 Training loss=3.3269870579242706 , Validation loss:3.944745123386383\n",
            "Epoch:445 Training loss=3.549451172351837 , Validation loss:4.604604959487915\n",
            "Epoch:446 Training loss=3.0049979835748672 , Validation loss:4.0234246253967285\n",
            "Epoch:447 Training loss=3.397714838385582 , Validation loss:4.3219926953315735\n",
            "Epoch:448 Training loss=3.1369917541742325 , Validation loss:3.8146244883537292\n",
            "Epoch:449 Training loss=3.7711416333913803 , Validation loss:3.7232067584991455\n",
            "Epoch:450 Training loss=2.9594959020614624 , Validation loss:3.3949193358421326\n",
            "Epoch:451 Training loss=3.241069406270981 , Validation loss:3.611881196498871\n",
            "Epoch:452 Training loss=2.7843239456415176 , Validation loss:4.126771807670593\n",
            "Epoch:453 Training loss=3.0449288934469223 , Validation loss:3.7280750274658203\n",
            "Epoch:454 Training loss=3.6933674216270447 , Validation loss:3.809052735567093\n",
            "Epoch:455 Training loss=2.9039161056280136 , Validation loss:3.66054105758667\n",
            "Epoch:456 Training loss=2.9548280462622643 , Validation loss:3.9423179030418396\n",
            "Epoch:457 Training loss=3.210623562335968 , Validation loss:3.5082204341888428\n",
            "Epoch:458 Training loss=3.276721715927124 , Validation loss:3.6390430629253387\n",
            "Epoch:459 Training loss=2.9436548724770546 , Validation loss:3.44295597076416\n",
            "Epoch:460 Training loss=3.556274265050888 , Validation loss:3.5369753241539\n",
            "Epoch:461 Training loss=3.4505623131990433 , Validation loss:3.679660141468048\n",
            "Epoch:462 Training loss=3.627795249223709 , Validation loss:3.6913338899612427\n",
            "Epoch:463 Training loss=2.93868288397789 , Validation loss:3.870796859264374\n",
            "Epoch:464 Training loss=3.3711675852537155 , Validation loss:3.3603177666664124\n",
            "Epoch:465 Training loss=3.1376123279333115 , Validation loss:3.768856406211853\n",
            "Epoch:466 Training loss=2.9378578066825867 , Validation loss:3.73713481426239\n",
            "Epoch:467 Training loss=3.2040311843156815 , Validation loss:3.793040633201599\n",
            "Epoch:468 Training loss=3.0071564316749573 , Validation loss:3.737147480249405\n",
            "Epoch:469 Training loss=3.153657853603363 , Validation loss:4.184133529663086\n",
            "Epoch:470 Training loss=3.247539147734642 , Validation loss:3.3102192282676697\n",
            "Epoch:471 Training loss=2.648058146238327 , Validation loss:3.7297548949718475\n",
            "Epoch:472 Training loss=3.2081801295280457 , Validation loss:3.5085643529891968\n",
            "Epoch:473 Training loss=2.612741708755493 , Validation loss:2.82880175113678\n",
            "Epoch:474 Training loss=3.0572959780693054 , Validation loss:3.265197992324829\n",
            "Epoch:475 Training loss=3.113942250609398 , Validation loss:2.96709543466568\n",
            "Epoch:476 Training loss=2.8997637778520584 , Validation loss:3.414766788482666\n",
            "Epoch:477 Training loss=3.411801353096962 , Validation loss:3.891679525375366\n",
            "Epoch:478 Training loss=2.941985860466957 , Validation loss:3.8442914485931396\n",
            "Epoch:479 Training loss=3.5713852494955063 , Validation loss:3.9600669741630554\n",
            "Epoch:480 Training loss=2.9845280051231384 , Validation loss:3.6805983185768127\n",
            "Epoch:481 Training loss=3.140376105904579 , Validation loss:3.5167486667633057\n",
            "Epoch:482 Training loss=3.2708890438079834 , Validation loss:3.7214516401290894\n",
            "Epoch:483 Training loss=2.6793918907642365 , Validation loss:3.623038172721863\n",
            "Epoch:484 Training loss=3.0639437586069107 , Validation loss:4.151051104068756\n",
            "Epoch:485 Training loss=2.6426817774772644 , Validation loss:4.23315966129303\n",
            "Epoch:486 Training loss=2.742634356021881 , Validation loss:3.2778342068195343\n",
            "Epoch:487 Training loss=3.2207040935754776 , Validation loss:3.7889420986175537\n",
            "Epoch:488 Training loss=2.4154828786849976 , Validation loss:3.1130608320236206\n",
            "Epoch:489 Training loss=2.8666301369667053 , Validation loss:3.665585517883301\n",
            "Epoch:490 Training loss=2.97861011326313 , Validation loss:3.777470350265503\n",
            "Epoch:491 Training loss=3.097513571381569 , Validation loss:3.3147537112236023\n",
            "Epoch:492 Training loss=3.0354793667793274 , Validation loss:3.696523368358612\n",
            "Epoch:493 Training loss=2.584646984934807 , Validation loss:3.986406087875366\n",
            "Epoch:494 Training loss=3.1662524342536926 , Validation loss:3.592765152454376\n",
            "Epoch:495 Training loss=2.7104461044073105 , Validation loss:3.6507257223129272\n",
            "Epoch:496 Training loss=2.820802092552185 , Validation loss:3.786309540271759\n",
            "Epoch:497 Training loss=2.6491740345954895 , Validation loss:3.7461883425712585\n",
            "Epoch:498 Training loss=2.8779706209897995 , Validation loss:3.392536759376526\n",
            "Epoch:499 Training loss=2.9208126217126846 , Validation loss:3.6265954971313477\n",
            "Epoch:500 Training loss=3.3477021902799606 , Validation loss:3.8185186982154846\n",
            "Epoch:501 Training loss=2.659358963370323 , Validation loss:3.1170437335968018\n",
            "Epoch:502 Training loss=3.125807210803032 , Validation loss:3.204072654247284\n",
            "Epoch:503 Training loss=2.6893009170889854 , Validation loss:4.368664562702179\n",
            "Epoch:504 Training loss=2.677616372704506 , Validation loss:3.2622271478176117\n",
            "Epoch:505 Training loss=2.4243603721261024 , Validation loss:3.1814587116241455\n",
            "Epoch:506 Training loss=2.802302211523056 , Validation loss:4.1571789383888245\n",
            "Epoch:507 Training loss=2.6866324990987778 , Validation loss:3.497214198112488\n",
            "Epoch:508 Training loss=3.436853900551796 , Validation loss:3.4058305621147156\n",
            "Epoch:509 Training loss=3.0925500690937042 , Validation loss:3.439400315284729\n",
            "Epoch:510 Training loss=2.6938609182834625 , Validation loss:4.067275285720825\n",
            "Epoch:511 Training loss=2.164138436317444 , Validation loss:4.67532604932785\n",
            "Epoch:512 Training loss=2.8164270520210266 , Validation loss:3.9635814428329468\n",
            "Epoch:513 Training loss=2.938724532723427 , Validation loss:3.5036149322986603\n",
            "Epoch:514 Training loss=2.6590253859758377 , Validation loss:3.654041588306427\n",
            "Epoch:515 Training loss=2.74554879963398 , Validation loss:3.1809370815753937\n",
            "Epoch:516 Training loss=3.01370170712471 , Validation loss:4.445350289344788\n",
            "Epoch:517 Training loss=2.369383931159973 , Validation loss:3.257363796234131\n",
            "Epoch:518 Training loss=2.8107413351535797 , Validation loss:3.0672110319137573\n",
            "Epoch:519 Training loss=2.8078679740428925 , Validation loss:3.879030466079712\n",
            "Epoch:520 Training loss=2.687538594007492 , Validation loss:3.5417290925979614\n",
            "Epoch:521 Training loss=2.3594766557216644 , Validation loss:4.0299171805381775\n",
            "Epoch:522 Training loss=2.4209868386387825 , Validation loss:4.26134592294693\n",
            "Epoch:523 Training loss=2.426056995987892 , Validation loss:3.8878631591796875\n",
            "Epoch:524 Training loss=2.2775067687034607 , Validation loss:4.009462416172028\n",
            "Epoch:525 Training loss=2.7089484483003616 , Validation loss:3.407526671886444\n",
            "Epoch:526 Training loss=2.34900563955307 , Validation loss:3.1767485439777374\n",
            "Epoch:527 Training loss=2.649367466568947 , Validation loss:3.765220284461975\n",
            "Epoch:528 Training loss=2.770813927054405 , Validation loss:3.823354482650757\n",
            "Epoch:529 Training loss=2.494072876870632 , Validation loss:3.2768911123275757\n",
            "Epoch:530 Training loss=2.8163713961839676 , Validation loss:3.966651439666748\n",
            "Epoch:531 Training loss=2.433919444680214 , Validation loss:3.5982049107551575\n",
            "Epoch:532 Training loss=2.3893997967243195 , Validation loss:3.525790184736252\n",
            "Epoch:533 Training loss=2.717220589518547 , Validation loss:3.7417080402374268\n",
            "Epoch:534 Training loss=2.298055738210678 , Validation loss:3.2899350821971893\n",
            "Epoch:535 Training loss=2.4554857909679413 , Validation loss:4.040472984313965\n",
            "Epoch:536 Training loss=2.469433292746544 , Validation loss:4.271875739097595\n",
            "Epoch:537 Training loss=2.5987997949123383 , Validation loss:3.7428829073905945\n",
            "Epoch:538 Training loss=2.572451636195183 , Validation loss:3.467189610004425\n",
            "Epoch:539 Training loss=2.670088469982147 , Validation loss:3.5024911761283875\n",
            "Epoch:540 Training loss=2.3378393948078156 , Validation loss:3.74856436252594\n",
            "Epoch:541 Training loss=2.4802088141441345 , Validation loss:3.5483022332191467\n",
            "Epoch:542 Training loss=2.5803835839033127 , Validation loss:3.584478974342346\n",
            "Epoch:543 Training loss=2.90651173889637 , Validation loss:4.695995926856995\n",
            "Epoch:544 Training loss=2.6465486735105515 , Validation loss:3.4406301379203796\n",
            "Epoch:545 Training loss=2.1799233108758926 , Validation loss:3.8763250708580017\n",
            "Epoch:546 Training loss=2.4286236464977264 , Validation loss:4.062260210514069\n",
            "Epoch:547 Training loss=3.018361657857895 , Validation loss:3.7131107449531555\n",
            "Epoch:548 Training loss=2.6380603164434433 , Validation loss:3.1967922151088715\n",
            "Epoch:549 Training loss=2.2810334637761116 , Validation loss:3.160439431667328\n",
            "Epoch:550 Training loss=3.0710714906454086 , Validation loss:4.270597875118256\n",
            "Epoch:551 Training loss=2.5576092898845673 , Validation loss:3.2564990520477295\n",
            "Epoch:552 Training loss=2.445214346051216 , Validation loss:3.877868354320526\n",
            "Epoch:553 Training loss=1.9942090436816216 , Validation loss:3.5961452424526215\n",
            "Epoch:554 Training loss=2.626556396484375 , Validation loss:3.0691282153129578\n",
            "Epoch:555 Training loss=2.8739410638809204 , Validation loss:3.312543570995331\n",
            "Epoch:556 Training loss=2.6826945692300797 , Validation loss:3.6478496193885803\n",
            "Epoch:557 Training loss=2.312319338321686 , Validation loss:3.2979201078414917\n",
            "Epoch:558 Training loss=2.283514052629471 , Validation loss:3.612205684185028\n",
            "Epoch:559 Training loss=2.7072553485631943 , Validation loss:3.6957411766052246\n",
            "Epoch:560 Training loss=1.9806683883070946 , Validation loss:3.575249493122101\n",
            "Epoch:561 Training loss=2.430804058909416 , Validation loss:3.88240385055542\n",
            "Epoch:562 Training loss=2.682859353721142 , Validation loss:3.679541230201721\n",
            "Epoch:563 Training loss=2.117311045527458 , Validation loss:3.1699941754341125\n",
            "Epoch:564 Training loss=2.5258902609348297 , Validation loss:3.981174349784851\n",
            "Epoch:565 Training loss=2.585501655936241 , Validation loss:3.4836592972278595\n",
            "Epoch:566 Training loss=2.490806072950363 , Validation loss:3.3738166093826294\n",
            "Epoch:567 Training loss=2.198634088039398 , Validation loss:3.2891541123390198\n",
            "Epoch:568 Training loss=2.4130634143948555 , Validation loss:3.4530727863311768\n",
            "Epoch:569 Training loss=2.0874681174755096 , Validation loss:4.690304934978485\n",
            "Epoch:570 Training loss=1.9311524331569672 , Validation loss:3.5099676847457886\n",
            "Epoch:571 Training loss=2.756659522652626 , Validation loss:4.978823781013489\n",
            "Epoch:572 Training loss=2.1272417530417442 , Validation loss:3.325278341770172\n",
            "Epoch:573 Training loss=2.384068861603737 , Validation loss:3.0051628947257996\n",
            "Epoch:574 Training loss=1.9495563581585884 , Validation loss:4.148142635822296\n",
            "Epoch:575 Training loss=3.083664357662201 , Validation loss:3.5392026901245117\n",
            "Epoch:576 Training loss=2.231294870376587 , Validation loss:4.012228608131409\n",
            "Epoch:577 Training loss=2.4595105946063995 , Validation loss:3.6197021305561066\n",
            "Epoch:578 Training loss=2.9353553503751755 , Validation loss:3.659138321876526\n",
            "Epoch:579 Training loss=2.436176151037216 , Validation loss:3.6695698499679565\n",
            "Epoch:580 Training loss=2.426861435174942 , Validation loss:3.955332398414612\n",
            "Epoch:581 Training loss=2.431490644812584 , Validation loss:3.511866807937622\n",
            "Epoch:582 Training loss=2.7565714567899704 , Validation loss:4.316144824028015\n",
            "Epoch:583 Training loss=2.3025088906288147 , Validation loss:3.206563025712967\n",
            "Epoch:584 Training loss=2.294704705476761 , Validation loss:3.079682946205139\n",
            "Epoch:585 Training loss=1.9562052711844444 , Validation loss:3.867985814809799\n",
            "Epoch:586 Training loss=2.3440148383378983 , Validation loss:4.189915686845779\n",
            "Epoch:587 Training loss=2.02366304397583 , Validation loss:3.7863877415657043\n",
            "Epoch:588 Training loss=2.3974196389317513 , Validation loss:3.0660414695739746\n",
            "Epoch:589 Training loss=2.4294715225696564 , Validation loss:4.320950150489807\n",
            "Epoch:590 Training loss=2.276159282773733 , Validation loss:4.233307123184204\n",
            "Epoch:591 Training loss=2.3892025500535965 , Validation loss:3.779339909553528\n",
            "Epoch:592 Training loss=2.2742270827293396 , Validation loss:3.2699645459651947\n",
            "Epoch:593 Training loss=2.0413234680891037 , Validation loss:4.040222585201263\n",
            "Epoch:594 Training loss=2.136271297931671 , Validation loss:4.262082040309906\n",
            "Epoch:595 Training loss=1.9645025134086609 , Validation loss:3.9918683767318726\n",
            "Epoch:596 Training loss=2.2646072059869766 , Validation loss:3.580716550350189\n",
            "Epoch:597 Training loss=2.4472383335232735 , Validation loss:3.444150149822235\n",
            "Epoch:598 Training loss=2.159257635474205 , Validation loss:3.9742990732192993\n",
            "Epoch:599 Training loss=2.285475790500641 , Validation loss:3.650260806083679\n",
            "Epoch:600 Training loss=2.365049809217453 , Validation loss:3.2468783855438232\n",
            "Epoch:601 Training loss=2.449475646018982 , Validation loss:4.284269452095032\n",
            "Epoch:602 Training loss=2.166545942425728 , Validation loss:3.714269757270813\n",
            "Epoch:603 Training loss=2.5362841188907623 , Validation loss:3.437355488538742\n",
            "Epoch:604 Training loss=1.7006840258836746 , Validation loss:4.178831160068512\n",
            "Epoch:605 Training loss=2.4595534205436707 , Validation loss:3.1256150901317596\n",
            "Epoch:606 Training loss=2.1644813790917397 , Validation loss:3.1032539010047913\n",
            "Epoch:607 Training loss=2.3953641653060913 , Validation loss:3.4651232957839966\n",
            "Epoch:608 Training loss=2.26298126578331 , Validation loss:4.011004269123077\n",
            "Epoch:609 Training loss=2.503380596637726 , Validation loss:3.872111141681671\n",
            "Epoch:610 Training loss=1.9847158417105675 , Validation loss:3.5381397008895874\n",
            "Epoch:611 Training loss=1.9335041046142578 , Validation loss:3.050336867570877\n",
            "Epoch:612 Training loss=2.2312452495098114 , Validation loss:3.740431487560272\n",
            "Epoch:613 Training loss=2.419170156121254 , Validation loss:2.9665160477161407\n",
            "Epoch:614 Training loss=2.0829059928655624 , Validation loss:2.6095626950263977\n",
            "Epoch:615 Training loss=2.4833660423755646 , Validation loss:3.2247493267059326\n",
            "Epoch:616 Training loss=2.10189488530159 , Validation loss:3.7784544229507446\n",
            "Epoch:617 Training loss=2.238299071788788 , Validation loss:3.8248817324638367\n",
            "Epoch:618 Training loss=2.400866821408272 , Validation loss:4.085984766483307\n",
            "Epoch:619 Training loss=1.961251713335514 , Validation loss:3.862809956073761\n",
            "Epoch:620 Training loss=2.205635294318199 , Validation loss:4.0651989579200745\n",
            "Epoch:621 Training loss=2.36103293299675 , Validation loss:3.459414839744568\n",
            "Epoch:622 Training loss=2.449666365981102 , Validation loss:3.7588579654693604\n",
            "Epoch:623 Training loss=2.393231824040413 , Validation loss:3.1910457015037537\n",
            "Epoch:624 Training loss=2.205170512199402 , Validation loss:3.253411889076233\n",
            "Epoch:625 Training loss=1.918639674782753 , Validation loss:3.233372151851654\n",
            "Epoch:626 Training loss=2.525502622127533 , Validation loss:3.374065577983856\n",
            "Epoch:627 Training loss=2.4108894765377045 , Validation loss:3.682058721780777\n",
            "Epoch:628 Training loss=1.7631731480360031 , Validation loss:3.4706152081489563\n",
            "Epoch:629 Training loss=2.31655053794384 , Validation loss:3.682336449623108\n",
            "Epoch:630 Training loss=1.5941662043333054 , Validation loss:3.797491192817688\n",
            "Epoch:631 Training loss=1.9015174135565758 , Validation loss:3.394311308860779\n",
            "Epoch:632 Training loss=3.11309265345335 , Validation loss:3.509279489517212\n",
            "Epoch:633 Training loss=1.9248622059822083 , Validation loss:3.2831438183784485\n",
            "Epoch:634 Training loss=2.3809130638837814 , Validation loss:3.584104508161545\n",
            "Epoch:635 Training loss=2.184020809829235 , Validation loss:3.5577842593193054\n",
            "Epoch:636 Training loss=2.404627203941345 , Validation loss:3.8586980998516083\n",
            "Epoch:637 Training loss=1.996843382716179 , Validation loss:3.229710578918457\n",
            "Epoch:638 Training loss=1.8581081554293633 , Validation loss:2.9745697379112244\n",
            "Epoch:639 Training loss=2.3136578127741814 , Validation loss:3.8246275186538696\n",
            "Epoch:640 Training loss=2.1212286576628685 , Validation loss:3.8469198048114777\n",
            "Epoch:641 Training loss=2.157457187771797 , Validation loss:3.9170950651168823\n",
            "Epoch:642 Training loss=2.400099888443947 , Validation loss:3.6180997490882874\n",
            "Epoch:643 Training loss=2.4757415130734444 , Validation loss:3.9755833745002747\n",
            "Epoch:644 Training loss=2.136746793985367 , Validation loss:3.7181299030780792\n",
            "Epoch:645 Training loss=2.055281698703766 , Validation loss:3.72288578748703\n",
            "Epoch:646 Training loss=1.9070290327072144 , Validation loss:4.113621890544891\n",
            "Epoch:647 Training loss=2.123921200633049 , Validation loss:3.8032907247543335\n",
            "Epoch:648 Training loss=2.3593636825680733 , Validation loss:3.619594931602478\n",
            "Epoch:649 Training loss=1.835226833820343 , Validation loss:3.736510753631592\n",
            "Epoch:650 Training loss=1.799794226884842 , Validation loss:3.9647260904312134\n",
            "Epoch:651 Training loss=2.4819133654236794 , Validation loss:3.1710833311080933\n",
            "Epoch:652 Training loss=2.6093206256628036 , Validation loss:4.337571680545807\n",
            "Epoch:653 Training loss=1.5976782739162445 , Validation loss:3.3309274911880493\n",
            "Epoch:654 Training loss=2.2305344492197037 , Validation loss:3.5036025643348694\n",
            "Epoch:655 Training loss=2.4859112054109573 , Validation loss:3.5379111766815186\n",
            "Epoch:656 Training loss=1.7413770779967308 , Validation loss:3.3142484426498413\n",
            "Epoch:657 Training loss=1.9424889832735062 , Validation loss:3.006940335035324\n",
            "Epoch:658 Training loss=2.2089077681303024 , Validation loss:3.7170976400375366\n",
            "Epoch:659 Training loss=1.8112086951732635 , Validation loss:4.0080801248550415\n",
            "Epoch:660 Training loss=2.027838781476021 , Validation loss:3.7327131032943726\n",
            "Epoch:661 Training loss=1.8184182792901993 , Validation loss:3.4991457760334015\n",
            "Epoch:662 Training loss=2.074686974287033 , Validation loss:3.4034858345985413\n",
            "Epoch:663 Training loss=2.0614462792873383 , Validation loss:4.01588374376297\n",
            "Epoch:664 Training loss=2.3043869733810425 , Validation loss:4.448558866977692\n",
            "Epoch:665 Training loss=2.0139342434704304 , Validation loss:3.388893812894821\n",
            "Epoch:666 Training loss=2.162185274064541 , Validation loss:3.408919394016266\n",
            "Epoch:667 Training loss=1.879091627895832 , Validation loss:3.7481497526168823\n",
            "Epoch:668 Training loss=2.275941342115402 , Validation loss:3.274402678012848\n",
            "Epoch:669 Training loss=2.1860237568616867 , Validation loss:2.9227542877197266\n",
            "Epoch:670 Training loss=1.7043240144848824 , Validation loss:3.8984740376472473\n",
            "Epoch:671 Training loss=1.608623430132866 , Validation loss:3.6367525458335876\n",
            "Epoch:672 Training loss=1.9123265221714973 , Validation loss:3.99028617143631\n",
            "Epoch:673 Training loss=2.0637629330158234 , Validation loss:3.934130549430847\n",
            "Epoch:674 Training loss=2.823278307914734 , Validation loss:3.8263121843338013\n",
            "Epoch:675 Training loss=2.5101662427186966 , Validation loss:2.991116613149643\n",
            "Epoch:676 Training loss=1.626063659787178 , Validation loss:4.023223042488098\n",
            "Epoch:677 Training loss=1.8752894401550293 , Validation loss:3.587433934211731\n",
            "Epoch:678 Training loss=2.025229960680008 , Validation loss:3.4581187069416046\n",
            "Epoch:679 Training loss=2.20063753426075 , Validation loss:3.922281563282013\n",
            "Epoch:680 Training loss=3.024939365684986 , Validation loss:3.2818145751953125\n",
            "Epoch:681 Training loss=2.4881056770682335 , Validation loss:2.9028470516204834\n",
            "Epoch:682 Training loss=1.8837788105010986 , Validation loss:3.5946442782878876\n",
            "Epoch:683 Training loss=2.141991652548313 , Validation loss:3.635168194770813\n",
            "Epoch:684 Training loss=1.8158401548862457 , Validation loss:4.064341366291046\n",
            "Epoch:685 Training loss=2.00125652551651 , Validation loss:3.2237799167633057\n",
            "Epoch:686 Training loss=1.906672403216362 , Validation loss:3.3348713517189026\n",
            "Epoch:687 Training loss=1.8797868862748146 , Validation loss:3.042847752571106\n",
            "Epoch:688 Training loss=2.3716610074043274 , Validation loss:3.0756437927484512\n",
            "Epoch:689 Training loss=1.9387114495038986 , Validation loss:3.3968779742717743\n",
            "Epoch:690 Training loss=1.6195140182971954 , Validation loss:4.027001082897186\n",
            "Epoch:691 Training loss=2.0399773120880127 , Validation loss:3.617084324359894\n",
            "Epoch:692 Training loss=1.5833869650959969 , Validation loss:3.139727622270584\n",
            "Epoch:693 Training loss=1.828505516052246 , Validation loss:4.630024194717407\n",
            "Epoch:694 Training loss=1.625201515853405 , Validation loss:3.646824300289154\n",
            "Epoch:695 Training loss=1.8783441931009293 , Validation loss:3.2465654015541077\n",
            "Epoch:696 Training loss=1.8359083831310272 , Validation loss:3.0876739621162415\n",
            "Epoch:697 Training loss=1.8952758386731148 , Validation loss:3.1175359785556793\n",
            "Epoch:698 Training loss=2.334413602948189 , Validation loss:3.6431058943271637\n",
            "Epoch:699 Training loss=1.7984916046261787 , Validation loss:3.3409530222415924\n",
            "Epoch:700 Training loss=2.1162759363651276 , Validation loss:4.399163544178009\n",
            "Epoch:701 Training loss=2.12240631878376 , Validation loss:3.39748078584671\n",
            "Epoch:702 Training loss=1.9585721120238304 , Validation loss:3.259703904390335\n",
            "Epoch:703 Training loss=2.2096346765756607 , Validation loss:3.5040000081062317\n",
            "Epoch:704 Training loss=1.81152805685997 , Validation loss:4.401751518249512\n",
            "Epoch:705 Training loss=2.24546454846859 , Validation loss:3.915924549102783\n",
            "Epoch:706 Training loss=1.9076409414410591 , Validation loss:3.6541322469711304\n",
            "Epoch:707 Training loss=1.662203922867775 , Validation loss:3.54658043384552\n",
            "Epoch:708 Training loss=1.9688334316015244 , Validation loss:3.433833420276642\n",
            "Epoch:709 Training loss=1.9557769000530243 , Validation loss:3.9020330905914307\n",
            "Epoch:710 Training loss=1.9265238344669342 , Validation loss:3.8691308796405792\n",
            "Epoch:711 Training loss=1.6748107895255089 , Validation loss:3.5689501762390137\n",
            "Epoch:712 Training loss=1.8389375135302544 , Validation loss:3.3239359259605408\n",
            "Epoch:713 Training loss=2.05810210108757 , Validation loss:3.7470562160015106\n",
            "Epoch:714 Training loss=1.9302360638976097 , Validation loss:3.4006946235895157\n",
            "Epoch:715 Training loss=1.8074299097061157 , Validation loss:3.173177480697632\n",
            "Epoch:716 Training loss=2.308300942182541 , Validation loss:3.732931137084961\n",
            "Epoch:717 Training loss=2.0104430839419365 , Validation loss:3.8917123079299927\n",
            "Epoch:718 Training loss=2.1991328448057175 , Validation loss:3.1222027838230133\n",
            "Epoch:719 Training loss=1.5380563139915466 , Validation loss:3.4783615469932556\n",
            "Epoch:720 Training loss=1.2896953374147415 , Validation loss:3.7765765488147736\n",
            "Epoch:721 Training loss=2.0035339891910553 , Validation loss:2.973495990037918\n",
            "Epoch:722 Training loss=1.801675133407116 , Validation loss:3.3518282771110535\n",
            "Epoch:723 Training loss=1.9418274983763695 , Validation loss:3.9305317401885986\n",
            "Epoch:724 Training loss=1.5671836882829666 , Validation loss:3.6604421734809875\n",
            "Epoch:725 Training loss=2.1543945893645287 , Validation loss:4.397176742553711\n",
            "Epoch:726 Training loss=1.8572422564029694 , Validation loss:4.017839729785919\n",
            "Epoch:727 Training loss=1.6787514314055443 , Validation loss:3.5382691025733948\n",
            "Epoch:728 Training loss=1.8668465912342072 , Validation loss:3.886568248271942\n",
            "Epoch:729 Training loss=2.6574610099196434 , Validation loss:3.87045019865036\n",
            "Epoch:730 Training loss=2.068920724093914 , Validation loss:3.8038086593151093\n",
            "Epoch:731 Training loss=2.055038742721081 , Validation loss:4.520077645778656\n",
            "Epoch:732 Training loss=1.9860680103302002 , Validation loss:3.424155294895172\n",
            "Epoch:733 Training loss=2.160172611474991 , Validation loss:4.089666664600372\n",
            "Epoch:734 Training loss=1.4236258417367935 , Validation loss:3.4602437019348145\n",
            "Epoch:735 Training loss=1.6402647197246552 , Validation loss:5.07857358455658\n",
            "Epoch:736 Training loss=1.6524278298020363 , Validation loss:3.056845337152481\n",
            "Epoch:737 Training loss=2.3169756829738617 , Validation loss:3.6887916326522827\n",
            "Epoch:738 Training loss=2.0854925736784935 , Validation loss:3.6361724138259888\n",
            "Epoch:739 Training loss=2.3500903993844986 , Validation loss:3.2656859755516052\n",
            "Epoch:740 Training loss=2.373430550098419 , Validation loss:4.425043106079102\n",
            "Epoch:741 Training loss=1.9881857372820377 , Validation loss:2.450306087732315\n",
            "Epoch:742 Training loss=1.9077044054865837 , Validation loss:3.5343568921089172\n",
            "Epoch:743 Training loss=1.7255458310246468 , Validation loss:3.9498775005340576\n",
            "Epoch:744 Training loss=1.765134260058403 , Validation loss:3.8787622451782227\n",
            "Epoch:745 Training loss=2.1402072682976723 , Validation loss:4.198979675769806\n",
            "Epoch:746 Training loss=1.8792699128389359 , Validation loss:3.5333366990089417\n",
            "Epoch:747 Training loss=2.075578451156616 , Validation loss:3.7015128135681152\n",
            "Epoch:748 Training loss=2.289731666445732 , Validation loss:4.13106244802475\n",
            "Epoch:749 Training loss=2.317505344748497 , Validation loss:4.779749155044556\n",
            "Epoch:750 Training loss=1.8122564256191254 , Validation loss:3.0226394534111023\n",
            "Epoch:751 Training loss=1.6910860538482666 , Validation loss:4.138346016407013\n",
            "Epoch:752 Training loss=1.3385543450713158 , Validation loss:3.8935155272483826\n",
            "Epoch:753 Training loss=1.715227797627449 , Validation loss:3.6398156583309174\n",
            "Epoch:754 Training loss=2.068282589316368 , Validation loss:3.5446953177452087\n",
            "Epoch:755 Training loss=1.7409214228391647 , Validation loss:3.2699814289808273\n",
            "Epoch:756 Training loss=1.8618666306138039 , Validation loss:3.095865309238434\n",
            "Epoch:757 Training loss=1.7281274124979973 , Validation loss:3.5804103016853333\n",
            "Epoch:758 Training loss=1.8088290691375732 , Validation loss:3.5619879364967346\n",
            "Epoch:759 Training loss=1.6021746546030045 , Validation loss:4.088515639305115\n",
            "Epoch:760 Training loss=1.8422628939151764 , Validation loss:3.3640263080596924\n",
            "Epoch:761 Training loss=1.429767593741417 , Validation loss:3.5598565340042114\n",
            "Epoch:762 Training loss=1.3185896575450897 , Validation loss:4.312856316566467\n",
            "Epoch:763 Training loss=1.743891641497612 , Validation loss:4.121775507926941\n",
            "Epoch:764 Training loss=1.5979179963469505 , Validation loss:3.7886516749858856\n",
            "Epoch:765 Training loss=1.9852548725903034 , Validation loss:3.903526872396469\n",
            "Epoch:766 Training loss=1.872363343834877 , Validation loss:4.441109210252762\n",
            "Epoch:767 Training loss=1.9273816645145416 , Validation loss:3.353227198123932\n",
            "Epoch:768 Training loss=1.6094316393136978 , Validation loss:4.318307280540466\n",
            "Epoch:769 Training loss=1.6200610101222992 , Validation loss:4.685508489608765\n",
            "Epoch:770 Training loss=1.454038068652153 , Validation loss:3.794037088751793\n",
            "Epoch:771 Training loss=1.80493725836277 , Validation loss:3.2607905864715576\n",
            "Epoch:772 Training loss=1.8428376615047455 , Validation loss:3.411263585090637\n",
            "Epoch:773 Training loss=1.5641685873270035 , Validation loss:4.38264787197113\n",
            "Epoch:774 Training loss=1.4701042771339417 , Validation loss:3.2833332121372223\n",
            "Epoch:775 Training loss=2.07433270663023 , Validation loss:4.246825218200684\n",
            "Epoch:776 Training loss=1.6077449917793274 , Validation loss:3.459120124578476\n",
            "Epoch:777 Training loss=1.2213994339108467 , Validation loss:3.279144287109375\n",
            "Epoch:778 Training loss=1.5832510739564896 , Validation loss:4.372806131839752\n",
            "Epoch:779 Training loss=1.8960170969367027 , Validation loss:2.334000527858734\n",
            "Epoch:780 Training loss=1.8558243960142136 , Validation loss:4.048637092113495\n",
            "Epoch:781 Training loss=1.872685745358467 , Validation loss:3.703415632247925\n",
            "Epoch:782 Training loss=1.8811642527580261 , Validation loss:3.414452850818634\n",
            "Epoch:783 Training loss=2.0294456109404564 , Validation loss:3.0851043462753296\n",
            "Epoch:784 Training loss=1.8996106088161469 , Validation loss:4.124069094657898\n",
            "Epoch:785 Training loss=1.7955744788050652 , Validation loss:3.2380495369434357\n",
            "Epoch:786 Training loss=1.7030197829008102 , Validation loss:4.112723410129547\n",
            "Epoch:787 Training loss=1.3546135872602463 , Validation loss:3.732767701148987\n",
            "Epoch:788 Training loss=1.5008847415447235 , Validation loss:3.3931256234645844\n",
            "Epoch:789 Training loss=1.7016067579388618 , Validation loss:3.099558413028717\n",
            "Epoch:790 Training loss=1.4719485715031624 , Validation loss:3.73981112241745\n",
            "Epoch:791 Training loss=1.4038235396146774 , Validation loss:3.36317977309227\n",
            "Epoch:792 Training loss=1.5750716924667358 , Validation loss:3.217068135738373\n",
            "Epoch:793 Training loss=1.7510991021990776 , Validation loss:3.189147174358368\n",
            "Epoch:794 Training loss=2.204872366040945 , Validation loss:3.475588411092758\n",
            "Epoch:795 Training loss=1.4823856577277184 , Validation loss:3.4591749608516693\n",
            "Epoch:796 Training loss=1.7397266328334808 , Validation loss:3.775297224521637\n",
            "Epoch:797 Training loss=2.0818981379270554 , Validation loss:3.177437871694565\n",
            "Epoch:798 Training loss=1.7121604681015015 , Validation loss:3.952725827693939\n",
            "Epoch:799 Training loss=1.7394522055983543 , Validation loss:3.4611388742923737\n",
            "Epoch:800 Training loss=1.5603640675544739 , Validation loss:3.6188386976718903\n",
            "Epoch:801 Training loss=1.4239164516329765 , Validation loss:3.5362733006477356\n",
            "Epoch:802 Training loss=1.686405897140503 , Validation loss:4.124581813812256\n",
            "Epoch:803 Training loss=1.5172066390514374 , Validation loss:3.9793378114700317\n",
            "Epoch:804 Training loss=1.596005767583847 , Validation loss:3.753767728805542\n",
            "Epoch:805 Training loss=1.3070513680577278 , Validation loss:3.3763484954833984\n",
            "Epoch:806 Training loss=1.8728831857442856 , Validation loss:4.890519559383392\n",
            "Epoch:807 Training loss=1.8944985568523407 , Validation loss:4.958125472068787\n",
            "Epoch:808 Training loss=1.8143495619297028 , Validation loss:3.4015273451805115\n",
            "Epoch:809 Training loss=1.6613513007760048 , Validation loss:4.132540643215179\n",
            "Epoch:810 Training loss=1.532453492283821 , Validation loss:3.452045977115631\n",
            "Epoch:811 Training loss=1.507948413491249 , Validation loss:2.554790437221527\n",
            "Epoch:812 Training loss=1.7570719718933105 , Validation loss:3.065984785556793\n",
            "Epoch:813 Training loss=1.5930421464145184 , Validation loss:3.0887982547283173\n",
            "Epoch:814 Training loss=1.8676539808511734 , Validation loss:4.098446220159531\n",
            "Epoch:815 Training loss=2.0118805915117264 , Validation loss:3.1136908531188965\n",
            "Epoch:816 Training loss=1.7897417694330215 , Validation loss:3.2080758213996887\n",
            "Epoch:817 Training loss=1.374169982969761 , Validation loss:2.95200052857399\n",
            "Epoch:818 Training loss=1.6994533389806747 , Validation loss:2.693492352962494\n",
            "Epoch:819 Training loss=1.4987711906433105 , Validation loss:3.257555663585663\n",
            "Epoch:820 Training loss=1.4947922825813293 , Validation loss:3.124915063381195\n",
            "Epoch:821 Training loss=2.2513057589530945 , Validation loss:3.1455059945583344\n",
            "Epoch:822 Training loss=1.6649879217147827 , Validation loss:3.2982934713363647\n",
            "Epoch:823 Training loss=1.5423743948340416 , Validation loss:3.5997523069381714\n",
            "Epoch:824 Training loss=2.627082444727421 , Validation loss:3.528717339038849\n",
            "Epoch:825 Training loss=1.3104141540825367 , Validation loss:2.8800401389598846\n",
            "Epoch:826 Training loss=1.7087605446577072 , Validation loss:2.9202016592025757\n",
            "Epoch:827 Training loss=1.8680331408977509 , Validation loss:2.866662383079529\n",
            "Epoch:828 Training loss=1.3934031128883362 , Validation loss:3.8482731580734253\n",
            "Epoch:829 Training loss=1.153673529624939 , Validation loss:3.8683244585990906\n",
            "Epoch:830 Training loss=1.6881707310676575 , Validation loss:4.02601033449173\n",
            "Epoch:831 Training loss=1.4881668090820312 , Validation loss:3.124434381723404\n",
            "Epoch:832 Training loss=1.3013410717248917 , Validation loss:3.7208995521068573\n",
            "Epoch:833 Training loss=1.4425527974963188 , Validation loss:3.6896777749061584\n",
            "Epoch:834 Training loss=1.6510734558105469 , Validation loss:2.7736401855945587\n",
            "Epoch:835 Training loss=1.079357035458088 , Validation loss:4.500715523958206\n",
            "Epoch:836 Training loss=1.5970263630151749 , Validation loss:4.394157230854034\n",
            "Epoch:837 Training loss=1.6767243444919586 , Validation loss:3.1993974149227142\n",
            "Epoch:838 Training loss=1.6605192571878433 , Validation loss:3.000313311815262\n",
            "Epoch:839 Training loss=2.273447871208191 , Validation loss:3.261649340391159\n",
            "Epoch:840 Training loss=1.1515941843390465 , Validation loss:3.6919418573379517\n",
            "Epoch:841 Training loss=1.0803033709526062 , Validation loss:3.154225170612335\n",
            "Epoch:842 Training loss=1.3388417139649391 , Validation loss:3.221298933029175\n",
            "Epoch:843 Training loss=1.3444111198186874 , Validation loss:3.5628098249435425\n",
            "Epoch:844 Training loss=1.2859674617648125 , Validation loss:3.840143382549286\n",
            "Epoch:845 Training loss=1.5078430771827698 , Validation loss:2.9747573286294937\n",
            "Epoch:846 Training loss=1.4624674394726753 , Validation loss:2.9805907011032104\n",
            "Epoch:847 Training loss=1.545868881046772 , Validation loss:3.0524603128433228\n",
            "Epoch:848 Training loss=1.673600159585476 , Validation loss:3.5809627175331116\n",
            "Epoch:849 Training loss=1.6685581281781197 , Validation loss:4.015090048313141\n",
            "Epoch:850 Training loss=1.2353021129965782 , Validation loss:3.3783501982688904\n",
            "Epoch:851 Training loss=1.2768894135951996 , Validation loss:3.7190889418125153\n",
            "Epoch:852 Training loss=1.4862549304962158 , Validation loss:3.879733145236969\n",
            "Epoch:853 Training loss=1.226573184132576 , Validation loss:3.0587067306041718\n",
            "Epoch:854 Training loss=2.262944884598255 , Validation loss:3.9539613723754883\n",
            "Epoch:855 Training loss=1.6327250599861145 , Validation loss:3.3450962603092194\n",
            "Epoch:856 Training loss=1.1923339664936066 , Validation loss:3.302333176136017\n",
            "Epoch:857 Training loss=1.4580879509449005 , Validation loss:3.672961473464966\n",
            "Epoch:858 Training loss=1.4876179322600365 , Validation loss:3.273418962955475\n",
            "Epoch:859 Training loss=1.6687870621681213 , Validation loss:4.476373076438904\n",
            "Epoch:860 Training loss=1.4496623873710632 , Validation loss:4.055107861757278\n",
            "Epoch:861 Training loss=1.4132940843701363 , Validation loss:2.558480590581894\n",
            "Epoch:862 Training loss=1.771798975765705 , Validation loss:3.7214642763137817\n",
            "Epoch:863 Training loss=1.8238507956266403 , Validation loss:3.9352768063545227\n",
            "Epoch:864 Training loss=1.5769880190491676 , Validation loss:4.2400460839271545\n",
            "Epoch:865 Training loss=1.6554206907749176 , Validation loss:4.246084094047546\n",
            "Epoch:866 Training loss=1.3207827284932137 , Validation loss:3.758662760257721\n",
            "Epoch:867 Training loss=2.089079797267914 , Validation loss:3.075257033109665\n",
            "Epoch:868 Training loss=1.6473086923360825 , Validation loss:2.825902223587036\n",
            "Epoch:869 Training loss=1.3568355813622475 , Validation loss:4.215548276901245\n",
            "Epoch:870 Training loss=1.4172010347247124 , Validation loss:4.412546753883362\n",
            "Epoch:871 Training loss=1.3600214794278145 , Validation loss:3.7812184393405914\n",
            "Epoch:872 Training loss=1.3102828487753868 , Validation loss:3.229462891817093\n",
            "Epoch:873 Training loss=1.0603336170315742 , Validation loss:3.300230026245117\n",
            "Epoch:874 Training loss=1.0617720410227776 , Validation loss:4.055099070072174\n",
            "Epoch:875 Training loss=1.1145829372107983 , Validation loss:3.28280907869339\n",
            "Epoch:876 Training loss=1.236227661371231 , Validation loss:3.197320744395256\n",
            "Epoch:877 Training loss=1.2818194180727005 , Validation loss:3.080304592847824\n",
            "Epoch:878 Training loss=1.4673471301794052 , Validation loss:2.9421672224998474\n",
            "Epoch:879 Training loss=1.8681676015257835 , Validation loss:3.4144411236047745\n",
            "Epoch:880 Training loss=1.3741117119789124 , Validation loss:2.877508908510208\n",
            "Epoch:881 Training loss=1.7086728736758232 , Validation loss:4.004057347774506\n",
            "Epoch:882 Training loss=1.5459768623113632 , Validation loss:4.144612789154053\n",
            "Epoch:883 Training loss=1.7860265523195267 , Validation loss:3.7888975739479065\n",
            "Epoch:884 Training loss=1.1946442499756813 , Validation loss:3.6819134652614594\n",
            "Epoch:885 Training loss=1.5140224248170853 , Validation loss:3.194055914878845\n",
            "Epoch:886 Training loss=2.049865923821926 , Validation loss:3.3355621099472046\n",
            "Epoch:887 Training loss=1.6562022864818573 , Validation loss:3.1443440467119217\n",
            "Epoch:888 Training loss=1.0487694516777992 , Validation loss:3.40908345580101\n",
            "Epoch:889 Training loss=1.5954187586903572 , Validation loss:3.0269483625888824\n",
            "Epoch:890 Training loss=1.39918202906847 , Validation loss:3.065260887145996\n",
            "Epoch:891 Training loss=1.4774819016456604 , Validation loss:3.364496946334839\n",
            "Epoch:892 Training loss=1.6240940317511559 , Validation loss:3.9665234982967377\n",
            "Epoch:893 Training loss=1.6410027891397476 , Validation loss:4.578098803758621\n",
            "Epoch:894 Training loss=1.4091471880674362 , Validation loss:3.2266785502433777\n",
            "Epoch:895 Training loss=1.383150115609169 , Validation loss:3.424778074026108\n",
            "Epoch:896 Training loss=2.327783443033695 , Validation loss:3.6911480128765106\n",
            "Epoch:897 Training loss=1.6769935712218285 , Validation loss:3.7044180035591125\n",
            "Epoch:898 Training loss=1.3009749688208103 , Validation loss:3.0812285095453262\n",
            "Epoch:899 Training loss=1.1410930082201958 , Validation loss:3.280181884765625\n",
            "Epoch:900 Training loss=1.1254435367882252 , Validation loss:3.2050876021385193\n",
            "Epoch:901 Training loss=1.065383680164814 , Validation loss:3.6453062295913696\n",
            "Epoch:902 Training loss=1.7201469391584396 , Validation loss:3.7514079809188843\n",
            "Epoch:903 Training loss=1.5182495787739754 , Validation loss:2.5281371474266052\n",
            "Epoch:904 Training loss=1.826915591955185 , Validation loss:3.7002034187316895\n",
            "Epoch:905 Training loss=1.6595793813467026 , Validation loss:3.415188193321228\n",
            "Epoch:906 Training loss=1.9064356982707977 , Validation loss:4.466399371623993\n",
            "Epoch:907 Training loss=1.4965320527553558 , Validation loss:3.2586266100406647\n",
            "Epoch:908 Training loss=1.6899032816290855 , Validation loss:4.290983617305756\n",
            "Epoch:909 Training loss=1.4363027438521385 , Validation loss:3.6342833042144775\n",
            "Epoch:910 Training loss=1.3087052032351494 , Validation loss:2.8433856070041656\n",
            "Epoch:911 Training loss=1.3002664148807526 , Validation loss:4.185943305492401\n",
            "Epoch:912 Training loss=1.1474301367998123 , Validation loss:3.9257939755916595\n",
            "Epoch:913 Training loss=1.462979294359684 , Validation loss:3.95054692029953\n",
            "Epoch:914 Training loss=1.3734222650527954 , Validation loss:2.949705809354782\n",
            "Epoch:915 Training loss=1.1418481841683388 , Validation loss:3.397521808743477\n",
            "Epoch:916 Training loss=1.213301181793213 , Validation loss:2.920427590608597\n",
            "Epoch:917 Training loss=1.3844740986824036 , Validation loss:3.408438742160797\n",
            "Epoch:918 Training loss=1.509828969836235 , Validation loss:2.686119854450226\n",
            "Epoch:919 Training loss=1.1438066959381104 , Validation loss:3.9351884722709656\n",
            "Epoch:920 Training loss=1.9936581514775753 , Validation loss:3.9576188027858734\n",
            "Epoch:921 Training loss=1.3374671339988708 , Validation loss:3.592220962047577\n",
            "Epoch:922 Training loss=1.5631264671683311 , Validation loss:3.218301936984062\n",
            "Epoch:923 Training loss=1.6263170316815376 , Validation loss:3.044051706790924\n",
            "Epoch:924 Training loss=1.6691884770989418 , Validation loss:3.3521655201911926\n",
            "Epoch:925 Training loss=1.4370544701814651 , Validation loss:3.6425353586673737\n",
            "Epoch:926 Training loss=1.9806336760520935 , Validation loss:4.652172386646271\n",
            "Epoch:927 Training loss=1.3853274434804916 , Validation loss:4.612558662891388\n",
            "Epoch:928 Training loss=1.5353313609957695 , Validation loss:3.172926038503647\n",
            "Epoch:929 Training loss=1.4625339917838573 , Validation loss:3.3562381863594055\n",
            "Epoch:930 Training loss=1.2680883184075356 , Validation loss:3.5138935446739197\n",
            "Epoch:931 Training loss=1.6564869619905949 , Validation loss:3.978841781616211\n",
            "Epoch:932 Training loss=1.2229323908686638 , Validation loss:3.9570066928863525\n",
            "Epoch:933 Training loss=1.3745848461985588 , Validation loss:3.0353078842163086\n",
            "Epoch:934 Training loss=2.3066464588046074 , Validation loss:3.3456614017486572\n",
            "Epoch:935 Training loss=1.3563936203718185 , Validation loss:3.729498565196991\n",
            "Epoch:936 Training loss=1.5665817558765411 , Validation loss:3.6432396173477173\n",
            "Epoch:937 Training loss=1.1438496261835098 , Validation loss:2.9042493402957916\n",
            "Epoch:938 Training loss=0.8505436033010483 , Validation loss:3.644852340221405\n",
            "Epoch:939 Training loss=1.3575685396790504 , Validation loss:4.006686359643936\n",
            "Epoch:940 Training loss=1.3855918794870377 , Validation loss:4.310105860233307\n",
            "Epoch:941 Training loss=1.7240073010325432 , Validation loss:5.051673859357834\n",
            "Epoch:942 Training loss=1.6282111704349518 , Validation loss:2.9888822436332703\n",
            "Epoch:943 Training loss=1.212057538330555 , Validation loss:3.756481021642685\n",
            "Epoch:944 Training loss=1.672618106007576 , Validation loss:3.5938395261764526\n",
            "Epoch:945 Training loss=1.5713495835661888 , Validation loss:4.165810018777847\n",
            "Epoch:946 Training loss=1.3685657158493996 , Validation loss:2.8176104724407196\n",
            "Epoch:947 Training loss=1.1790684461593628 , Validation loss:3.2821654081344604\n",
            "Epoch:948 Training loss=1.3455891460180283 , Validation loss:3.1145867705345154\n",
            "Epoch:949 Training loss=1.0929183587431908 , Validation loss:4.446061730384827\n",
            "Epoch:950 Training loss=1.262672021985054 , Validation loss:3.530041456222534\n",
            "Epoch:951 Training loss=0.8524557873606682 , Validation loss:3.602858394384384\n",
            "Epoch:952 Training loss=1.1276260167360306 , Validation loss:4.414088577032089\n",
            "Epoch:953 Training loss=1.1581691913306713 , Validation loss:4.190431833267212\n",
            "Epoch:954 Training loss=1.6238176673650742 , Validation loss:3.440296024084091\n",
            "Epoch:955 Training loss=1.150776982307434 , Validation loss:3.7090100049972534\n",
            "Epoch:956 Training loss=1.2276856899261475 , Validation loss:3.833399772644043\n",
            "Epoch:957 Training loss=1.5034526586532593 , Validation loss:4.3719868659973145\n",
            "Epoch:958 Training loss=1.2113525494933128 , Validation loss:3.017667293548584\n",
            "Epoch:959 Training loss=1.6513600572943687 , Validation loss:4.098629653453827\n",
            "Epoch:960 Training loss=1.020118623971939 , Validation loss:4.031510144472122\n",
            "Epoch:961 Training loss=1.56057957559824 , Validation loss:3.3553355932235718\n",
            "Epoch:962 Training loss=0.9290220886468887 , Validation loss:3.226371109485626\n",
            "Epoch:963 Training loss=1.699104867875576 , Validation loss:3.0817911326885223\n",
            "Epoch:964 Training loss=1.1731959357857704 , Validation loss:2.832731246948242\n",
            "Epoch:965 Training loss=1.0471372231841087 , Validation loss:3.58424311876297\n",
            "Epoch:966 Training loss=1.502400666475296 , Validation loss:3.5494287610054016\n",
            "Epoch:967 Training loss=1.3348004072904587 , Validation loss:3.94822159409523\n",
            "Epoch:968 Training loss=1.199777491390705 , Validation loss:3.7376184463500977\n",
            "Epoch:969 Training loss=1.336001843214035 , Validation loss:3.7066665291786194\n",
            "Epoch:970 Training loss=1.1697716042399406 , Validation loss:4.663767457008362\n",
            "Epoch:971 Training loss=1.3128201216459274 , Validation loss:3.0565523207187653\n",
            "Epoch:972 Training loss=1.4783409163355827 , Validation loss:3.998321682214737\n",
            "Epoch:973 Training loss=1.3667297810316086 , Validation loss:3.5157580971717834\n",
            "Epoch:974 Training loss=1.6518050357699394 , Validation loss:3.3164886832237244\n",
            "Epoch:975 Training loss=1.220984935760498 , Validation loss:4.220730423927307\n",
            "Epoch:976 Training loss=1.6397042721509933 , Validation loss:3.953873932361603\n",
            "Epoch:977 Training loss=1.5331691056489944 , Validation loss:2.896456018090248\n",
            "Epoch:978 Training loss=1.36859779804945 , Validation loss:3.4332505762577057\n",
            "Epoch:979 Training loss=1.1493961662054062 , Validation loss:2.9061340391635895\n",
            "Epoch:980 Training loss=1.1017846763134003 , Validation loss:3.6519173681735992\n",
            "Epoch:981 Training loss=1.4996222034096718 , Validation loss:4.0510823130607605\n",
            "Epoch:982 Training loss=1.3151268921792507 , Validation loss:3.549047529697418\n",
            "Epoch:983 Training loss=1.416907750070095 , Validation loss:3.1450984477996826\n",
            "Epoch:984 Training loss=1.3192577958106995 , Validation loss:3.5772905349731445\n",
            "Epoch:985 Training loss=1.3494724556803703 , Validation loss:3.175929546356201\n",
            "Epoch:986 Training loss=1.0847652927041054 , Validation loss:3.1472378969192505\n",
            "Epoch:987 Training loss=1.138088621199131 , Validation loss:3.3414385616779327\n",
            "Epoch:988 Training loss=0.976248562335968 , Validation loss:3.5907450020313263\n",
            "Epoch:989 Training loss=1.4659008532762527 , Validation loss:2.754501134157181\n",
            "Epoch:990 Training loss=1.2268822714686394 , Validation loss:3.349622666835785\n",
            "Epoch:991 Training loss=2.2894449681043625 , Validation loss:2.858454942703247\n",
            "Epoch:992 Training loss=1.5684634447097778 , Validation loss:4.677474737167358\n",
            "Epoch:993 Training loss=1.468206763267517 , Validation loss:3.1932483315467834\n",
            "Epoch:994 Training loss=1.7220345586538315 , Validation loss:3.044825166463852\n",
            "Epoch:995 Training loss=1.301756426692009 , Validation loss:3.0980547666549683\n",
            "Epoch:996 Training loss=0.8459554947912693 , Validation loss:3.411612182855606\n",
            "Epoch:997 Training loss=1.7693763561546803 , Validation loss:2.9156434535980225\n",
            "Epoch:998 Training loss=1.1318984627723694 , Validation loss:3.107814610004425\n",
            "Epoch:999 Training loss=1.6011208966374397 , Validation loss:3.9245691299438477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4TEkKZhNWSl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "9d396ed5-e6cf-4e1f-a900-593718e4afcf"
      },
      "source": [
        "action_criterion(p_a,Y1)"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-212-33b7d5d7162f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maction_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: multi-target not supported at /pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15"
          ]
        }
      ]
    }
  ]
}