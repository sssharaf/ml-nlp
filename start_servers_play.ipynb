{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "start-servers-play.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sssharaf/ml-nlp/blob/master/start_servers_play.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5lt8ZaM-nCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls -ltr /gdrive/'My Drive'/ML/data/start-servers-play\n",
        "!pip install pytorch_transformers\n",
        "!ln -s  /gdrive/'My Drive'/ML/data/start-servers-play data\n",
        "!ls -ltr data/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjxaBXQ6_z49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import pytorch_transformers as pt\n",
        "from pytorch_transformers import BertTokenizer, BertConfig,BertForMaskedLM,BertModel,DistilBertTokenizer, DistilBertModel,DistilBertForSequenceClassification \n",
        "import os\n",
        "import typing\n",
        "from typing import Dict,List,Sequence,Set\n",
        "from types import SimpleNamespace as SN\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight,compute_sample_weight\n",
        "T_BertTokenizer = typing.NewType(\"BertTokenizer\",BertTokenizer)\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziNHWdP8_8UQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AExKHXwAJlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_df = pd.read_csv('data/train.csv',dtype={'action':'category','component':'category'})\n",
        "val_df = pd.read_csv('data/val.csv',dtype={'action':'category','component':'category'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-PHDUlxAkSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a_ohe = OneHotEncoder(sparse=False)\n",
        "a_ohe.fit(trn_df.loc[:,['action']])\n",
        "print(a_ohe.transform([['start']]))\n",
        "print(a_ohe.categories_[0])\n",
        "c_ohe = OneHotEncoder(sparse=False)\n",
        "c_ohe.fit(trn_df.loc[:,['component']])\n",
        "\n",
        "action_le = LabelEncoder()\n",
        "action_le.fit(trn_df.action)\n",
        "component_le = LabelEncoder()\n",
        "component_le.fit(trn_df.component)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPyjvoDcAa6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def encode_X(comment:str,max_len):\n",
        "  X = f\"[CLS] {comment}[SEP]\"\n",
        "  encoded = torch.tensor(tokenizer.encode(X),dtype=torch.long)\n",
        "  X = torch.zeros(max_len,dtype=torch.long)\n",
        "  X[:len(encoded)] = encoded\n",
        "  X[len(encoded)+1:] = torch.tensor(tokenizer.pad_token_id,dtype=torch.long)  \n",
        "  X_attn_mask = X!=tokenizer.pad_token_id\n",
        "  return X,X_attn_mask\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self,df:DataFrame,max_len = 16):\n",
        "        self.df = df\n",
        "        self.max_len=max_len\n",
        "        self.action = self.df.action.cat.codes\n",
        "        self.component = self.df.component.cat.codes\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        X = self.df.iloc[index]['comment_text']\n",
        "        X,X_attn_mask = encode_X(X,self.max_len)\n",
        "        Y1 = self.df.iloc[index]['action']\n",
        "        Y1 = action_le.transform([Y1])\n",
        "        #Y1 = a_ohe.transform([[Y1]])\n",
        "        Y1 = torch.tensor(Y1,dtype=torch.long)\n",
        "        Y2 = self.df.iloc[index]['component']\n",
        "        Y2 = component_le.transform([Y2])\n",
        "        #Y2 = c_ohe.transform([[Y2]])\n",
        "        Y2 = torch.tensor(Y2, dtype=torch.long)\n",
        "        return (X,X_attn_mask),(Y1.squeeze(),Y2.squeeze())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def components(self):\n",
        "        return self.component"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPDP-89dCVOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_ds = MyDataset(trn_df)\n",
        "val_ds = MyDataset(val_df)\n",
        "trn_ds[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXTXXGTiCzmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_dl = DataLoader(dataset=trn_ds,batch_size=4,pin_memory=True)\n",
        "val_dl = DataLoader(dataset=val_ds,batch_size=4,pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQUqP1PEGuFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model\n",
        "class MyModel(nn.Module):\n",
        "  def __init__(self, freeze_bert = True):\n",
        "    super().__init__()\n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased')\n",
        "    self.attn = nn.Linear(768,1)\n",
        "    self.c_attn = nn.Linear(768,1)\n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "        #nn.Dropout(0.1),\n",
        "        nn.Linear(768,len(a_ohe.categories_[0]),bias=False)\n",
        "    )\n",
        "    self.component_cls_lyr = nn.Sequential(\n",
        "                #nn.Dropout(0.1),\n",
        "                nn.Linear(768,len(c_ohe.categories_[0]),bias=False)\n",
        "    )\n",
        "    \n",
        "\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        for p in self.bert_lyr.parameters():\n",
        "            p.requires_grad = False\n",
        "    #nn.init.xavier_uniform_(self.action_cls_lyr.weight)\n",
        "    #nn.init.xavier_uniform_(self.component_cls_lyr.weight)\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False):\n",
        "    seq_emb,ctx = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    a = self.attn(seq_emb)\n",
        "    a = a_output = a.softmax(dim=1)\n",
        "    a = a*seq_emb\n",
        "    a = a.sum(dim=1)\n",
        "\n",
        "    c = self.c_attn(seq_emb)\n",
        "    c = c.softmax(dim=1)\n",
        "    c = c*seq_emb\n",
        "    c = c.sum(dim=1)\n",
        "\n",
        "    outputs = [self.action_cls_lyr(a),self.component_cls_lyr(c)]\n",
        "    if (output_attn):\n",
        "      outputs += [a_output]\n",
        "    return outputs\n",
        "\n",
        "model = MyModel(freeze_bert=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s11DA4cwKnDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_XdOgy_DHCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Device type is {DEVICE.type}')\n",
        "model.to(DEVICE)\n",
        "\n",
        "le_trnf = action_le.transform(trn_df.action)\n",
        "u,c = np.unique(le_trnf,return_counts=True)\n",
        "ac_class_weight=compute_class_weight('balanced',classes=u,y=le_trnf)\n",
        "ac_class_weight = torch.tensor(ac_class_weight,dtype=torch.float,device=DEVICE)\n",
        "le_trnf = component_le.transform(trn_df.component)\n",
        "u,c = np.unique(le_trnf,return_counts=True)\n",
        "com_class_weight=compute_class_weight('balanced',classes=u,y=le_trnf)\n",
        "com_class_weight = torch.tensor(com_class_weight,dtype=torch.float,device=DEVICE)\n",
        "\n",
        "print(f'ac_class_weight={ac_class_weight}')\n",
        "print(f'com_class_weight={com_class_weight}')\n",
        "\n",
        "optimizer = torch.optim.AdamW([{'params':model.bert_lyr.parameters(),'lr':1e-6},\n",
        "                               {'params':model.attn.parameters(),'lr':1e-5},\n",
        "                               {'params':model.action_cls_lyr.parameters(),'lr':1e-5},\n",
        "                               {'params':model.component_cls_lyr.parameters(),'lr':1e-5},\n",
        "                               ],lr=1e-5 )\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4,weight_decay=0.1 )\n",
        "action_criterion = nn.modules.loss.CrossEntropyLoss(weight=ac_class_weight,reduction='mean')\n",
        "component_criterion = nn.modules.loss.CrossEntropyLoss(weight=com_class_weight,reduction='mean')\n",
        "\n",
        "n_epochs = 1000\n",
        "\n",
        "def evaluate_model(model:MyModel,dl:DataLoader,optimizer):\n",
        "  t_loss=0\n",
        "  for (X, attn_mask),(Y1,Y2) in dl:\n",
        "    X , attn_mask,Y1,Y2 = X.to(DEVICE),attn_mask.to(DEVICE),Y1.to(DEVICE),Y2.to(DEVICE)\n",
        "    p_a,p_c = model(X,attn_mask)\n",
        "    loss = action_criterion(p_a,Y1) + component_criterion(p_c,Y2)\n",
        "    if optimizer is not None:\n",
        "      optimizer.zero_grad()\n",
        "      torch.nn.utils.clip_grad_norm(model.parameters(),1)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    t_loss += loss.item()\n",
        "  return t_loss\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  t_loss=0\n",
        "  t_loss = evaluate_model(model,trn_dl,optimizer)\n",
        "  v_loss = 0\n",
        "  with torch.no_grad():\n",
        "    v_loss = evaluate_model(model,val_dl,None)\n",
        "  print(f'Epoch:{epoch} Training loss={t_loss} , Validation loss:{v_loss}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4TEkKZhNWSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "  X,X_attn_mask = encode_X('commence begin hedge engine',max_len=16)\n",
        "  X,X_attn_mask = X.to(DEVICE), X_attn_mask.to(DEVICE)\n",
        "  X.unsqueeze_(0)\n",
        "  X_attn_mask.unsqueeze_(0)\n",
        "  action,component,a_attn = model(X,X_attn_mask,output_attn=True)\n",
        "  print(a_attn.squeeze())\n",
        "  action = torch.argmax(action.detach().cpu()).item()\n",
        "  print(action)\n",
        "  print(action_le.classes_)\n",
        "  component = torch.argmax(component.detach().cpu()).item()\n",
        "  print(action_le.inverse_transform([action]))\n",
        "  print(component_le.inverse_transform([component]))\n",
        "  print(component_le.classes_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UGbEM_6fjB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = torch.ones(4,16,768)\n",
        "attn = nn.Linear(768,1)\n",
        "a = attn(i)\n",
        "a = a.softmax(dim=1)\n",
        "o = a*i\n",
        "\n",
        "print(o.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4mB7LOFJmj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_df"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}