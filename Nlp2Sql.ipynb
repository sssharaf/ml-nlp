{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nlp2Sql.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sssharaf/ml-nlp/blob/master/Nlp2Sql.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0sdwqwB7m4m",
        "colab_type": "code",
        "outputId": "7f707ca5-f9d5-4ab2-8e0e-4eeb2cc3e116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls -ltr /gdrive/'My Drive'/ML/data/nlp2sql\n",
        "!ln -s  /gdrive/'My Drive'/ML/data/nlp2sql wikisql\n",
        "!pip install pytorch_transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "total 180484\n",
            "-rw------- 1 root root 33179166 Aug 11  2017 train.tables.jsonl\n",
            "-rw------- 1 root root  9858694 Aug 11  2017 test.tables.jsonl\n",
            "-rw------- 1 root root  4652052 Aug 11  2017 dev.tables.jsonl\n",
            "-rw------- 1 root root 12271616 Aug 11  2017 dev.db\n",
            "-rw------- 1 root root 85348352 Aug 11  2017 train.db\n",
            "-rw------- 1 root root 24453120 Aug 11  2017 test.db\n",
            "-rw------- 1 root root 10511763 Oct  9  2017 train.jsonl\n",
            "-rw------- 1 root root  1570387 Oct  9  2017 dev.jsonl\n",
            "-rw------- 1 root root  2967862 Oct  9  2017 test.jsonl\n",
            "Collecting pytorch_transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.17.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (4.28.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 51.0MB/s \n",
            "\u001b[?25hCollecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/1d/13cc7d174cd2d05808abac3f5fb37433e30c4cd93b152d2a9c09c926d7e8/regex-2019.11.1.tar.gz (669kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 35.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 36.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.3.0+cu100)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.10.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (0.14.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2019.9.11)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.4 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (1.13.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.9.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.4->boto3->pytorch_transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.4->boto3->pytorch_transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses, regex\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=a55255e0b0a01de47f76e77361f5323c87c8bc0e205d420bf6b5dfbb7d171e6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.11.1-cp36-cp36m-linux_x86_64.whl size=609220 sha256=12f8e3d4e1aa94851193a2fa0a3f4f4d269e1be9993dcb0328e5e66779c8b7de\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/c6/c1/0bc8d16ea38c44536a82dd1bec665996e5af37489fa88826b6\n",
            "Successfully built sacremoses regex\n",
            "Installing collected packages: sacremoses, regex, sentencepiece, pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.2.0 regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGb2Jk5s9chT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import pytorch_transformers as pt\n",
        "from pytorch_transformers import BertTokenizer, BertConfig,BertForMaskedLM,BertModel,DistilBertTokenizer, DistilBertModel,DistilBertForSequenceClassification \n",
        "import os\n",
        "import typing\n",
        "from typing import Dict,List,Sequence,Set\n",
        "from types import SimpleNamespace as SN\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight,compute_sample_weight\n",
        "\n",
        "T_BertTokenizer = typing.NewType(\"BertTokenizer\",BertTokenizer)\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rmMXWXp-gLV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0db5792-c485-4210-d960-f4d582363799"
      },
      "source": [
        "EMPTY_COL='[EMPTY]'\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "#bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "max_hs_len = 3"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 939185.92B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua9dCo9t91oB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def data_from_tables(tab_file:str,dir:str='wikisql/data') -> Dict[str,SN]:\n",
        "    tab_map:Dict[str,SN]={}\n",
        "    for l in open(dir + f'{os.sep}' + tab_file):\n",
        "        l = json.loads(l.strip())\n",
        "        header = l['header']\n",
        "        header.append(EMPTY_COL)\n",
        "        e_header = [tokenizer.encode(h) for h in header]\n",
        "        types = l['types']\n",
        "        id = l['id']\n",
        "        sn = SN()\n",
        "        sn.header = header\n",
        "        sn.e_header = e_header\n",
        "        sn.types = types\n",
        "        tab_map[id]=sn\n",
        "    return tab_map\n",
        "\n",
        "def data_from_sql(sql_file:str,dir='wikisql/data') -> (List[str], List[str],List[str],List[List[SN]]):\n",
        "    sql_text: List[str]=[]\n",
        "    table_id: List[str]=[]\n",
        "    all_conds: List[List[SN]]=[]\n",
        "    for l in open(dir + f'{os.sep}' + sql_file):\n",
        "        l = json.loads(l.strip())\n",
        "        tab_id = l['table_id']\n",
        "        table_id.append(tab_id)\n",
        "        sql_text.append(l['question'])\n",
        "        sql = l['sql']\n",
        "        conds = []\n",
        "        for cond in sql['conds']:\n",
        "            sn = SN()\n",
        "            sn.ci = cond[0]\n",
        "            sn.oi = cond[1]\n",
        "            sn.c = cond[2]\n",
        "            conds.append(sn)\n",
        "        all_conds.append(conds)\n",
        "    e_sql_text = [tokenizer.encode(s) for s in sql_text]\n",
        "    return table_id,sql_text,e_sql_text, all_conds\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL4_5vZ799De",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class MyDataSet(Dataset) :\n",
        "    def __init__(self,max_hs_len=3,sql_file='train.jsonl',tab_file='train.tables.jsonl',dir='wikisql/data'):\n",
        "        super(MyDataSet).__init__()\n",
        "        tab_map = data_from_tables(tab_file,dir)\n",
        "        table_ids, sql_texts,e_sql_text, all_conds = data_from_sql(sql_file,dir)\n",
        "        data = []\n",
        "\n",
        "        for tab_id,enc_s in zip(table_ids,e_sql_text):\n",
        "            x = []\n",
        "            x.extend(tokenizer.encode(tokenizer.cls_token))\n",
        "            x.extend(enc_s)\n",
        "            tab_info = tab_map[tab_id]\n",
        "            x.extend(tokenizer.encode(tokenizer.sep_token))\n",
        "            for enc_h in tab_info.e_header:\n",
        "                enc_h = enc_h[:max_hs_len]\n",
        "                x.extend(enc_h)\n",
        "                x.extend(tokenizer.encode(tokenizer.sep_token))\n",
        "            data.append(x)\n",
        "\n",
        "        max_l = 0\n",
        "        for d in data:\n",
        "            if len(d) > max_l:\n",
        "                max_l = len(d)\n",
        "        self.data = data\n",
        "        self.max_l = max_l\n",
        "        self.all_conds = all_conds\n",
        "        self.n_where_conds = [len(c) for c in all_conds]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        d = self.data[idx]\n",
        "        X = torch.zeros(self.max_l,dtype=torch.long)\n",
        "        X[:len(d)] = torch.tensor(d,dtype=torch.long)\n",
        "        #X = X.to(DEVICE)\n",
        "        Y = torch.tensor(self.n_where_conds[idx],dtype=torch.long)\n",
        "        #Y = Y.to(DEVICE)\n",
        "        return X,Y\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD6aFVBr_TXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds = MyDataSet(dir='/gdrive/My Drive/ML/data/nlp2sql')\n",
        "ds_dev = MyDataSet(dir='/gdrive/My Drive/ML/data/nlp2sql',sql_file='dev.jsonl',tab_file='dev.tables.jsonl')\n",
        "\n",
        "u,c = np.unique(ds.n_where_conds,return_counts=True)\n",
        "class_weight=compute_class_weight('balanced',classes=u,y=ds.n_where_conds)\n",
        "class_weight = torch.tensor(class_weight,dtype=torch.float,device=DEVICE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzmKrQpbCSDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dl = DataLoader(num_workers=4,dataset=ds,batch_size=64,pin_memory=True)\n",
        "dl_dev = DataLoader(num_workers=4,dataset=ds_dev,batch_size=64,pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "322An-KEBZES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "83c658b6-9297-4c59-f559-e509d0e921df"
      },
      "source": [
        "from pytorch_transformers.modeling_bert import BertPooler\n",
        "class MyModel(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(MyModel,self).__init__()\n",
        "    #self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    self.bert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "    self.pooler = BertPooler(self.bert_model.config)\n",
        "\n",
        "    self.linear = nn.Sequential(\n",
        "      nn.Linear(768,5,bias=False),\n",
        "    )\n",
        "\n",
        "  def forward(self,X):\n",
        "    attn_mask = X > 1\n",
        "    ctx = self.bert_model(X,attention_mask=attn_mask)\n",
        "    ctx = ctx[0]\n",
        "    ctx = self.pooler(ctx)\n",
        "    #print(ctx.shape)\n",
        "    output = self.linear(ctx)\n",
        "    return output\n",
        "\n",
        "try:\n",
        "  del model\n",
        "except NameError:\n",
        "  pass\n",
        "finally :\n",
        "  print('Cleaning memory')\n",
        "torch.cuda.empty_cache()\n",
        "model = MyModel()\n",
        "criterion = nn.modules.loss.CrossEntropyLoss(weight=class_weight)\n",
        "model.to(DEVICE)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaning memory\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyModel(\n",
              "  (bert_model): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0): TransformerBlock(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerBlock(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerBlock(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerBlock(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerBlock(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerBlock(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              "  (linear): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=5, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62yMz7IgYeIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#torch.quantization.quantize(bert_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGHJ06ImBtvL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "439c8325-783c-41f6-c418-5ee463281c34"
      },
      "source": [
        "#model.half()\n",
        "#torch.quantization.quantize(bert_model)\n",
        "torch.cuda.reset_max_memory_allocated()\n",
        "torch.cuda.reset_max_memory_cached()\n",
        "optimizer = torch.optim.AdamW( model.parameters(),lr=0.00001)\n",
        "if (torch.cuda.is_available()):\n",
        "  print(\"Cuda available\")\n",
        "else:\n",
        "  print(\"No Cuda\")\n",
        "\n",
        "print(f'Total # of training batch is {len(dl)}')\n",
        "n_epochs=10\n",
        "for epoch in range(n_epochs):\n",
        "  t_loss = 0\n",
        "  print(f'Running epoch {epoch}')\n",
        "  for i,(X,Y) in enumerate(dl):\n",
        "    print(f'Running batch # {i}')\n",
        "    try:\n",
        "      X = X.to(DEVICE)\n",
        "      Y = Y.to(DEVICE)\n",
        "      #print(Y)\n",
        "      output = model(X)\n",
        "      loss = criterion(output,Y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      t_loss = t_loss + loss.item()\n",
        "    finally:\n",
        "      del X\n",
        "      del Y\n",
        "    torch.cuda.empty_cache()\n",
        "  # inference\n",
        "  t_loss_dev = 0\n",
        "  with torch.no_grad():\n",
        "    for X,Y in dl_dev:\n",
        "      X = X.to(DEVICE)\n",
        "      Y = Y.to(DEVICE)      \n",
        "      output = model(X)\n",
        "      loss = criterion(output,Y)\n",
        "      t_loss_dev = t_loss_dev + loss.item()\n",
        "\n",
        "  print(f'Loss={t_loss}, Dev loss={t_loss_dev}')\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda available\n",
            "Total # of training batch is 881\n",
            "Running epoch 0\n",
            "Running batch # 0\n",
            "Running batch # 1\n",
            "Running batch # 2\n",
            "Running batch # 3\n",
            "Running batch # 4\n",
            "Running batch # 5\n",
            "Running batch # 6\n",
            "Running batch # 7\n",
            "Running batch # 8\n",
            "Running batch # 9\n",
            "Running batch # 10\n",
            "Running batch # 11\n",
            "Running batch # 12\n",
            "Running batch # 13\n",
            "Running batch # 14\n",
            "Running batch # 15\n",
            "Running batch # 16\n",
            "Running batch # 17\n",
            "Running batch # 18\n",
            "Running batch # 19\n",
            "Running batch # 20\n",
            "Running batch # 21\n",
            "Running batch # 22\n",
            "Running batch # 23\n",
            "Running batch # 24\n",
            "Running batch # 25\n",
            "Running batch # 26\n",
            "Running batch # 27\n",
            "Running batch # 28\n",
            "Running batch # 29\n",
            "Running batch # 30\n",
            "Running batch # 31\n",
            "Running batch # 32\n",
            "Running batch # 33\n",
            "Running batch # 34\n",
            "Running batch # 35\n",
            "Running batch # 36\n",
            "Running batch # 37\n",
            "Running batch # 38\n",
            "Running batch # 39\n",
            "Running batch # 40\n",
            "Running batch # 41\n",
            "Running batch # 42\n",
            "Running batch # 43\n",
            "Running batch # 44\n",
            "Running batch # 45\n",
            "Running batch # 46\n",
            "Running batch # 47\n",
            "Running batch # 48\n",
            "Running batch # 49\n",
            "Running batch # 50\n",
            "Running batch # 51\n",
            "Running batch # 52\n",
            "Running batch # 53\n",
            "Running batch # 54\n",
            "Running batch # 55\n",
            "Running batch # 56\n",
            "Running batch # 57\n",
            "Running batch # 58\n",
            "Running batch # 59\n",
            "Running batch # 60\n",
            "Running batch # 61\n",
            "Running batch # 62\n",
            "Running batch # 63\n",
            "Running batch # 64\n",
            "Running batch # 65\n",
            "Running batch # 66\n",
            "Running batch # 67\n",
            "Running batch # 68\n",
            "Running batch # 69\n",
            "Running batch # 70\n",
            "Running batch # 71\n",
            "Running batch # 72\n",
            "Running batch # 73\n",
            "Running batch # 74\n",
            "Running batch # 75\n",
            "Running batch # 76\n",
            "Running batch # 77\n",
            "Running batch # 78\n",
            "Running batch # 79\n",
            "Running batch # 80\n",
            "Running batch # 81\n",
            "Running batch # 82\n",
            "Running batch # 83\n",
            "Running batch # 84\n",
            "Running batch # 85\n",
            "Running batch # 86\n",
            "Running batch # 87\n",
            "Running batch # 88\n",
            "Running batch # 89\n",
            "Running batch # 90\n",
            "Running batch # 91\n",
            "Running batch # 92\n",
            "Running batch # 93\n",
            "Running batch # 94\n",
            "Running batch # 95\n",
            "Running batch # 96\n",
            "Running batch # 97\n",
            "Running batch # 98\n",
            "Running batch # 99\n",
            "Running batch # 100\n",
            "Running batch # 101\n",
            "Running batch # 102\n",
            "Running batch # 103\n",
            "Running batch # 104\n",
            "Running batch # 105\n",
            "Running batch # 106\n",
            "Running batch # 107\n",
            "Running batch # 108\n",
            "Running batch # 109\n",
            "Running batch # 110\n",
            "Running batch # 111\n",
            "Running batch # 112\n",
            "Running batch # 113\n",
            "Running batch # 114\n",
            "Running batch # 115\n",
            "Running batch # 116\n",
            "Running batch # 117\n",
            "Running batch # 118\n",
            "Running batch # 119\n",
            "Running batch # 120\n",
            "Running batch # 121\n",
            "Running batch # 122\n",
            "Running batch # 123\n",
            "Running batch # 124\n",
            "Running batch # 125\n",
            "Running batch # 126\n",
            "Running batch # 127\n",
            "Running batch # 128\n",
            "Running batch # 129\n",
            "Running batch # 130\n",
            "Running batch # 131\n",
            "Running batch # 132\n",
            "Running batch # 133\n",
            "Running batch # 134\n",
            "Running batch # 135\n",
            "Running batch # 136\n",
            "Running batch # 137\n",
            "Running batch # 138\n",
            "Running batch # 139\n",
            "Running batch # 140\n",
            "Running batch # 141\n",
            "Running batch # 142\n",
            "Running batch # 143\n",
            "Running batch # 144\n",
            "Running batch # 145\n",
            "Running batch # 146\n",
            "Running batch # 147\n",
            "Running batch # 148\n",
            "Running batch # 149\n",
            "Running batch # 150\n",
            "Running batch # 151\n",
            "Running batch # 152\n",
            "Running batch # 153\n",
            "Running batch # 154\n",
            "Running batch # 155\n",
            "Running batch # 156\n",
            "Running batch # 157\n",
            "Running batch # 158\n",
            "Running batch # 159\n",
            "Running batch # 160\n",
            "Running batch # 161\n",
            "Running batch # 162\n",
            "Running batch # 163\n",
            "Running batch # 164\n",
            "Running batch # 165\n",
            "Running batch # 166\n",
            "Running batch # 167\n",
            "Running batch # 168\n",
            "Running batch # 169\n",
            "Running batch # 170\n",
            "Running batch # 171\n",
            "Running batch # 172\n",
            "Running batch # 173\n",
            "Running batch # 174\n",
            "Running batch # 175\n",
            "Running batch # 176\n",
            "Running batch # 177\n",
            "Running batch # 178\n",
            "Running batch # 179\n",
            "Running batch # 180\n",
            "Running batch # 181\n",
            "Running batch # 182\n",
            "Running batch # 183\n",
            "Running batch # 184\n",
            "Running batch # 185\n",
            "Running batch # 186\n",
            "Running batch # 187\n",
            "Running batch # 188\n",
            "Running batch # 189\n",
            "Running batch # 190\n",
            "Running batch # 191\n",
            "Running batch # 192\n",
            "Running batch # 193\n",
            "Running batch # 194\n",
            "Running batch # 195\n",
            "Running batch # 196\n",
            "Running batch # 197\n",
            "Running batch # 198\n",
            "Running batch # 199\n",
            "Running batch # 200\n",
            "Running batch # 201\n",
            "Running batch # 202\n",
            "Running batch # 203\n",
            "Running batch # 204\n",
            "Running batch # 205\n",
            "Running batch # 206\n",
            "Running batch # 207\n",
            "Running batch # 208\n",
            "Running batch # 209\n",
            "Running batch # 210\n",
            "Running batch # 211\n",
            "Running batch # 212\n",
            "Running batch # 213\n",
            "Running batch # 214\n",
            "Running batch # 215\n",
            "Running batch # 216\n",
            "Running batch # 217\n",
            "Running batch # 218\n",
            "Running batch # 219\n",
            "Running batch # 220\n",
            "Running batch # 221\n",
            "Running batch # 222\n",
            "Running batch # 223\n",
            "Running batch # 224\n",
            "Running batch # 225\n",
            "Running batch # 226\n",
            "Running batch # 227\n",
            "Running batch # 228\n",
            "Running batch # 229\n",
            "Running batch # 230\n",
            "Running batch # 231\n",
            "Running batch # 232\n",
            "Running batch # 233\n",
            "Running batch # 234\n",
            "Running batch # 235\n",
            "Running batch # 236\n",
            "Running batch # 237\n",
            "Running batch # 238\n",
            "Running batch # 239\n",
            "Running batch # 240\n",
            "Running batch # 241\n",
            "Running batch # 242\n",
            "Running batch # 243\n",
            "Running batch # 244\n",
            "Running batch # 245\n",
            "Running batch # 246\n",
            "Running batch # 247\n",
            "Running batch # 248\n",
            "Running batch # 249\n",
            "Running batch # 250\n",
            "Running batch # 251\n",
            "Running batch # 252\n",
            "Running batch # 253\n",
            "Running batch # 254\n",
            "Running batch # 255\n",
            "Running batch # 256\n",
            "Running batch # 257\n",
            "Running batch # 258\n",
            "Running batch # 259\n",
            "Running batch # 260\n",
            "Running batch # 261\n",
            "Running batch # 262\n",
            "Running batch # 263\n",
            "Running batch # 264\n",
            "Running batch # 265\n",
            "Running batch # 266\n",
            "Running batch # 267\n",
            "Running batch # 268\n",
            "Running batch # 269\n",
            "Running batch # 270\n",
            "Running batch # 271\n",
            "Running batch # 272\n",
            "Running batch # 273\n",
            "Running batch # 274\n",
            "Running batch # 275\n",
            "Running batch # 276\n",
            "Running batch # 277\n",
            "Running batch # 278\n",
            "Running batch # 279\n",
            "Running batch # 280\n",
            "Running batch # 281\n",
            "Running batch # 282\n",
            "Running batch # 283\n",
            "Running batch # 284\n",
            "Running batch # 285\n",
            "Running batch # 286\n",
            "Running batch # 287\n",
            "Running batch # 288\n",
            "Running batch # 289\n",
            "Running batch # 290\n",
            "Running batch # 291\n",
            "Running batch # 292\n",
            "Running batch # 293\n",
            "Running batch # 294\n",
            "Running batch # 295\n",
            "Running batch # 296\n",
            "Running batch # 297\n",
            "Running batch # 298\n",
            "Running batch # 299\n",
            "Running batch # 300\n",
            "Running batch # 301\n",
            "Running batch # 302\n",
            "Running batch # 303\n",
            "Running batch # 304\n",
            "Running batch # 305\n",
            "Running batch # 306\n",
            "Running batch # 307\n",
            "Running batch # 308\n",
            "Running batch # 309\n",
            "Running batch # 310\n",
            "Running batch # 311\n",
            "Running batch # 312\n",
            "Running batch # 313\n",
            "Running batch # 314\n",
            "Running batch # 315\n",
            "Running batch # 316\n",
            "Running batch # 317\n",
            "Running batch # 318\n",
            "Running batch # 319\n",
            "Running batch # 320\n",
            "Running batch # 321\n",
            "Running batch # 322\n",
            "Running batch # 323\n",
            "Running batch # 324\n",
            "Running batch # 325\n",
            "Running batch # 326\n",
            "Running batch # 327\n",
            "Running batch # 328\n",
            "Running batch # 329\n",
            "Running batch # 330\n",
            "Running batch # 331\n",
            "Running batch # 332\n",
            "Running batch # 333\n",
            "Running batch # 334\n",
            "Running batch # 335\n",
            "Running batch # 336\n",
            "Running batch # 337\n",
            "Running batch # 338\n",
            "Running batch # 339\n",
            "Running batch # 340\n",
            "Running batch # 341\n",
            "Running batch # 342\n",
            "Running batch # 343\n",
            "Running batch # 344\n",
            "Running batch # 345\n",
            "Running batch # 346\n",
            "Running batch # 347\n",
            "Running batch # 348\n",
            "Running batch # 349\n",
            "Running batch # 350\n",
            "Running batch # 351\n",
            "Running batch # 352\n",
            "Running batch # 353\n",
            "Running batch # 354\n",
            "Running batch # 355\n",
            "Running batch # 356\n",
            "Running batch # 357\n",
            "Running batch # 358\n",
            "Running batch # 359\n",
            "Running batch # 360\n",
            "Running batch # 361\n",
            "Running batch # 362\n",
            "Running batch # 363\n",
            "Running batch # 364\n",
            "Running batch # 365\n",
            "Running batch # 366\n",
            "Running batch # 367\n",
            "Running batch # 368\n",
            "Running batch # 369\n",
            "Running batch # 370\n",
            "Running batch # 371\n",
            "Running batch # 372\n",
            "Running batch # 373\n",
            "Running batch # 374\n",
            "Running batch # 375\n",
            "Running batch # 376\n",
            "Running batch # 377\n",
            "Running batch # 378\n",
            "Running batch # 379\n",
            "Running batch # 380\n",
            "Running batch # 381\n",
            "Running batch # 382\n",
            "Running batch # 383\n",
            "Running batch # 384\n",
            "Running batch # 385\n",
            "Running batch # 386\n",
            "Running batch # 387\n",
            "Running batch # 388\n",
            "Running batch # 389\n",
            "Running batch # 390\n",
            "Running batch # 391\n",
            "Running batch # 392\n",
            "Running batch # 393\n",
            "Running batch # 394\n",
            "Running batch # 395\n",
            "Running batch # 396\n",
            "Running batch # 397\n",
            "Running batch # 398\n",
            "Running batch # 399\n",
            "Running batch # 400\n",
            "Running batch # 401\n",
            "Running batch # 402\n",
            "Running batch # 403\n",
            "Running batch # 404\n",
            "Running batch # 405\n",
            "Running batch # 406\n",
            "Running batch # 407\n",
            "Running batch # 408\n",
            "Running batch # 409\n",
            "Running batch # 410\n",
            "Running batch # 411\n",
            "Running batch # 412\n",
            "Running batch # 413\n",
            "Running batch # 414\n",
            "Running batch # 415\n",
            "Running batch # 416\n",
            "Running batch # 417\n",
            "Running batch # 418\n",
            "Running batch # 419\n",
            "Running batch # 420\n",
            "Running batch # 421\n",
            "Running batch # 422\n",
            "Running batch # 423\n",
            "Running batch # 424\n",
            "Running batch # 425\n",
            "Running batch # 426\n",
            "Running batch # 427\n",
            "Running batch # 428\n",
            "Running batch # 429\n",
            "Running batch # 430\n",
            "Running batch # 431\n",
            "Running batch # 432\n",
            "Running batch # 433\n",
            "Running batch # 434\n",
            "Running batch # 435\n",
            "Running batch # 436\n",
            "Running batch # 437\n",
            "Running batch # 438\n",
            "Running batch # 439\n",
            "Running batch # 440\n",
            "Running batch # 441\n",
            "Running batch # 442\n",
            "Running batch # 443\n",
            "Running batch # 444\n",
            "Running batch # 445\n",
            "Running batch # 446\n",
            "Running batch # 447\n",
            "Running batch # 448\n",
            "Running batch # 449\n",
            "Running batch # 450\n",
            "Running batch # 451\n",
            "Running batch # 452\n",
            "Running batch # 454\n",
            "Running batch # 455\n",
            "Running batch # 456\n",
            "Running batch # 457\n",
            "Running batch # 458\n",
            "Running batch # 459\n",
            "Running batch # 460\n",
            "Running batch # 461\n",
            "Running batch # 462\n",
            "Running batch # 463\n",
            "Running batch # 464\n",
            "Running batch # 465\n",
            "Running batch # 466\n",
            "Running batch # 467\n",
            "Running batch # 468\n",
            "Running batch # 469\n",
            "Running batch # 470\n",
            "Running batch # 471\n",
            "Running batch # 472\n",
            "Running batch # 473\n",
            "Running batch # 474\n",
            "Running batch # 475\n",
            "Running batch # 476\n",
            "Running batch # 477\n",
            "Running batch # 478\n",
            "Running batch # 479\n",
            "Running batch # 480\n",
            "Running batch # 481\n",
            "Running batch # 482\n",
            "Running batch # 483\n",
            "Running batch # 484\n",
            "Running batch # 485\n",
            "Running batch # 486\n",
            "Running batch # 487\n",
            "Running batch # 488\n",
            "Running batch # 489\n",
            "Running batch # 490\n",
            "Running batch # 491\n",
            "Running batch # 492\n",
            "Running batch # 493\n",
            "Running batch # 494\n",
            "Running batch # 495\n",
            "Running batch # 496\n",
            "Running batch # 497\n",
            "Running batch # 498\n",
            "Running batch # 499\n",
            "Running batch # 500\n",
            "Running batch # 501\n",
            "Running batch # 502\n",
            "Running batch # 503\n",
            "Running batch # 504\n",
            "Running batch # 505\n",
            "Running batch # 506\n",
            "Running batch # 507\n",
            "Running batch # 508\n",
            "Running batch # 509\n",
            "Running batch # 510\n",
            "Running batch # 511\n",
            "Running batch # 512\n",
            "Running batch # 513\n",
            "Running batch # 514\n",
            "Running batch # 515\n",
            "Running batch # 516\n",
            "Running batch # 517\n",
            "Running batch # 518\n",
            "Running batch # 519\n",
            "Running batch # 520\n",
            "Running batch # 521\n",
            "Running batch # 522\n",
            "Running batch # 523\n",
            "Running batch # 524\n",
            "Running batch # 525\n",
            "Running batch # 526\n",
            "Running batch # 527\n",
            "Running batch # 528\n",
            "Running batch # 529\n",
            "Running batch # 530\n",
            "Running batch # 531\n",
            "Running batch # 532\n",
            "Running batch # 533\n",
            "Running batch # 534\n",
            "Running batch # 535\n",
            "Running batch # 536\n",
            "Running batch # 537\n",
            "Running batch # 538\n",
            "Running batch # 539\n",
            "Running batch # 540\n",
            "Running batch # 541\n",
            "Running batch # 542\n",
            "Running batch # 543\n",
            "Running batch # 544\n",
            "Running batch # 545\n",
            "Running batch # 546\n",
            "Running batch # 547\n",
            "Running batch # 548\n",
            "Running batch # 549\n",
            "Running batch # 550\n",
            "Running batch # 551\n",
            "Running batch # 552\n",
            "Running batch # 553\n",
            "Running batch # 554\n",
            "Running batch # 555\n",
            "Running batch # 556\n",
            "Running batch # 557\n",
            "Running batch # 558\n",
            "Running batch # 559\n",
            "Running batch # 560\n",
            "Running batch # 561\n",
            "Running batch # 562\n",
            "Running batch # 563\n",
            "Running batch # 564\n",
            "Running batch # 565\n",
            "Running batch # 566\n",
            "Running batch # 567\n",
            "Running batch # 568\n",
            "Running batch # 569\n",
            "Running batch # 570\n",
            "Running batch # 571\n",
            "Running batch # 572\n",
            "Running batch # 573\n",
            "Running batch # 574\n",
            "Running batch # 575\n",
            "Running batch # 576\n",
            "Running batch # 577\n",
            "Running batch # 578\n",
            "Running batch # 579\n",
            "Running batch # 580\n",
            "Running batch # 581\n",
            "Running batch # 582\n",
            "Running batch # 583\n",
            "Running batch # 584\n",
            "Running batch # 585\n",
            "Running batch # 586\n",
            "Running batch # 587\n",
            "Running batch # 588\n",
            "Running batch # 589\n",
            "Running batch # 590\n",
            "Running batch # 591\n",
            "Running batch # 592\n",
            "Running batch # 593\n",
            "Running batch # 594\n",
            "Running batch # 595\n",
            "Running batch # 596\n",
            "Running batch # 597\n",
            "Running batch # 598\n",
            "Running batch # 599\n",
            "Running batch # 600\n",
            "Running batch # 601\n",
            "Running batch # 602\n",
            "Running batch # 603\n",
            "Running batch # 604\n",
            "Running batch # 605\n",
            "Running batch # 606\n",
            "Running batch # 607\n",
            "Running batch # 608\n",
            "Running batch # 609\n",
            "Running batch # 610\n",
            "Running batch # 611\n",
            "Running batch # 612\n",
            "Running batch # 613\n",
            "Running batch # 614\n",
            "Running batch # 615\n",
            "Running batch # 616\n",
            "Running batch # 617\n",
            "Running batch # 618\n",
            "Running batch # 619\n",
            "Running batch # 620\n",
            "Running batch # 621\n",
            "Running batch # 622\n",
            "Running batch # 623\n",
            "Running batch # 624\n",
            "Running batch # 625\n",
            "Running batch # 626\n",
            "Running batch # 627\n",
            "Running batch # 628\n",
            "Running batch # 629\n",
            "Running batch # 630\n",
            "Running batch # 631\n",
            "Running batch # 632\n",
            "Running batch # 633\n",
            "Running batch # 634\n",
            "Running batch # 635\n",
            "Running batch # 636\n",
            "Running batch # 637\n",
            "Running batch # 638\n",
            "Running batch # 639\n",
            "Running batch # 640\n",
            "Running batch # 641\n",
            "Running batch # 642\n",
            "Running batch # 643\n",
            "Running batch # 644\n",
            "Running batch # 645\n",
            "Running batch # 646\n",
            "Running batch # 647\n",
            "Running batch # 648\n",
            "Running batch # 649\n",
            "Running batch # 650\n",
            "Running batch # 651\n",
            "Running batch # 652\n",
            "Running batch # 653\n",
            "Running batch # 654\n",
            "Running batch # 655\n",
            "Running batch # 656\n",
            "Running batch # 657\n",
            "Running batch # 658\n",
            "Running batch # 659\n",
            "Running batch # 660\n",
            "Running batch # 661\n",
            "Running batch # 662\n",
            "Running batch # 663\n",
            "Running batch # 664\n",
            "Running batch # 665\n",
            "Running batch # 666\n",
            "Running batch # 667\n",
            "Running batch # 668\n",
            "Running batch # 669\n",
            "Running batch # 670\n",
            "Running batch # 671\n",
            "Running batch # 672\n",
            "Running batch # 673\n",
            "Running batch # 674\n",
            "Running batch # 675\n",
            "Running batch # 676\n",
            "Running batch # 677\n",
            "Running batch # 678\n",
            "Running batch # 679\n",
            "Running batch # 680\n",
            "Running batch # 681\n",
            "Running batch # 682\n",
            "Running batch # 683\n",
            "Running batch # 684\n",
            "Running batch # 685\n",
            "Running batch # 686\n",
            "Running batch # 687\n",
            "Running batch # 688\n",
            "Running batch # 689\n",
            "Running batch # 690\n",
            "Running batch # 691\n",
            "Running batch # 692\n",
            "Running batch # 693\n",
            "Running batch # 694\n",
            "Running batch # 695\n",
            "Running batch # 696\n",
            "Running batch # 697\n",
            "Running batch # 698\n",
            "Running batch # 699\n",
            "Running batch # 700\n",
            "Running batch # 701\n",
            "Running batch # 702\n",
            "Running batch # 703\n",
            "Running batch # 704\n",
            "Running batch # 705\n",
            "Running batch # 706\n",
            "Running batch # 707\n",
            "Running batch # 708\n",
            "Running batch # 709\n",
            "Running batch # 710\n",
            "Running batch # 711\n",
            "Running batch # 712\n",
            "Running batch # 713\n",
            "Running batch # 714\n",
            "Running batch # 715\n",
            "Running batch # 716\n",
            "Running batch # 717\n",
            "Running batch # 718\n",
            "Running batch # 719\n",
            "Running batch # 720\n",
            "Running batch # 721\n",
            "Running batch # 722\n",
            "Running batch # 723\n",
            "Running batch # 724\n",
            "Running batch # 725\n",
            "Running batch # 726\n",
            "Running batch # 727\n",
            "Running batch # 728\n",
            "Running batch # 729\n",
            "Running batch # 730\n",
            "Running batch # 731\n",
            "Running batch # 732\n",
            "Running batch # 733\n",
            "Running batch # 734\n",
            "Running batch # 735\n",
            "Running batch # 736\n",
            "Running batch # 737\n",
            "Running batch # 738\n",
            "Running batch # 739\n",
            "Running batch # 740\n",
            "Running batch # 741\n",
            "Running batch # 742\n",
            "Running batch # 743\n",
            "Running batch # 744\n",
            "Running batch # 745\n",
            "Running batch # 746\n",
            "Running batch # 747\n",
            "Running batch # 748\n",
            "Running batch # 749\n",
            "Running batch # 750\n",
            "Running batch # 751\n",
            "Running batch # 752\n",
            "Running batch # 753\n",
            "Running batch # 754\n",
            "Running batch # 755\n",
            "Running batch # 756\n",
            "Running batch # 757\n",
            "Running batch # 758\n",
            "Running batch # 759\n",
            "Running batch # 760\n",
            "Running batch # 761\n",
            "Running batch # 762\n",
            "Running batch # 763\n",
            "Running batch # 764\n",
            "Running batch # 765\n",
            "Running batch # 766\n",
            "Running batch # 767\n",
            "Running batch # 768\n",
            "Running batch # 769\n",
            "Running batch # 770\n",
            "Running batch # 771\n",
            "Running batch # 772\n",
            "Running batch # 773\n",
            "Running batch # 774\n",
            "Running batch # 775\n",
            "Running batch # 776\n",
            "Running batch # 777\n",
            "Running batch # 778\n",
            "Running batch # 779\n",
            "Running batch # 780\n",
            "Running batch # 781\n",
            "Running batch # 782\n",
            "Running batch # 783\n",
            "Running batch # 784\n",
            "Running batch # 785\n",
            "Running batch # 786\n",
            "Running batch # 787\n",
            "Running batch # 788\n",
            "Running batch # 789\n",
            "Running batch # 790\n",
            "Running batch # 791\n",
            "Running batch # 792\n",
            "Running batch # 793\n",
            "Running batch # 794\n",
            "Running batch # 795\n",
            "Running batch # 796\n",
            "Running batch # 797\n",
            "Running batch # 798\n",
            "Running batch # 799\n",
            "Running batch # 800\n",
            "Running batch # 801\n",
            "Running batch # 802\n",
            "Running batch # 803\n",
            "Running batch # 804\n",
            "Running batch # 805\n",
            "Running batch # 806\n",
            "Running batch # 807\n",
            "Running batch # 808\n",
            "Running batch # 809\n",
            "Running batch # 810\n",
            "Running batch # 811\n",
            "Running batch # 812\n",
            "Running batch # 813\n",
            "Running batch # 814\n",
            "Running batch # 815\n",
            "Running batch # 816\n",
            "Running batch # 817\n",
            "Running batch # 818\n",
            "Running batch # 819\n",
            "Running batch # 820\n",
            "Running batch # 821\n",
            "Running batch # 822\n",
            "Running batch # 823\n",
            "Running batch # 824\n",
            "Running batch # 825\n",
            "Running batch # 826\n",
            "Running batch # 827\n",
            "Running batch # 828\n",
            "Running batch # 829\n",
            "Running batch # 830\n",
            "Running batch # 831\n",
            "Running batch # 832\n",
            "Running batch # 833\n",
            "Running batch # 834\n",
            "Running batch # 835\n",
            "Running batch # 836\n",
            "Running batch # 837\n",
            "Running batch # 838\n",
            "Running batch # 839\n",
            "Running batch # 840\n",
            "Running batch # 841\n",
            "Running batch # 842\n",
            "Running batch # 843\n",
            "Running batch # 844\n",
            "Running batch # 845\n",
            "Running batch # 846\n",
            "Running batch # 847\n",
            "Running batch # 848\n",
            "Running batch # 849\n",
            "Running batch # 850\n",
            "Running batch # 851\n",
            "Running batch # 852\n",
            "Running batch # 853\n",
            "Running batch # 854\n",
            "Running batch # 855\n",
            "Running batch # 856\n",
            "Running batch # 857\n",
            "Running batch # 858\n",
            "Running batch # 859\n",
            "Running batch # 860\n",
            "Running batch # 861\n",
            "Running batch # 862\n",
            "Running batch # 863\n",
            "Running batch # 864\n",
            "Running batch # 865\n",
            "Running batch # 866\n",
            "Running batch # 867\n",
            "Running batch # 868\n",
            "Running batch # 869\n",
            "Running batch # 870\n",
            "Running batch # 871\n",
            "Running batch # 872\n",
            "Running batch # 873\n",
            "Running batch # 874\n",
            "Running batch # 875\n",
            "Running batch # 876\n",
            "Running batch # 877\n",
            "Running batch # 878\n",
            "Running batch # 879\n",
            "Running batch # 880\n",
            "Loss=548.7170829549432, Dev loss=115.34176447987556\n",
            "Running epoch 1\n",
            "Running batch # 0\n",
            "Running batch # 1\n",
            "Running batch # 2\n",
            "Running batch # 3\n",
            "Running batch # 4\n",
            "Running batch # 5\n",
            "Running batch # 6\n",
            "Running batch # 7\n",
            "Running batch # 8\n",
            "Running batch # 9\n",
            "Running batch # 10\n",
            "Running batch # 11\n",
            "Running batch # 12\n",
            "Running batch # 13\n",
            "Running batch # 14\n",
            "Running batch # 15\n",
            "Running batch # 16\n",
            "Running batch # 17\n",
            "Running batch # 18\n",
            "Running batch # 19\n",
            "Running batch # 20\n",
            "Running batch # 21\n",
            "Running batch # 22\n",
            "Running batch # 23\n",
            "Running batch # 24\n",
            "Running batch # 25\n",
            "Running batch # 26\n",
            "Running batch # 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKLRUpe3IOaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight,compute_sample_weight\n",
        "\n",
        "#Y.numpy()\n",
        "u,_ = np.unique([ len(c) for c in ds.all_conds],return_counts=True)\n",
        "compute_class_weight('balanced',classes=u,y=[len(c) for c in ds.all_conds])\n",
        "compute_sample_weight('balanced',[ len(c) for c in ds.all_conds])\n",
        "np.bincount?\n",
        "pt.DistilBertModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4ziC0n8WVrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output.shape"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}