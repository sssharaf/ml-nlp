{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "start-servers-play - 1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sssharaf/ml-nlp/blob/master/start_servers_play_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5lt8ZaM-nCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls -ltr /gdrive/'My Drive'/ML/data/start-servers-play\n",
        "!pip install pytorch_transformers\n",
        "!ln -s  /gdrive/'My Drive'/ML/data/start-servers-play data\n",
        "!ls -ltr data/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjxaBXQ6_z49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "import pytorch_transformers as pt\n",
        "from pytorch_transformers import BertTokenizer, BertConfig,BertForMaskedLM,BertModel,DistilBertTokenizer, DistilBertModel,DistilBertForSequenceClassification \n",
        "import os\n",
        "import typing\n",
        "from typing import Dict,List,Sequence,Set\n",
        "from types import SimpleNamespace as SN\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight,compute_sample_weight\n",
        "T_BertTokenizer = typing.NewType(\"BertTokenizer\",BertTokenizer)\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziNHWdP8_8UQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AExKHXwAJlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_df = pd.read_csv('data/train.csv',dtype={'action':'category','component':'category'},)\n",
        "val_df = pd.read_csv('data/val-new.csv',dtype={'action':'category','component':'category'})\n",
        "#trn_df.loc[trn_df.action=='noaction']\n",
        "trn_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYhoktKLPhdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-PHDUlxAkSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "action_le = LabelEncoder()\n",
        "action_le.fit(trn_df.action)\n",
        "component_le = LabelEncoder()\n",
        "component_le.fit(trn_df.component)\n",
        "print(action_le.classes_)\n",
        "print(component_le.classes_)\n",
        "\n",
        "with open(f'data/action_le.dat','wb') as f:\n",
        "  pickle.dump(action_le,f)\n",
        "\n",
        "with open(f'data/component_le.dat','wb') as f:\n",
        "  pickle.dump(component_le,f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPyjvoDcAa6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def encode_X(comment:str,max_len):\n",
        "  X = f\"[CLS] {comment} [SEP]\"\n",
        "  encoded = torch.tensor(tokenizer.encode(X),dtype=torch.long)\n",
        "  X = torch.zeros(max_len,dtype=torch.long)\n",
        "  X[:len(encoded)] = encoded\n",
        "  X[len(encoded)+1:] = torch.tensor(tokenizer.pad_token_id,dtype=torch.long)  \n",
        "  X_attn_mask = X!=tokenizer.pad_token_id\n",
        "  X_attn_mask = X_attn_mask.int()\n",
        "  return X,X_attn_mask\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self,df:DataFrame,max_len = 16):\n",
        "        self.df = df\n",
        "        self.max_len=max_len\n",
        "        self.action = self.df.action.cat.codes\n",
        "        self.component = self.df.component.cat.codes\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        X = self.df.iloc[index]['comment_text']\n",
        "        X,X_attn_mask = encode_X(X,self.max_len)\n",
        "        Y1 = self.df.iloc[index]['action']\n",
        "        Y1 = action_le.transform([Y1])\n",
        "        #Y1 = a_ohe.transform([[Y1]])\n",
        "        Y1 = torch.tensor(Y1,dtype=torch.long)\n",
        "        Y2 = self.df.iloc[index]['component']\n",
        "        Y2 = component_le.transform([Y2])\n",
        "        #Y2 = c_ohe.transform([[Y2]])\n",
        "        Y2 = torch.tensor(Y2, dtype=torch.long)\n",
        "        return (X,X_attn_mask),(Y1.squeeze(),Y2.squeeze())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def components(self):\n",
        "        return self.component"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPDP-89dCVOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "trn_ds = MyDataset(trn_df,max_len=25)\n",
        "val_ds = MyDataset(val_df,max_len=25)\n",
        "\n",
        "trn_dl = DataLoader(dataset=trn_ds,batch_size=16,pin_memory=True,shuffle=True)\n",
        "val_dl = DataLoader(dataset=val_ds,batch_size=16,pin_memory=True,shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXTXXGTiCzmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQUqP1PEGuFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model\n",
        "class MyModel3(nn.Module):\n",
        "  def __init__(self, freeze_bert = True):\n",
        "    super().__init__()\n",
        "    self.model_version = 3\n",
        "\n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True)\n",
        "    \n",
        "    self.a_attn = nn.Linear(768,1,bias=False)\n",
        "    \n",
        "    self.c_attn = nn.Linear(768,1,bias=False)\n",
        "\n",
        "    self.attn_dropout = nn.Dropout(0.1,inplace=False)\n",
        "    \n",
        "    self.a_switch = nn.Sequential(\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(768*2,64),\n",
        "        nn.ELU(),\n",
        "        nn.LayerNorm(64),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(64,1),\n",
        "        nn.Sigmoid(),\n",
        "        )\n",
        "    \n",
        "    self.c_switch = nn.Sequential(\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(768*2,64),\n",
        "        nn.ELU(),\n",
        "        nn.LayerNorm(64),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(64,1),\n",
        "        nn.Sigmoid(),\n",
        "    )\n",
        "\n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(768,64,bias=False),\n",
        "        nn.ELU(),\n",
        "        nn.LayerNorm(64),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(64,len(action_le.classes_),bias=False),   \n",
        "    )\n",
        "\n",
        "    self.component_cls_lyr = nn.Sequential(\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(768,64,bias=False),\n",
        "                nn.ELU(),\n",
        "                nn.LayerNorm(64),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(64,len(component_le.classes_),bias=False),\n",
        "    )\n",
        "    \n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "       for p in self.bert_lyr.parameters():#self.bert_lyr.parameters():\n",
        "              p.requires_grad = False\n",
        "    #nn.init.xavier_uniform_(self.action_cls_lyr)\n",
        "    #nn.init.xavier_uniform_(self.component_cls_lyr)\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False,output_switch=False):\n",
        "    attn_mask_cls = (1 - attn_masks)*-10000\n",
        "    attn_mask_cls.unsqueeze_(dim=-1)\n",
        "\n",
        "    seq_emb,ctx,hs = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "   \n",
        "    a = self.a_attn(seq_emb)\n",
        "    a = a + attn_mask_cls\n",
        "    a =  a_output = a.softmax(dim=1)\n",
        "    a = self.attn_dropout(a)\n",
        "    a = torch.mul(seq_emb,a)\n",
        "    a = a.mean(dim=1)\n",
        "\n",
        "    c = self.c_attn(seq_emb)\n",
        "    c = c + attn_mask_cls\n",
        "    c =  c_output = c.softmax(dim=1)\n",
        "    c = self.attn_dropout(c)\n",
        "    c = torch.mul(seq_emb,c)\n",
        "    c = c.mean(dim=1)\n",
        "\n",
        "    a_switch = self.a_switch(torch.cat([ctx.detach(),a],dim=-1))    \n",
        "    a = (1 - a_switch)*a + a_switch*ctx\n",
        "\n",
        "    c_switch = self.c_switch(torch.cat([ctx.detach(),c],dim=-1))\n",
        "    c = (1 - c_switch)*c + c_switch*ctx\n",
        "\n",
        "    outputs = [self.action_cls_lyr(a),self.component_cls_lyr(c)]\n",
        "    if (output_attn):\n",
        "      outputs += [a_output,c_output]\n",
        "    if output_hs:\n",
        "      outputs +=[hs]\n",
        "    if output_switch:\n",
        "      outputs +=[a_switch,c_switch]\n",
        "    return outputs\n",
        "\n",
        "############################### Model 2  ############################################\n",
        "# Model\n",
        "class MyModel2(nn.Module):\n",
        "  def __init__(self, freeze_bert = True):\n",
        "    super().__init__()\n",
        "    self.model_version = 2-1\n",
        "    #self.static_bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=False)\n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True)\n",
        "    \n",
        "    self.a_attn = nn.Linear(768,1)\n",
        "    \n",
        "    self.c_attn = nn.Linear(768,1)\n",
        "    \n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(768,64,bias=True),\n",
        "        nn.LayerNorm(64),\n",
        "        nn.ELU(),\n",
        "        nn.Linear(64,len(action_le.classes_),bias=True),\n",
        "        nn.LayerNorm(len(action_le.classes_)),\n",
        "        #nn.Linear(768,len(action_le.classes_),bias=False),      \n",
        "    )\n",
        "\n",
        "    self.component_cls_lyr = nn.Sequential(\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(768,64,bias=True),\n",
        "                nn.LayerNorm(64),\n",
        "                nn.ELU(),\n",
        "                nn.Linear(64,len(component_le.classes_),bias=True),\n",
        "                nn.LayerNorm(len(component_le.classes_)),\n",
        "                #nn.Linear(768,len(component_le.classes_),bias=False),\n",
        "    )\n",
        "    \n",
        "    # for p in self.static_bert_lyr.parameters():\n",
        "    #   p.requires_grad = False\n",
        "\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        for lyr in self.bert_lyr.encoder.layer[:-2]:\n",
        "          for p in lyr.parameters():#self.bert_lyr.parameters():\n",
        "              p.requires_grad = False\n",
        "    #nn.init.xavier_uniform_(self.action_cls_lyr)\n",
        "    #nn.init.xavier_uniform_(self.component_cls_lyr)\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False):\n",
        "    attn_mask_cls = (1 - attn_masks)*-10000\n",
        "    attn_mask_cls.unsqueeze_(dim=-1)\n",
        "\n",
        "    #static_emb,static_ctx = self.static_bert_lyr(seq,attention_mask =attn_masks)\n",
        "    seq_emb,ctx,hs = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    #seq_emb +=static_emb\n",
        "    a = self.a_attn(seq_emb)\n",
        "    a = a + attn_mask_cls\n",
        "    a = a_output = a.softmax(dim=1)\n",
        "    a = torch.mul(seq_emb,a)\n",
        "    a = a.mean(dim=1)\n",
        "\n",
        "    c = self.c_attn(seq_emb)\n",
        "    c = c + attn_mask_cls\n",
        "    c = c_output =  c.softmax(dim=1)\n",
        "    c = torch.mul(seq_emb,c)\n",
        "    c = c.mean(dim=1)\n",
        "\n",
        "    outputs = [self.action_cls_lyr(a),self.component_cls_lyr(c)]\n",
        "    if (output_attn):\n",
        "      outputs += [a_output,c_output]\n",
        "    if output_hs:\n",
        "      outputs +=[hs]\n",
        "    return outputs\n",
        "#####################################################################################\n",
        "model = MyModel3(freeze_bert=False)\n",
        "model = model.to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15EENDfO9JAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers import BertForSequenceClassification\n",
        "############################### Model 1  ############################################\n",
        "# Model\n",
        "class MyModel4(nn.Module):\n",
        "  def __init__(self, freeze_bert = True):\n",
        "    super().__init__()\n",
        "    self.model_version = 4\n",
        "    \n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "\n",
        "    #self.comp_bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "    \n",
        "    self.config = self.bert_lyr.config;\n",
        "\n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(768,4),\n",
        "    )\n",
        "    self.comp_cls_lyr = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(768,5),\n",
        "        )    \n",
        "\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        for p in self.bert_lyr.parameters():\n",
        "          p.requires_grad = False\n",
        "        for p in self.comp_bert_lyr.parameters():\n",
        "          p.requires_grad = False\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False):\n",
        "    \n",
        "    seq_emb,pooled,hs,attn = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    \n",
        "    #c_seq_emb,c_pooled,c_hs,c_attn = self.comp_bert_lyr(seq,attention_mask =attn_masks)\n",
        "    \n",
        "    outputs=[]\n",
        "    outputs += [\n",
        "                self.action_cls_lyr(pooled),\n",
        "                self.comp_cls_lyr(pooled),\n",
        "                ]\n",
        "    return outputs\n",
        "\n",
        "try:\n",
        "  del model\n",
        "  torch.cuda.reset_max_memory_allocated()\n",
        "  torch.cuda.reset_max_memory_cached()\n",
        "except NameError:\n",
        "  pass\n",
        "\n",
        "model = MyModel1(freeze_bert=False)\n",
        "model = model.to(DEVICE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvE6VXRMOilX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63V0iudows0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers import BertForSequenceClassification\n",
        "############################### Model 1  ############################################\n",
        "# Model\n",
        "class MyModel5(nn.Module):\n",
        "  def __init__(self, freeze_bert = True):\n",
        "    super().__init__()\n",
        "    self.model_version = 5\n",
        "    \n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "\n",
        "    self.comp_bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "    \n",
        "    self.config = self.bert_lyr.config;\n",
        "\n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(768,4),\n",
        "    )\n",
        "    self.comp_cls_lyr = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(768,5),\n",
        "        )    \n",
        "\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        for p in self.bert_lyr.parameters():\n",
        "          p.requires_grad = False\n",
        "        for p in self.comp_bert_lyr.parameters():\n",
        "          p.requires_grad = False\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False):\n",
        "    \n",
        "    seq_emb,pooled,hs,attn = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    \n",
        "    c_seq_emb,c_pooled,c_hs,c_attn = self.comp_bert_lyr(seq,attention_mask =attn_masks)\n",
        "    \n",
        "    outputs=[]\n",
        "    outputs += [\n",
        "                self.action_cls_lyr(pooled),\n",
        "                self.comp_cls_lyr(c_pooled),\n",
        "                ]\n",
        "    return outputs\n",
        "\n",
        "try:\n",
        "  del model\n",
        "  torch.cuda.reset_max_memory_allocated()\n",
        "  torch.cuda.reset_max_memory_cached()\n",
        "except NameError:\n",
        "  pass\n",
        "\n",
        "model = MyModel5(freeze_bert=False)\n",
        "model = model.to(DEVICE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8Yf8oSUTGzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers import BertForSequenceClassification\n",
        "############################### Model 1  ############################################\n",
        "# Model\n",
        "class MyModel7(nn.Module):\n",
        "  def __init__(self, freeze_bert = True):\n",
        "    super().__init__()\n",
        "    self.model_version = '7_1'\n",
        "    \n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "\n",
        "    #self.comp_bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "    \n",
        "    self.config = self.bert_lyr.config;\n",
        "\n",
        "    self.a_attn = nn.Linear(768,1)\n",
        "    \n",
        "    self.c_attn = nn.Linear(768,1)\n",
        "\n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(768,4),\n",
        "    )\n",
        "    self.comp_cls_lyr = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(768,5),\n",
        "        )    \n",
        "\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        self.freeze_bert()\n",
        "    else:\n",
        "        self.freeze_bert()\n",
        "        self.unfreeze_bert()\n",
        "        \n",
        "  def freeze_bert(self):\n",
        "      for p in self.bert_lyr.parameters():\n",
        "        p.requires_grad = False\n",
        "      # for p in self.comp_bert_lyr.parameters():\n",
        "      #   p.requires_grad = False\n",
        "\n",
        "  def unfreeze_bert(self,from_lyr=6):\n",
        "\n",
        "    for lyr in self.bert_lyr.encoder.layer[-6:]:\n",
        "      for p in lyr.parameters():\n",
        "        p.requires_grad = True\n",
        "    # for lyr in self.comp_bert_lyr.encoder.layer[-6:]:\n",
        "    #   for p in lyr.parameters():\n",
        "    #     p.requires_grad = True\n",
        "\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False):\n",
        "    attn_mask_cls = (1 - attn_masks)*-10000\n",
        "    attn_mask_cls.unsqueeze_(dim=-1)\n",
        "    seq_emb,pooled,hs,attn = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    \n",
        "    #c_seq_emb,c_pooled,c_hs,c_attn = self.comp_bert_lyr(seq,attention_mask =attn_masks)\n",
        "    a,a_output = self.attention(seq_emb,self.a_attn,attn_mask_cls)\n",
        "\n",
        "    c,c_output = self.attention(seq_emb,self.c_attn,attn_mask_cls)\n",
        "\n",
        "    a_pooled  = a\n",
        "    c_pooled  = c\n",
        "    outputs=[]\n",
        "    outputs += [\n",
        "                self.action_cls_lyr(a_pooled),\n",
        "                self.comp_cls_lyr(c_pooled),\n",
        "                ]\n",
        "    if (output_attn):\n",
        "      outputs += [a_output,c_output]\n",
        "    return outputs\n",
        "\n",
        "  def attention(self,seq_emb,attn_lyr,attn_mask_cls):\n",
        "    a = attn_lyr(seq_emb)\n",
        "    a = a + attn_mask_cls\n",
        "    a = a_output = a.softmax(dim=1)\n",
        "    #a_output = a.clone()\n",
        "    a = torch.mul(seq_emb,a)\n",
        "    a = a.mean(dim=1)\n",
        "    return a,a_output\n",
        "\n",
        "try:\n",
        "  del model\n",
        "  torch.cuda.reset_max_memory_allocated()\n",
        "  torch.cuda.reset_max_memory_cached()\n",
        "except NameError:\n",
        "  pass\n",
        "\n",
        "model = MyModel7(freeze_bert=False)\n",
        "model = model.to(DEVICE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnwTCsoH_KDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'data/model-{model.model_version}.dat')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s11DA4cwKnDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X, X_attn),(_,_) = next(iter(trn_dl))\n",
        "X, X_attn = X.to(DEVICE),X_attn.to(DEVICE)\n",
        "a = model(X,X_attn,output_attn=True,output_hs=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IKDBrDGnXWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load model\n",
        "model = MyModel7(freeze_bert=False)\n",
        "#model.load_state_dict(torch.load(f'data/model-{model.model_version}.dat'))\n",
        "model.to(DEVICE)\n",
        "\n",
        "g_wd = 1\n",
        "g_lr = 1e-4\n",
        "optimizer = torch.optim.AdamW([\n",
        "                               {'params':model.bert_lyr.parameters(),'lr':5e-6,'weight_decay':0.1},\n",
        "                              #  {'params':model.comp_bert_lyr.parameters(),'lr':5e-6,'weight_decay':0.1},\n",
        "                                {'params':model.a_attn.parameters()},\n",
        "                                {'params':model.c_attn.parameters()},\n",
        "                               {'params':model.action_cls_lyr.parameters()},\n",
        "                               {'params':model.comp_cls_lyr.parameters(),},\n",
        "                               ],lr=g_lr,weight_decay=g_wd )\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=20,T_mult=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_XdOgy_DHCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train model\n",
        "# torch.cuda.reset_max_memory_allocated()\n",
        "# torch.cuda.reset_max_memory_cached()\n",
        "print(f'Device type is {DEVICE.type}')\n",
        "model.to(DEVICE)\n",
        "model.train()\n",
        "le_trnf = action_le.transform(trn_df.action)\n",
        "u,c = np.unique(le_trnf,return_counts=True)\n",
        "ac_class_weight=compute_class_weight('balanced',classes=u,y=le_trnf)\n",
        "ac_class_weight = torch.tensor(ac_class_weight,dtype=torch.float,device=DEVICE)\n",
        "le_trnf = component_le.transform(trn_df.component)\n",
        "u,c = np.unique(le_trnf,return_counts=True)\n",
        "com_class_weight=compute_class_weight('balanced',classes=u,y=le_trnf)\n",
        "com_class_weight = torch.tensor(com_class_weight,dtype=torch.float,device=DEVICE)\n",
        "\n",
        "print(f'ac_class_weight={ac_class_weight}')\n",
        "print(f'com_class_weight={com_class_weight}')\n",
        "\n",
        "\n",
        "action_criterion = nn.modules.loss.CrossEntropyLoss(weight=ac_class_weight,reduction='mean')\n",
        "component_criterion = nn.modules.loss.CrossEntropyLoss(weight=com_class_weight,reduction='mean')\n",
        "\n",
        "n_epochs = 2000\n",
        "\n",
        "def evaluate_model(epoch:int,model:nn.Module,dl:DataLoader,optimizer):\n",
        "  t_loss=0.0\n",
        "  t_a_loss=0.0\n",
        "  t_c_loss=0.0\n",
        "  iters = len(dl)\n",
        "  for i,((X, attn_mask),(Y1,Y2)) in enumerate(dl):\n",
        "    X , attn_mask,Y1,Y2 = X.to(DEVICE),attn_mask.to(DEVICE),Y1.to(DEVICE),Y2.to(DEVICE)\n",
        "    p_a,p_c = model(X,attn_mask)\n",
        "    action_loss = action_criterion(p_a.view(-1,4),Y1)\n",
        "    component_loss = component_criterion(p_c.view(-1,5),Y2)\n",
        "    loss =   action_loss + component_loss\n",
        "    if optimizer is not None:\n",
        "      scheduler.step(epoch+i/iters)\n",
        "      optimizer.zero_grad()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(),5)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    t_loss += loss.item()\n",
        "    t_a_loss += action_loss.item()\n",
        "    t_c_loss += component_loss.item()\n",
        "    \n",
        "  return t_loss,t_a_loss,t_c_loss\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  t_loss=0\n",
        "  a_loss=0\n",
        "  c_loss=0\n",
        "  t_loss,a_loss,c_loss = evaluate_model(epoch,model,trn_dl,optimizer)\n",
        "  if epoch%10 == 0:\n",
        "    torch.save(model.state_dict(),f'data/model-{model.model_version}.dat')\n",
        "  v_loss = 0\n",
        "  with torch.no_grad():\n",
        "    v_loss = evaluate_model(epoch,model,val_dl,None)\n",
        "    v_loss = [f'{v:0.4}' for v in v_loss]\n",
        "  print(f'Epoch:{epoch} Trn loss={t_loss:0.4}, Actn loss:{a_loss:0.4} ,Comp loss:{c_loss:0.4},  Validation loss:{v_loss}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4TEkKZhNWSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_tensor(t:torch.tensor):\n",
        "  t = t.squeeze().detach().cpu().numpy()\n",
        "  t = [f'{item:0.3}' for item in t]\n",
        "  print(t)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  X,X_attn_mask = encode_X('',max_len=20)\n",
        "  \n",
        "  X_tokenized = X\n",
        "  X,X_attn_mask = X.to(DEVICE), X_attn_mask.to(DEVICE)\n",
        "  X.unsqueeze_(0)\n",
        "  X_attn_mask.unsqueeze_(0)\n",
        "  #action,component,a_attn,c_attn,a_switch,c_switch = model(X,X_attn_mask,output_attn=True,output_switch=True)\n",
        "  action,component = model(X,X_attn_mask,output_attn=False)\n",
        "  action = action.softmax(dim=1)\n",
        "  component = component.softmax(dim=1)\n",
        "  action,component = [a.squeeze().detach().cpu().numpy() for a in [action,component]]\n",
        "\n",
        "  print(action_le.classes_)  \n",
        "  print(action)\n",
        "  #print(a_attn.squeeze())\n",
        "  action = np.argmax(action)\n",
        "  print(f'Selected Action: {action_le.inverse_transform([action])}')\n",
        "\n",
        "  print(component_le.classes_)\n",
        "  print(component)\n",
        "  component = np.argmax(component)\n",
        "  print(component)\n",
        "  print(f'Selected component: {component_le.inverse_transform([component])}')\n",
        "\n",
        "  print(tokenizer.convert_ids_to_tokens(X_tokenized.numpy()))\n",
        "  #print_tensor(a_attn)\n",
        "  #print_tensor(c_attn)\n",
        "  #print(a_attn.squeeze().detach().cpu().numpy())\n",
        "  #print(c_attn.squeeze().detach().cpu().numpy())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UGbEM_6fjB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [p for p in iter(model.action_cls_lyr.named_parameters())]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4mB7LOFJmj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}