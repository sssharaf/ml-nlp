{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "start-servers-play - 1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sssharaf/ml-nlp/blob/master/start_servers_play_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5lt8ZaM-nCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls -ltr /gdrive/'My Drive'/ML/data/start-servers-play\n",
        "!pip install pytorch_transformers\n",
        "!ln -s  /gdrive/'My Drive'/ML/data/start-servers-play data\n",
        "!ls -ltr data/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjxaBXQ6_z49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "import pytorch_transformers as pt\n",
        "from pytorch_transformers import BertTokenizer, BertConfig,BertForMaskedLM,BertModel,DistilBertTokenizer, DistilBertModel,DistilBertForSequenceClassification \n",
        "import os\n",
        "import typing\n",
        "from typing import Dict,List,Sequence,Set\n",
        "from types import SimpleNamespace as SN\n",
        "import numpy as np\n",
        "import pickle\n",
        "import math\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight,compute_sample_weight\n",
        "T_BertTokenizer = typing.NewType(\"BertTokenizer\",BertTokenizer)\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziNHWdP8_8UQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AExKHXwAJlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_df = pd.read_csv('data/train.csv',dtype={'action':'category','component':'category'},)\n",
        "val_df = pd.read_csv('data/val-new.csv',dtype={'action':'category','component':'category'})\n",
        "#trn_df.loc[trn_df.action=='noaction']\n",
        "trn_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-PHDUlxAkSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "action_le = LabelEncoder()\n",
        "action_le.fit(trn_df.action)\n",
        "component_le = LabelEncoder()\n",
        "component_le.fit(trn_df.component)\n",
        "print(action_le.classes_)\n",
        "print(component_le.classes_)\n",
        "\n",
        "with open(f'data/action_le.dat','wb') as f:\n",
        "  pickle.dump(action_le,f)\n",
        "\n",
        "with open(f'data/component_le.dat','wb') as f:\n",
        "  pickle.dump(component_le,f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPyjvoDcAa6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def encode_X(comment:str,max_len):\n",
        "  X = f\"[CLS] {comment} [SEP]\"\n",
        "  encoded = torch.tensor(tokenizer.encode(X),dtype=torch.long)\n",
        "  X = torch.zeros(max_len,dtype=torch.long)\n",
        "  X[:len(encoded)] = encoded\n",
        "  X[len(encoded)+1:] = torch.tensor(tokenizer.pad_token_id,dtype=torch.long)  \n",
        "  X_attn_mask = X!=tokenizer.pad_token_id\n",
        "  X_attn_mask = X_attn_mask.int()\n",
        "  return X,X_attn_mask\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self,df:DataFrame,max_len = 16):\n",
        "        self.df = df\n",
        "        self.max_len=max_len\n",
        "        self.action = self.df.action.cat.codes\n",
        "        self.component = self.df.component.cat.codes\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        X = self.df.iloc[index]['comment_text']\n",
        "        X,X_attn_mask = encode_X(X,self.max_len)\n",
        "        Y1 = self.df.iloc[index]['action']\n",
        "        Y1 = action_le.transform([Y1])\n",
        "        #Y1 = a_ohe.transform([[Y1]])\n",
        "        Y1 = torch.tensor(Y1,dtype=torch.long)\n",
        "        Y2 = self.df.iloc[index]['component']\n",
        "        Y2 = component_le.transform([Y2])\n",
        "        #Y2 = c_ohe.transform([[Y2]])\n",
        "        Y2 = torch.tensor(Y2, dtype=torch.long)\n",
        "        return (X,X_attn_mask),(Y1.squeeze(),Y2.squeeze())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def components(self):\n",
        "        return self.component"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPDP-89dCVOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "trn_ds = MyDataset(trn_df,max_len=25)\n",
        "val_ds = MyDataset(val_df,max_len=25)\n",
        "\n",
        "trn_dl = DataLoader(dataset=trn_ds,batch_size=32,pin_memory=True,shuffle=True)\n",
        "val_dl = DataLoader(dataset=val_ds,batch_size=32,pin_memory=True,shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXTXXGTiCzmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(trn_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvE6VXRMOilX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers import BertForSequenceClassification\n",
        "############################### Model 5  ############################################\n",
        "# Model\n",
        "class MyModel5(nn.Module):\n",
        "  def __init__(self, freeze_bert = True):\n",
        "    super().__init__()\n",
        "    self.model_version = 5\n",
        "    \n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "\n",
        "    self.config = self.bert_lyr.config;\n",
        "\n",
        "    self.cls_lyr = nn.Sequential(\n",
        "        nn.LayerNorm(768),\n",
        "        nn.Linear(768,9)\n",
        "    )\n",
        "\n",
        "    # self.cls_lyr = nn.Sequential(\n",
        "    #         nn.LayerNorm(768),\n",
        "    #         nn.Dropout(0.3),\n",
        "    #         nn.Linear(768,64),\n",
        "    #         nn.ReLU(),\n",
        "    #         nn.LayerNorm(64),\n",
        "    #         nn.Dropout(0.1),\n",
        "    #         nn.Linear(64,9),\n",
        "    # )\n",
        "\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        self.freeze_bert()\n",
        "        \n",
        "  def freeze_bert(self):\n",
        "      for p in self.bert_lyr.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "  def unfreeze_bert(self,from_lyr=6):\n",
        "\n",
        "    for lyr in self.bert_lyr.encoder.layer[-11:]:\n",
        "      for p in lyr.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False):\n",
        "    \n",
        "    seq_emb,pooled,hs,attn = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    o=self.cls_lyr(pooled)     \n",
        "    outputs=[]\n",
        "    outputs += [\n",
        "                o[:,:4],\n",
        "                o[:,4:],\n",
        "                ]\n",
        "    return outputs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8Yf8oSUTGzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers import BertForSequenceClassification\n",
        "############################### Model 1  ############################################\n",
        "# Model\n",
        "class MyModel7(nn.Module):\n",
        "  def __init__(self, freeze_bert = True,attn_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.model_version = '7_1'\n",
        "    \n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "    \n",
        "    self.config = self.bert_lyr.config;\n",
        "\n",
        "    self.a_attn = nn.Linear(768,1)\n",
        "    \n",
        "    self.c_attn = nn.Linear(768,1)\n",
        "\n",
        "    self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "\n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "        nn.Dropout(0.3),\n",
        "        nn.LayerNorm(768),\n",
        "        nn.Linear(768,4),\n",
        "    )\n",
        "    self.comp_cls_lyr = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.LayerNorm(768),\n",
        "            nn.Linear(768,5),\n",
        "        )    \n",
        "\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        self.freeze_bert()\n",
        "        \n",
        "  def freeze_bert(self):\n",
        "      for p in self.bert_lyr.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "  def unfreeze_bert(self,from_lyr=6):\n",
        "\n",
        "    for lyr in self.bert_lyr.encoder.layer[-6:]:\n",
        "      for p in lyr.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False):\n",
        "    attn_mask_cls = (1 - attn_masks)*-10000\n",
        "    attn_mask_cls.unsqueeze_(dim=-1)\n",
        "\n",
        "    seq_emb,pooled,hs,attn = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    \n",
        "    a,a_output = self.attention(seq_emb,self.a_attn,attn_mask_cls)\n",
        "\n",
        "    c,c_output = self.attention(seq_emb,self.c_attn,attn_mask_cls)\n",
        "\n",
        "    a_pooled  = a\n",
        "    c_pooled  = c\n",
        "    outputs=[]\n",
        "    outputs += [\n",
        "                self.action_cls_lyr(a_pooled),\n",
        "                self.comp_cls_lyr(c_pooled),\n",
        "                ]\n",
        "    if (output_attn):\n",
        "      outputs += [a_output,c_output]\n",
        "    return outputs\n",
        "\n",
        "  def attention(self,seq_emb,attn_lyr,attn_mask_cls):\n",
        "    a = attn_lyr(seq_emb)\n",
        "    a = a + attn_mask_cls\n",
        "    a = a_output = a.softmax(dim=1)\n",
        "    a = self.attn_dropout(a)\n",
        "    #a_output = a.clone()\n",
        "    a = torch.mul(seq_emb,a)\n",
        "    a = a.sum(dim=1)\n",
        "    return a,a_output.unsqueeze(dim=0)\n",
        "\n",
        "# try:\n",
        "#   del model\n",
        "#   torch.cuda.reset_max_memory_allocated()\n",
        "#   torch.cuda.reset_max_memory_cached()\n",
        "# except NameError:\n",
        "#   pass\n",
        "\n",
        "# model = MyModel7(freeze_bert=False)\n",
        "# model = model.to(DEVICE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wE002lx1Ke1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers import BertForSequenceClassification\n",
        "############################### Model 1  ############################################\n",
        "# Model\n",
        "class MyModel8(nn.Module):\n",
        "  def __init__(self, freeze_bert = True,attn_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.model_version = '8'\n",
        "    \n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "    \n",
        "    self.config = self.bert_lyr.config;\n",
        "\n",
        "    self.a_attn = nn.Linear(768,1)\n",
        "    \n",
        "    self.c_attn = nn.Linear(768,1)\n",
        "\n",
        "    self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "\n",
        "    self.shared_lyr = nn.Sequential(\n",
        "         nn.LayerNorm(768),\n",
        "         nn.Dropout(0.3),\n",
        "         nn.Linear(768,768),\n",
        "         nn.ReLU(),\n",
        "         nn.LayerNorm(768),\n",
        "    )\n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(768,4),\n",
        "    )\n",
        "    self.comp_cls_lyr = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(768,5),\n",
        "        )    \n",
        "\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        self.freeze_bert()\n",
        "        \n",
        "  def freeze_bert(self):\n",
        "      for p in self.bert_lyr.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "  def unfreeze_bert(self,from_lyr=6):\n",
        "\n",
        "    for lyr in self.bert_lyr.encoder.layer[-6:]:\n",
        "      for p in lyr.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False):\n",
        "    attn_mask_cls = (1 - attn_masks)*-10000\n",
        "    attn_mask_cls.unsqueeze_(dim=-1)\n",
        "\n",
        "    seq_emb,pooled,hs,attn = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    seq_emb = seq_emb[:,1:,:]\n",
        "    attn_mask_cls = attn_mask_cls[:,1:]\n",
        "    a,a_output = self.attention(seq_emb,self.a_attn,attn_mask_cls)\n",
        "\n",
        "    c,c_output = self.attention(seq_emb,self.c_attn,attn_mask_cls)\n",
        "\n",
        "    pooled = self.shared_lyr(pooled)\n",
        "    a_pooled  = a+pooled\n",
        "    c_pooled  = c+pooled\n",
        "    outputs=[]\n",
        "    outputs += [\n",
        "                self.action_cls_lyr(a_pooled),\n",
        "                self.comp_cls_lyr(c_pooled),\n",
        "                ]\n",
        "    if (output_attn):\n",
        "      outputs += [a_output,c_output]\n",
        "    return outputs\n",
        "\n",
        "  def attention(self,seq_emb,attn_lyr,attn_mask_cls):\n",
        "    a = attn_lyr(seq_emb)\n",
        "    a = a + attn_mask_cls\n",
        "    a = a_output = a.softmax(dim=1)\n",
        "    a = self.attn_dropout(a)\n",
        "    #a_output = a.clone()\n",
        "    a = torch.mul(seq_emb,a)\n",
        "    a = a.sum(dim=1)\n",
        "    return a,a_output.unsqueeze(dim=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7mO4b5s1t8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MyModel8()\n",
        "model.to(DEVICE)\n",
        "(X, X_attn),(_,_) = next(iter(trn_dl))\n",
        "X, X_attn = X.to(DEVICE),X_attn.to(DEVICE)\n",
        "print(f'X.size()={X.size()}')\n",
        "a,c,a_a,c_a = model(X,X_attn,output_attn=True,output_hs=True)\n",
        "a_a.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l017tvps2fJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers import BertForSequenceClassification\n",
        "############################### Model 1  ############################################\n",
        "# Model\n",
        "class MyModel8_LSTM(nn.Module):\n",
        "  def __init__(self, freeze_bert = True):\n",
        "    super().__init__()\n",
        "    self.model_version = '8'\n",
        "    \n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "    \n",
        "    self.config = self.bert_lyr.config;\n",
        "\n",
        "    n_features = 64\n",
        "    self.a_attn = nn.Linear(768,1)\n",
        "    \n",
        "    self.c_attn = nn.Linear(768,1)\n",
        "\n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(n_features,4),\n",
        "    )\n",
        "    self.comp_cls_lyr = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(n_features,5),\n",
        "        )    \n",
        "\n",
        "    self.a_lstm = nn.LSTM(batch_first=True,input_size=768,hidden_size=n_features,num_layers=1)\n",
        "    self.c_lstm = nn.LSTM(batch_first=True,input_size=768,hidden_size=n_features,num_layers=1)\n",
        "    \n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        self.freeze_bert()\n",
        "    else:\n",
        "        self.freeze_bert()\n",
        "        self.unfreeze_bert()\n",
        "        \n",
        "  def freeze_bert(self):\n",
        "      for p in self.bert_lyr.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "  def unfreeze_bert(self,from_lyr=6):\n",
        "\n",
        "    for lyr in self.bert_lyr.encoder.layer[-6:]:\n",
        "      for p in lyr.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "\n",
        "  def attention(self,seq_emb,attn_lyr,attn_mask_cls):\n",
        "    a = attn_lyr(seq_emb)\n",
        "    a = a + attn_mask_cls\n",
        "    a = a_output = a.softmax(dim=1)\n",
        "    a = torch.mul(seq_emb,a)\n",
        "    #a = a.mean(dim=1)\n",
        "    return a,a_output\n",
        "\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False):\n",
        "    attn_mask_cls = (1 - attn_masks)*-10000\n",
        "    attn_mask_cls.unsqueeze_(dim=-1)\n",
        "    seq_emb,pooled,hs,attn = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    #seq_emb, (_,_) = self.lstm(seq_emb)\n",
        "    a,a_output = self.attention(seq_emb,self.a_attn,attn_mask_cls)\n",
        "    _, (a,_) = self.a_lstm(a)\n",
        "    a=a.sum(dim=0)\n",
        "    c,c_output = self.attention(seq_emb,self.c_attn,attn_mask_cls)\n",
        "    _, (c,_) = self.c_lstm(c)\n",
        "    c=c.sum(dim=0)\n",
        "\n",
        "    a_pooled  = a\n",
        "    c_pooled  = c\n",
        "    outputs=[]\n",
        "    outputs += [\n",
        "                self.action_cls_lyr(a_pooled),\n",
        "                self.comp_cls_lyr(c_pooled),\n",
        "                ]\n",
        "    if (output_attn):\n",
        "      outputs += [a_output,c_output]\n",
        "    return outputs\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  del model\n",
        "  torch.cuda.reset_max_memory_allocated()\n",
        "  torch.cuda.reset_max_memory_cached()\n",
        "except NameError:\n",
        "  pass\n",
        "\n",
        "model = MyModel8(freeze_bert=False)\n",
        "model = model.to(DEVICE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLsZhocod-wL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers import BertForSequenceClassification\n",
        "\n",
        "class MHA(nn.Module):\n",
        "\n",
        "   def __init__(self, attns:int=1, attn_dropout=0.1,use_attn_proj=False,use_out_proj=False):\n",
        "     super().__init__()\n",
        "\n",
        "     self.attns = attns\n",
        "     self.attn_dropout = attn_dropout\n",
        "     self.attn_vec = nn.Parameter(torch.Tensor(768))\n",
        "     self.use_attn_proj = use_attn_proj\n",
        "     self.use_out_proj=use_out_proj\n",
        "     self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "     if use_attn_proj:\n",
        "       att_hs = int(768 / attns)\n",
        "       self.attn_proj=nn.Linear(att_hs,att_hs)\n",
        "     if use_out_proj:\n",
        "       self.out_proj=nn.Sequential(nn.Linear(768,768),nn.LayerNorm(768))\n",
        "     nn.init.normal_(self.attn_vec)\n",
        "\n",
        "   def forward(self,seq_emb,attn_mask_cls):\n",
        "    attn_vec= self.attn_vec\n",
        "    bs,ss,es = seq_emb.size()\n",
        "    seq_emb_t = seq_emb.view(bs,ss,self.attns,-1)\n",
        "    #(s,w,a,d/a) -> (s,a,w,d/a)\n",
        "    seq_emb_t = seq_emb_t.permute(0,2,1,3)\n",
        "\n",
        "    if self.use_attn_proj:\n",
        "      seq_emb_t = self.attn_proj(seq_emb_t)\n",
        "\n",
        "    #(d) -> (a,d/a,1)\n",
        "    attn = attn_vec.view(self.attns,-1).unsqueeze(dim=-1)\n",
        "    #(s,a,w,1)\n",
        "    alpha = torch.matmul(seq_emb_t,attn)\n",
        "    alpha /= math.sqrt(es)    \n",
        "    attn_mask_cls = attn_mask_cls.unsqueeze(dim=1)\n",
        "    attn_mask_cls = torch.cat([attn_mask_cls]*self.attns,dim=1)\n",
        "    attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "    #print(f'alpha.size()={alpha.size()}')\n",
        "    #print(f'attn_mask_cls.size()={attn_mask_cls.size()}')\n",
        "    alpha  = alpha + attn_mask_cls\n",
        "    alpha = alpha.softmax(dim=2)\n",
        "    alpha = self.attn_dropout(alpha)\n",
        "    #(s,a,w,d/a)\n",
        "    att_s = seq_emb_t*alpha\n",
        "    att_s = att_s.sum(dim=-2)\n",
        "    att_s = att_s.view(bs,-1).contiguous()\n",
        "    if self.use_out_proj:\n",
        "      att_s = self.out_proj(att_s)\n",
        "    return att_s,alpha.squeeze()\n",
        "\n",
        "\n",
        "############################### Model 9  ############################################\n",
        "# With different attention\n",
        "class MyModel9(nn.Module):\n",
        "  def __init__(self, freeze_bert = True, attns:int=1,cls_dropout=0.1, attn_dropout=0.1,use_attn_proj=False,use_out_proj=False):\n",
        "    super().__init__()\n",
        "    \n",
        "    attns = int(attns)\n",
        "\n",
        "    self.model_version = '9'\n",
        "    \n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "    \n",
        "    self.config = self.bert_lyr.config;\n",
        "\n",
        "    self.attns = attns\n",
        "\n",
        "    self.a_attn = MHA(attns=attns,attn_dropout=attn_dropout,use_attn_proj=use_attn_proj,use_out_proj=use_out_proj)\n",
        "\n",
        "    self.c_attn = MHA(attns=attns,attn_dropout=attn_dropout,use_attn_proj=use_attn_proj,use_out_proj=use_out_proj)\n",
        "\n",
        "    self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "    \n",
        "    assert 768 % attns == 0\n",
        "    \n",
        "    att_hs = int(768 / attns)\n",
        "\n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "            nn.Dropout(cls_dropout),\n",
        "            nn.LayerNorm(768),            \n",
        "            nn.Linear(768,4),\n",
        "            # nn.LayerNorm(768),\n",
        "            # nn.Dropout(cls_dropout),\n",
        "            # nn.Linear(768,64),\n",
        "            # nn.ReLU(),\n",
        "            # nn.LayerNorm(64),\n",
        "            # nn.Dropout(0.1),\n",
        "            # nn.Linear(64,4),\n",
        "    )\n",
        "    self.comp_cls_lyr = nn.Sequential(\n",
        "            nn.Dropout(cls_dropout),\n",
        "            nn.LayerNorm(768),            \n",
        "            nn.Linear(768,5),\n",
        "            # nn.LayerNorm(768),\n",
        "            # nn.Dropout(cls_dropout),\n",
        "            # nn.Linear(768,64),\n",
        "            # nn.ReLU(),\n",
        "            # nn.LayerNorm(64),\n",
        "            # nn.Dropout(0.1),\n",
        "            # nn.Linear(64,5),        \n",
        "            )    \n",
        "\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        self.freeze_bert()\n",
        "    # else:\n",
        "    #     self.freeze_bert()\n",
        "    #     self.unfreeze_bert()\n",
        "        \n",
        "  def freeze_bert(self):\n",
        "      for p in self.bert_lyr.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "  def unfreeze_bert(self,from_lyr=6):\n",
        "\n",
        "    for lyr in self.bert_lyr.encoder.layer[-6:]:\n",
        "      for p in lyr.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "  def attention_base(self,seq_emb,attn_lyr,attn_mask_cls):\n",
        "      attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "      a = attn_lyr(seq_emb)\n",
        "      a = a + attn_mask_cls\n",
        "      a = a_output = a.softmax(dim=1)\n",
        "      a = torch.mul(seq_emb,a)\n",
        "      a = a.sum(dim=1)\n",
        "      return a,a_output\n",
        "\n",
        "  def attention(self,seq_emb,attn_lyr,attn_mask_cls):\n",
        "    attns = self.attns\n",
        "    bs,ss,es = seq_emb.size()\n",
        "    #(s,w,d) -> (s,w,a,d/a)\n",
        "    seq_emb_t = seq_emb.view(bs,ss,attns,-1).contiguous()\n",
        "    if attns > 1:\n",
        "        attn_mask_cls =attn_mask_cls.repeat(1,attns).view(bs,attns,-1).permute(0,-1,-2).contiguous()\n",
        "    else:\n",
        "      attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "    # (s,w,a,d/a) -> (s,w,a,d/a,1)\n",
        "    attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "\n",
        "    # (s,w,a,d/a) -> (s,w,a,1)\n",
        "    alpha = attn_lyr(seq_emb_t)\n",
        "    alpha += attn_mask_cls\n",
        "    alpha = alpha.softmax(dim=-3)\n",
        "    alpha = self.attn_dropout(alpha)\n",
        "    #(s,w,a,d/a)\n",
        "    att_s = torch.mul(seq_emb_t, alpha)\n",
        "    att_s_ctx = att_s.sum(dim=1).view(bs,-1).contiguous()\n",
        "    alpha = alpha.squeeze(dim=-1).sum(dim=-1)\n",
        "    return att_s_ctx,alpha.squeeze(dim=-1)\n",
        "\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False):\n",
        "    attn_mask_cls = (1 - attn_masks)*-10000\n",
        "    #attn_mask_cls.unsqueeze_(dim=-1)\n",
        "    seq_emb,pooled,hs,attn = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    \n",
        "    #a,a_output = self.attention(seq_emb,self.a_attn,attn_mask_cls)\n",
        "    a,a_output = self.a_attn(seq_emb,attn_mask_cls)\n",
        "\n",
        "    #c,c_output = self.attention(seq_emb,self.c_attn,attn_mask_cls)\n",
        "    c,c_output = self.c_attn(seq_emb,attn_mask_cls)\n",
        "\n",
        "    a_pooled  = a\n",
        "    c_pooled  = c\n",
        "    outputs=[]\n",
        "    outputs += [\n",
        "                self.action_cls_lyr(a_pooled),\n",
        "                self.comp_cls_lyr(c_pooled),\n",
        "                ]\n",
        "    if (output_attn):\n",
        "      outputs += [a_output,c_output]\n",
        "    return outputs\n",
        "\n",
        "\n",
        "# try:\n",
        "#   del model\n",
        "#   torch.cuda.reset_max_memory_allocated()\n",
        "#   torch.cuda.reset_max_memory_cached()\n",
        "# except NameError:\n",
        "#   pass\n",
        "\n",
        "# model = MyModel9(freeze_bert=False,attns=3,use_out_proj=True)\n",
        "# model = model.to(DEVICE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXybETXDA3di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X, X_attn),(_,_) = next(iter(trn_dl))\n",
        "X, X_attn = X.to(DEVICE),X_attn.to(DEVICE)\n",
        "print(f'X.size()={X.size()}')\n",
        "a,c,a_a,c_a = model(X,X_attn,output_attn=True,output_hs=True)\n",
        "a_a.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnwTCsoH_KDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# #attn_lyr= model.a_attn\n",
        "# seq_emb = i.clone()\n",
        "# attn_mask_cls = attn_mask.clone()\n",
        "\n",
        "# bs,ss,es = seq_emb.size()\n",
        "# #(s,w,d) -> (s,w,a,d/a)\n",
        "# seq_emb_t = seq_emb.view(bs,ss,attns,-1).contiguous()\n",
        "# if attns > 1:\n",
        "#   attn_mask_cls =attn_mask_cls.repeat(1,attns).view(bs,attns,-1).permute(0,-1,-2).contiguous()\n",
        "# else:\n",
        "#   attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "\n",
        "# # (s,w,a,d/a) -> (s,w,a,d/a,1)\n",
        "# attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "\n",
        "# # (s,w,a,d/a) -> (s,w,a,1)\n",
        "# alpha = attn_lyr(seq_emb_t)\n",
        "# alpha += attn_mask_cls\n",
        "# alpha = alpha.softmax(dim=-3)\n",
        "\n",
        "# print(f'[Attention-1]alpha.size()={alpha.size()}')\n",
        "# print(alpha)\n",
        "# att_s = seq_emb_t * alpha\n",
        "# print(f'att_s={att_s.size()}')\n",
        "# att_s = att_s.mean(dim=1).view(bs,-1)\n",
        "# print(f'att_s={att_s.size()}')\n",
        "# print(att_s)\n",
        "\n",
        "# #Attention - Base\n",
        "# print('Attention - Base')\n",
        "# seq_emb = i.clone()\n",
        "# attn_mask_cls = attn_mask.clone()\n",
        "# attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "# alpha = attn_lyr(seq_emb)\n",
        "# alpha += attn_mask_cls\n",
        "# alpha = alpha.softmax(dim=1)\n",
        "# print(alpha)\n",
        "# att_s = torch.mul(seq_emb,alpha)\n",
        "# att_s = att_s.mean(dim=1)\n",
        "# print(att_s)\n",
        "\n",
        "# Attention - 2 \n",
        "seq_emb = i.clone()\n",
        "attn_mask_cls = attn_mask.clone()\n",
        "attn_mask_cls[0:-1]=-10000\n",
        "bs,ss,es = seq_emb.size()\n",
        "#print(bs,ss,es)\n",
        "#(s,w,d) -> (s,a,w,d/a)\n",
        "seq_emb_t = seq_emb.view(bs,ss,attns,-1)\n",
        "seq_emb_t = seq_emb_t.permute(0,2,1,3)\n",
        "\n",
        "# if attns > 1:\n",
        "#   # (s,w) -> (s,a,w)\n",
        "#   attn_mask_cls = attn_mask_cls.repeat(attns,1).contiguous()\n",
        "# else:\n",
        "#   attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "# # (s,a,w) -> (s,a,w,1)\n",
        "# attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "\n",
        "# (s,a,w,d/a) -> (s,a,w,1)\n",
        "attn = attn_vec.view(attns,-1).unsqueeze(dim=-1)\n",
        "alpha = torch.matmul(seq_emb_t,attn)\n",
        "alpha /=math.sqrt(es)\n",
        "#alpha += attn_mask_cls\n",
        "alpha  = alpha + attn_mask.unsqueeze(dim=-1)\n",
        "alpha = alpha.softmax(dim=2)\n",
        "att_s = seq_emb_t*alpha\n",
        "#att_s = att_s.sum(dim=2)\n",
        "#att_s = att_s.view(bs,-1).contiguous()\n",
        "print(f'[Attention-MH]alpha.size()={alpha.size()}')\n",
        "print(alpha)\n",
        "print(att_s)\n",
        "att_s = att_s.sum(dim=2)\n",
        "att_s = att_s.view(bs,-1).contiguous()\n",
        "print(att_s)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdbNjYOM8XEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = MyModel9(attns=1)\n",
        "model1= model1.to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s11DA4cwKnDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X, X_attn),(_,_) = next(iter(trn_dl))\n",
        "X, X_attn = X.to(DEVICE),X_attn.to(DEVICE)\n",
        "print(f'X.size()={X.size()}')\n",
        "a,c,a_a,c_a = model(X,X_attn,output_attn=True,output_hs=True)\n",
        "a_a.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IKDBrDGnXWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def configure_bert_optim(model:nn.Module,lr=2e-6,wd=0.1):\n",
        "      param_optimizer = list(model.named_parameters())\n",
        "      no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
        "      optimizer_grouped_parameters = [\n",
        "              {\n",
        "                  \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "                  \"weight_decay_rate\": wd,\n",
        "                  \"lr\":lr,\n",
        "              },\n",
        "              {\n",
        "                  \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "                  \"weight_decay_rate\": 0.0,\n",
        "                  \"lr\":lr,\n",
        "              },\n",
        "              ]\n",
        "      return optimizer_grouped_parameters\n",
        "\n",
        "\n",
        "# Load model\n",
        "#model = MyModel5(freeze_bert=False)\n",
        "# optimizer_params += [\n",
        "#                                 {'params':model.cls_lyr.parameters()},\n",
        "# ]\n",
        "\n",
        "MODEL=8\n",
        "\n",
        "if MODEL ==7:\n",
        "  model = MyModel7(freeze_bert=False)\n",
        "  optimizer_params = configure_bert_optim(model.bert_lyr,lr=1e-5)\n",
        "  optimizer_params += [\n",
        "                                  #{'params':model.bert_lyr.parameters(),'lr':1e-5,'weight_decay':0.1},\n",
        "                                  {'params':model.a_attn.parameters()},\n",
        "                                  {'params':model.c_attn.parameters()},\n",
        "                                  {'params':model.action_cls_lyr.parameters()},\n",
        "                                  {'params':model.comp_cls_lyr.parameters(),},                     \n",
        "  ]\n",
        "if MODEL == 8:\n",
        "  model = MyModel8(freeze_bert=False)\n",
        "  optimizer_params = configure_bert_optim(model.bert_lyr,lr=1e-5)\n",
        "  optimizer_params += [\n",
        "                                  #{'params':model.bert_lyr.parameters(),'lr':1e-5,'weight_decay':0.1},\n",
        "                                  {'params':model.a_attn.parameters()},\n",
        "                                  {'params':model.c_attn.parameters()},\n",
        "                                  {'params':model.shared_lyr.parameters()},\n",
        "                                  {'params':model.action_cls_lyr.parameters()},\n",
        "                                  {'params':model.comp_cls_lyr.parameters(),},                     \n",
        "  ]\n",
        "\n",
        "#model = MyModel9(freeze_bert=False,attns=4,cls_dropout=0.2,attn_dropout=0.2,use_attn_proj=False,use_out_proj=False)\n",
        "#model = MyModel10(freeze_bert=False,attns=1,attn_dropout=0.3)\n",
        "#model.load_state_dict(torch.load(f'data/model-{model.model_version}.dat'))\n",
        "\n",
        "\n",
        "g_wd = 1\n",
        "g_lr = 1e-3\n",
        "model.to(DEVICE)\n",
        "optimizer = torch.optim.AdamW(optimizer_params,lr=g_lr,weight_decay=g_wd )\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=20,T_mult=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_XdOgy_DHCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train model\n",
        "# torch.cuda.reset_max_memory_allocated()\n",
        "# torch.cuda.reset_max_memory_cached()\n",
        "SAVE_TO_FILE=False\n",
        "print(f'Device type is {DEVICE.type}')\n",
        "print(f'Runnig model version {model.model_version}')\n",
        "model.to(DEVICE)\n",
        "model.train()\n",
        "le_trnf = action_le.transform(trn_df.action)\n",
        "u,c = np.unique(le_trnf,return_counts=True)\n",
        "ac_class_weight=compute_class_weight('balanced',classes=u,y=le_trnf)\n",
        "ac_class_weight = torch.tensor(ac_class_weight,dtype=torch.float,device=DEVICE)\n",
        "le_trnf = component_le.transform(trn_df.component)\n",
        "u,c = np.unique(le_trnf,return_counts=True)\n",
        "com_class_weight=compute_class_weight('balanced',classes=u,y=le_trnf)\n",
        "com_class_weight = torch.tensor(com_class_weight,dtype=torch.float,device=DEVICE)\n",
        "\n",
        "print(f'ac_class_weight={ac_class_weight}')\n",
        "print(f'com_class_weight={com_class_weight}')\n",
        "\n",
        "\n",
        "action_criterion = nn.modules.loss.CrossEntropyLoss(weight=ac_class_weight,reduction='mean')\n",
        "component_criterion = nn.modules.loss.CrossEntropyLoss(weight=com_class_weight,reduction='mean')\n",
        "\n",
        "n_epochs = 500\n",
        "\n",
        "def evaluate_model(epoch:int,model:nn.Module,dl:DataLoader,optimizer):\n",
        "  t_loss=0.0\n",
        "  t_a_loss=0.0\n",
        "  t_c_loss=0.0\n",
        "  iters = len(dl)\n",
        "  for i,((X, attn_mask),(Y1,Y2)) in enumerate(dl):\n",
        "    X , attn_mask,Y1,Y2 = X.to(DEVICE),attn_mask.to(DEVICE),Y1.to(DEVICE),Y2.to(DEVICE)\n",
        "    p_a,p_c = model(X,attn_mask)\n",
        "    #print(f'p_a size={p_a.size()}')\n",
        "    action_loss = action_criterion(p_a.view(-1,4),Y1)\n",
        "    component_loss = component_criterion(p_c.view(-1,5),Y2)\n",
        "    loss =   action_loss + component_loss\n",
        "    if optimizer is not None:\n",
        "      scheduler.step(epoch+i/iters)\n",
        "      optimizer.zero_grad()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(),5)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    t_loss += loss.item()\n",
        "    t_a_loss += action_loss.item()\n",
        "    t_c_loss += component_loss.item()\n",
        "    \n",
        "  return t_loss,t_a_loss,t_c_loss\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  if epoch == 10:\n",
        "    print('Freezing BERT')\n",
        "    model.freeze_bert()\n",
        "  t_loss=0\n",
        "  a_loss=0\n",
        "  c_loss=0\n",
        "  t_loss,a_loss,c_loss = evaluate_model(epoch,model,trn_dl,optimizer)\n",
        "  if SAVE_TO_FILE and epoch%10 == 0:\n",
        "    torch.save(model.state_dict(),f'data/model-{model.model_version}.dat')\n",
        "  v_loss = 0\n",
        "  with torch.no_grad():\n",
        "    v_loss = evaluate_model(epoch,model,val_dl,None)\n",
        "    v_loss = [f'{v:0.4}' for v in v_loss]\n",
        "  print(f'Epoch:{epoch} Trn loss={t_loss:0.4}, Actn loss:{a_loss:0.4} ,Comp loss:{c_loss:0.4},  Validation loss:{v_loss}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4TEkKZhNWSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_tensor(t:torch.tensor):\n",
        "  t = t.squeeze().detach().cpu().numpy()\n",
        "  sh = t.shape[0]\n",
        "  print(f'sh={sh}')\n",
        "  if sh ==1:\n",
        "    t = [f'{item:.2}' for item in t]\n",
        "    print(t)\n",
        "  else:\n",
        "    for arr in t:\n",
        "      i = [f'{item:.2}' for item in arr]\n",
        "      print(i)\n",
        "      \n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  X,X_attn_mask = encode_X('',max_len=25)\n",
        "  \n",
        "  X_tokenized = X\n",
        "  X,X_attn_mask = X.to(DEVICE), X_attn_mask.to(DEVICE)\n",
        "  X.unsqueeze_(0)\n",
        "  X_attn_mask.unsqueeze_(0)\n",
        "  action,component,a_attn,c_attn = model(X,X_attn_mask,output_attn=True)\n",
        "  #action,component = model(X,X_attn_mask,output_attn=False)\n",
        "  action = action.softmax(dim=1)\n",
        "  component = component.softmax(dim=1)\n",
        "  action,component = [a.squeeze().detach().cpu().numpy() for a in [action,component]]\n",
        "\n",
        "  print(action_le.classes_)  \n",
        "  print(action)\n",
        "  print(a_attn.size())\n",
        "  action = np.argmax(action)\n",
        "  print(f'Selected Action: {action_le.inverse_transform([action])}')\n",
        "\n",
        "  print(component_le.classes_)\n",
        "  print(component)\n",
        "  component = np.argmax(component)\n",
        "  print(component)\n",
        "  print(f'Selected component: {component_le.inverse_transform([component])}')\n",
        "\n",
        "  print(tokenizer.convert_ids_to_tokens(X_tokenized.numpy().squeeze()))\n",
        "  print_tensor(a_attn)\n",
        "  print_tensor(c_attn)\n",
        "  # print(a_attn.squeeze().detach().cpu().numpy())\n",
        "  # print(c_attn.squeeze().detach().cpu().numpy())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8JOQMxA5AqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.a_attn_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UGbEM_6fjB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.a_attn_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4mB7LOFJmj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}